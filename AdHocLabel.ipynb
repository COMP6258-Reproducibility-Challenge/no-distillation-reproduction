{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "wyR7lH3JCwP0"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Cifar100"
      ],
      "metadata": {
        "id": "UUrydatHC0Yq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision.datasets.cifar import CIFAR100\n",
        "CIFAR100(root=\"/content/cifar100/\", train=True, download=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TMG1iDORHW9B",
        "outputId": "3f406332-98ed-48b8-d6ce-df0041769795"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset CIFAR100\n",
              "    Number of datapoints: 50000\n",
              "    Root location: /content/cifar100/\n",
              "    Split: Train"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "print(os.getcwd())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "48glvk_UEXJP",
        "outputId": "dcc73517-3347-40fc-8d72-a1f9b0140e60"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "C:\\Users\\nv1n24\\OneDrive - University of Southampton\\PhD\\Taught Modules\\COMP6258\\GroupWork\\no-distillation\\softlabel\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.chdir(r\"C:\\Users\\nv1n24\\OneDrive - University of Southampton\\PhD\\Taught Modules\\COMP6258\\GroupWork\\no-distillation\\train_expert\")"
      ],
      "metadata": {
        "id": "8MDnDfRnEmm4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "print(os.getcwd())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vADJRZ2SEtl0",
        "outputId": "c42591b3-66cb-49b1-d681-65e6c718c4b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "C:\\Users\\nv1n24\\OneDrive - University of Southampton\\PhD\\Taught Modules\\COMP6258\\GroupWork\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# %cd no-distillation/train_expert\n",
        "!python buffer.py --dataset=CIFAR100 --model=ConvNet --train_epochs=300 --num_experts=1  \\\n",
        "  --buffer_path=.  --data_path=/content/cifar100 --save_interval 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ef4PP3keEbPb",
        "outputId": "581e6b2a-cad9-4a53-9170-eb4fdd29c314"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Hyper-parameters: \n",
            " {'dataset': 'CIFAR100', 'subset': 'imagenette', 'model': 'ConvNet', 'num_experts': 1, 'lr_teacher': 0.01, 'teacher_label': False, 'selection_strategy': 'random', 'batch_train': 256, 'batch_real': 256, 'dsa': True, 'dsa_strategy': 'color_crop_cutout_flip_scale_rotate', 'data_path': '/content/cifar100', 'buffer_path': '.', 'train_epochs': 300, 'zca': False, 'decay': False, 'mom': 0, 'l2': 0, 'save_interval': 1, 'device': device(type='cuda'), 'dsa_param': <softlabel.utils.ParamDiffAug object at 0x000001F04375DE50>, 'decoder': None}\n",
            "BUILDING DATASET\n",
            "BUILDING DATASET\n",
            "class c = 0: 500 real images\n",
            "class c = 1: 500 real images\n",
            "class c = 2: 500 real images\n",
            "class c = 3: 500 real images\n",
            "class c = 4: 500 real images\n",
            "class c = 5: 500 real images\n",
            "class c = 6: 500 real images\n",
            "class c = 7: 500 real images\n",
            "class c = 8: 500 real images\n",
            "class c = 9: 500 real images\n",
            "class c = 10: 500 real images\n",
            "class c = 11: 500 real images\n",
            "class c = 12: 500 real images\n",
            "class c = 13: 500 real images\n",
            "class c = 14: 500 real images\n",
            "class c = 15: 500 real images\n",
            "class c = 16: 500 real images\n",
            "class c = 17: 500 real images\n",
            "class c = 18: 500 real images\n",
            "class c = 19: 500 real images\n",
            "class c = 20: 500 real images\n",
            "class c = 21: 500 real images\n",
            "class c = 22: 500 real images\n",
            "class c = 23: 500 real images\n",
            "class c = 24: 500 real images\n",
            "class c = 25: 500 real images\n",
            "class c = 26: 500 real images\n",
            "class c = 27: 500 real images\n",
            "class c = 28: 500 real images\n",
            "class c = 29: 500 real images\n",
            "class c = 30: 500 real images\n",
            "class c = 31: 500 real images\n",
            "class c = 32: 500 real images\n",
            "class c = 33: 500 real images\n",
            "class c = 34: 500 real images\n",
            "class c = 35: 500 real images\n",
            "class c = 36: 500 real images\n",
            "class c = 37: 500 real images\n",
            "class c = 38: 500 real images\n",
            "class c = 39: 500 real images\n",
            "class c = 40: 500 real images\n",
            "class c = 41: 500 real images\n",
            "class c = 42: 500 real images\n",
            "class c = 43: 500 real images\n",
            "class c = 44: 500 real images\n",
            "class c = 45: 500 real images\n",
            "class c = 46: 500 real images\n",
            "class c = 47: 500 real images\n",
            "class c = 48: 500 real images\n",
            "class c = 49: 500 real images\n",
            "class c = 50: 500 real images\n",
            "class c = 51: 500 real images\n",
            "class c = 52: 500 real images\n",
            "class c = 53: 500 real images\n",
            "class c = 54: 500 real images\n",
            "class c = 55: 500 real images\n",
            "class c = 56: 500 real images\n",
            "class c = 57: 500 real images\n",
            "class c = 58: 500 real images\n",
            "class c = 59: 500 real images\n",
            "class c = 60: 500 real images\n",
            "class c = 61: 500 real images\n",
            "class c = 62: 500 real images\n",
            "class c = 63: 500 real images\n",
            "class c = 64: 500 real images\n",
            "class c = 65: 500 real images\n",
            "class c = 66: 500 real images\n",
            "class c = 67: 500 real images\n",
            "class c = 68: 500 real images\n",
            "class c = 69: 500 real images\n",
            "class c = 70: 500 real images\n",
            "class c = 71: 500 real images\n",
            "class c = 72: 500 real images\n",
            "class c = 73: 500 real images\n",
            "class c = 74: 500 real images\n",
            "class c = 75: 500 real images\n",
            "class c = 76: 500 real images\n",
            "class c = 77: 500 real images\n",
            "class c = 78: 500 real images\n",
            "class c = 79: 500 real images\n",
            "class c = 80: 500 real images\n",
            "class c = 81: 500 real images\n",
            "class c = 82: 500 real images\n",
            "class c = 83: 500 real images\n",
            "class c = 84: 500 real images\n",
            "class c = 85: 500 real images\n",
            "class c = 86: 500 real images\n",
            "class c = 87: 500 real images\n",
            "class c = 88: 500 real images\n",
            "class c = 89: 500 real images\n",
            "class c = 90: 500 real images\n",
            "class c = 91: 500 real images\n",
            "class c = 92: 500 real images\n",
            "class c = 93: 500 real images\n",
            "class c = 94: 500 real images\n",
            "class c = 95: 500 real images\n",
            "class c = 96: 500 real images\n",
            "class c = 97: 500 real images\n",
            "class c = 98: 500 real images\n",
            "class c = 99: 500 real images\n",
            "real images channel 0, mean = 0.0775, std = 1.3215\n",
            "real images channel 1, mean = 0.0218, std = 1.2861\n",
            "real images channel 2, mean = -0.0278, std = 1.3739\n",
            "DC augmentation parameters: \n",
            " {'crop': 4, 'scale': 0.2, 'rotate': 45, 'noise': 0.001, 'strategy': 'crop_scale_rotate'}\n",
            "Itr: 0\tEpoch: 0\tTrain Acc: 0.06008\tTest Acc: 0.1001\n",
            "[INFO] Saving soft labels at epoch 0\n",
            "Saving labels to .\\CIFAR100_NO_ZCA\\ConvNet\\../../soft_labels_epoch_0.pt\n",
            "2025-05-13 02:34:53.641976 Itr: 0\tEpoch: 0\tTrain Acc: 0.06008\n",
            "Itr: 0\tEpoch: 1\tTrain Acc: 0.11372\tTest Acc: 0.134\n",
            "[INFO] Saving soft labels at epoch 1\n",
            "Saving labels to .\\CIFAR100_NO_ZCA\\ConvNet\\../../soft_labels_epoch_1.pt\n",
            "2025-05-13 02:35:22.158584 Itr: 0\tEpoch: 1\tTrain Acc: 0.11372\n",
            "Itr: 0\tEpoch: 2\tTrain Acc: 0.14258\tTest Acc: 0.1602\n",
            "[INFO] Saving soft labels at epoch 2\n",
            "Saving labels to .\\CIFAR100_NO_ZCA\\ConvNet\\../../soft_labels_epoch_2.pt\n",
            "2025-05-13 02:35:51.478563 Itr: 0\tEpoch: 2\tTrain Acc: 0.14258\n",
            "Itr: 0\tEpoch: 3\tTrain Acc: 0.16836\tTest Acc: 0.1832\n",
            "[INFO] Saving soft labels at epoch 3\n",
            "Saving labels to .\\CIFAR100_NO_ZCA\\ConvNet\\../../soft_labels_epoch_3.pt\n",
            "2025-05-13 02:36:20.536033 Itr: 0\tEpoch: 3\tTrain Acc: 0.16836\n",
            "Itr: 0\tEpoch: 4\tTrain Acc: 0.19222\tTest Acc: 0.2012\n",
            "[INFO] Saving soft labels at epoch 4\n",
            "Saving labels to .\\CIFAR100_NO_ZCA\\ConvNet\\../../soft_labels_epoch_4.pt\n",
            "2025-05-13 02:36:49.847644 Itr: 0\tEpoch: 4\tTrain Acc: 0.19222\n",
            "Itr: 0\tEpoch: 5\tTrain Acc: 0.21302\tTest Acc: 0.2347\n",
            "[INFO] Saving soft labels at epoch 5\n",
            "Saving labels to .\\CIFAR100_NO_ZCA\\ConvNet\\../../soft_labels_epoch_5.pt\n",
            "2025-05-13 02:37:18.090547 Itr: 0\tEpoch: 5\tTrain Acc: 0.21302\n",
            "Itr: 0\tEpoch: 6\tTrain Acc: 0.23144\tTest Acc: 0.246\n",
            "[INFO] Saving soft labels at epoch 6\n",
            "Saving labels to .\\CIFAR100_NO_ZCA\\ConvNet\\../../soft_labels_epoch_6.pt\n",
            "2025-05-13 02:37:46.920408 Itr: 0\tEpoch: 6\tTrain Acc: 0.23144\n",
            "Itr: 0\tEpoch: 7\tTrain Acc: 0.24602\tTest Acc: 0.2584\n",
            "[INFO] Saving soft labels at epoch 7\n",
            "Saving labels to .\\CIFAR100_NO_ZCA\\ConvNet\\../../soft_labels_epoch_7.pt\n",
            "2025-05-13 02:38:16.367519 Itr: 0\tEpoch: 7\tTrain Acc: 0.24602\n",
            "Itr: 0\tEpoch: 8\tTrain Acc: 0.2636\tTest Acc: 0.2772\n",
            "[INFO] Saving soft labels at epoch 8\n",
            "Saving labels to .\\CIFAR100_NO_ZCA\\ConvNet\\../../soft_labels_epoch_8.pt\n",
            "2025-05-13 02:38:45.456851 Itr: 0\tEpoch: 8\tTrain Acc: 0.2636\n",
            "Itr: 0\tEpoch: 9\tTrain Acc: 0.27678\tTest Acc: 0.2628\n",
            "[INFO] Saving soft labels at epoch 9\n",
            "Saving labels to .\\CIFAR100_NO_ZCA\\ConvNet\\../../soft_labels_epoch_9.pt\n",
            "2025-05-13 02:39:14.427544 Itr: 0\tEpoch: 9\tTrain Acc: 0.27678\n",
            "Itr: 0\tEpoch: 10\tTrain Acc: 0.2873\tTest Acc: 0.2982\n",
            "[INFO] Saving soft labels at epoch 10\n",
            "Saving labels to .\\CIFAR100_NO_ZCA\\ConvNet\\../../soft_labels_epoch_10.pt\n",
            "2025-05-13 02:39:43.014223 Itr: 0\tEpoch: 10\tTrain Acc: 0.2873\n",
            "Itr: 0\tEpoch: 11\tTrain Acc: 0.301\tTest Acc: 0.3147\n",
            "[INFO] Saving soft labels at epoch 11\n",
            "Saving labels to .\\CIFAR100_NO_ZCA\\ConvNet\\../../soft_labels_epoch_11.pt\n",
            "2025-05-13 02:40:11.815436 Itr: 0\tEpoch: 11\tTrain Acc: 0.301\n",
            "Itr: 0\tEpoch: 12\tTrain Acc: 0.31084\tTest Acc: 0.3189\n",
            "[INFO] Saving soft labels at epoch 12\n",
            "Saving labels to .\\CIFAR100_NO_ZCA\\ConvNet\\../../soft_labels_epoch_12.pt\n",
            "2025-05-13 02:40:41.038118 Itr: 0\tEpoch: 12\tTrain Acc: 0.31084\n",
            "Itr: 0\tEpoch: 13\tTrain Acc: 0.32118\tTest Acc: 0.3278\n",
            "[INFO] Saving soft labels at epoch 13\n",
            "Saving labels to .\\CIFAR100_NO_ZCA\\ConvNet\\../../soft_labels_epoch_13.pt\n",
            "2025-05-13 02:41:08.990998 Itr: 0\tEpoch: 13\tTrain Acc: 0.32118\n",
            "Itr: 0\tEpoch: 14\tTrain Acc: 0.33052\tTest Acc: 0.3299\n",
            "[INFO] Saving soft labels at epoch 14\n",
            "Saving labels to .\\CIFAR100_NO_ZCA\\ConvNet\\../../soft_labels_epoch_14.pt\n",
            "2025-05-13 02:41:38.191766 Itr: 0\tEpoch: 14\tTrain Acc: 0.33052\n",
            "Itr: 0\tEpoch: 15\tTrain Acc: 0.33922\tTest Acc: 0.3473\n",
            "[INFO] Saving soft labels at epoch 15\n",
            "Saving labels to .\\CIFAR100_NO_ZCA\\ConvNet\\../../soft_labels_epoch_15.pt\n",
            "2025-05-13 02:42:06.781498 Itr: 0\tEpoch: 15\tTrain Acc: 0.33922\n",
            "Itr: 0\tEpoch: 16\tTrain Acc: 0.34764\tTest Acc: 0.3453\n",
            "[INFO] Saving soft labels at epoch 16\n",
            "Saving labels to .\\CIFAR100_NO_ZCA\\ConvNet\\../../soft_labels_epoch_16.pt\n",
            "2025-05-13 02:42:36.323152 Itr: 0\tEpoch: 16\tTrain Acc: 0.34764\n",
            "Itr: 0\tEpoch: 17\tTrain Acc: 0.35282\tTest Acc: 0.3519\n",
            "[INFO] Saving soft labels at epoch 17\n",
            "Saving labels to .\\CIFAR100_NO_ZCA\\ConvNet\\../../soft_labels_epoch_17.pt\n",
            "2025-05-13 02:43:06.206126 Itr: 0\tEpoch: 17\tTrain Acc: 0.35282\n",
            "Itr: 0\tEpoch: 18\tTrain Acc: 0.36264\tTest Acc: 0.3572\n",
            "[INFO] Saving soft labels at epoch 18\n",
            "Saving labels to .\\CIFAR100_NO_ZCA\\ConvNet\\../../soft_labels_epoch_18.pt\n",
            "2025-05-13 02:43:35.150428 Itr: 0\tEpoch: 18\tTrain Acc: 0.36264\n",
            "Itr: 0\tEpoch: 19\tTrain Acc: 0.36626\tTest Acc: 0.3534\n",
            "[INFO] Saving soft labels at epoch 19\n",
            "Saving labels to .\\CIFAR100_NO_ZCA\\ConvNet\\../../soft_labels_epoch_19.pt\n",
            "2025-05-13 02:44:04.282101 Itr: 0\tEpoch: 19\tTrain Acc: 0.36626\n",
            "Itr: 0\tEpoch: 20\tTrain Acc: 0.37482\tTest Acc: 0.3633\n",
            "[INFO] Saving soft labels at epoch 20\n",
            "Saving labels to .\\CIFAR100_NO_ZCA\\ConvNet\\../../soft_labels_epoch_20.pt\n",
            "2025-05-13 02:44:33.713384 Itr: 0\tEpoch: 20\tTrain Acc: 0.37482\n",
            "Itr: 0\tEpoch: 21\tTrain Acc: 0.38156\tTest Acc: 0.3653\n",
            "[INFO] Saving soft labels at epoch 21\n",
            "Saving labels to .\\CIFAR100_NO_ZCA\\ConvNet\\../../soft_labels_epoch_21.pt\n",
            "2025-05-13 02:45:16.248162 Itr: 0\tEpoch: 21\tTrain Acc: 0.38156\n",
            "Itr: 0\tEpoch: 22\tTrain Acc: 0.38936\tTest Acc: 0.3896\n",
            "[INFO] Saving soft labels at epoch 22\n",
            "Saving labels to .\\CIFAR100_NO_ZCA\\ConvNet\\../../soft_labels_epoch_22.pt\n",
            "2025-05-13 02:45:45.722925 Itr: 0\tEpoch: 22\tTrain Acc: 0.38936\n",
            "Itr: 0\tEpoch: 23\tTrain Acc: 0.39478\tTest Acc: 0.3873\n",
            "[INFO] Saving soft labels at epoch 23\n",
            "Saving labels to .\\CIFAR100_NO_ZCA\\ConvNet\\../../soft_labels_epoch_23.pt\n",
            "2025-05-13 02:46:14.998746 Itr: 0\tEpoch: 23\tTrain Acc: 0.39478\n",
            "Itr: 0\tEpoch: 24\tTrain Acc: 0.40034\tTest Acc: 0.3888\n",
            "[INFO] Saving soft labels at epoch 24\n",
            "Saving labels to .\\CIFAR100_NO_ZCA\\ConvNet\\../../soft_labels_epoch_24.pt\n",
            "2025-05-13 02:46:44.357342 Itr: 0\tEpoch: 24\tTrain Acc: 0.40034\n",
            "Itr: 0\tEpoch: 25\tTrain Acc: 0.41022\tTest Acc: 0.398\n",
            "[INFO] Saving soft labels at epoch 25\n",
            "Saving labels to .\\CIFAR100_NO_ZCA\\ConvNet\\../../soft_labels_epoch_25.pt\n",
            "2025-05-13 02:47:13.326837 Itr: 0\tEpoch: 25\tTrain Acc: 0.41022\n",
            "Itr: 0\tEpoch: 26\tTrain Acc: 0.41638\tTest Acc: 0.4168\n",
            "[INFO] Saving soft labels at epoch 26\n",
            "Saving labels to .\\CIFAR100_NO_ZCA\\ConvNet\\../../soft_labels_epoch_26.pt\n",
            "2025-05-13 02:47:46.075817 Itr: 0\tEpoch: 26\tTrain Acc: 0.41638\n",
            "Itr: 0\tEpoch: 27\tTrain Acc: 0.42118\tTest Acc: 0.3995\n",
            "[INFO] Saving soft labels at epoch 27\n",
            "Saving labels to .\\CIFAR100_NO_ZCA\\ConvNet\\../../soft_labels_epoch_27.pt\n",
            "2025-05-13 02:48:15.849319 Itr: 0\tEpoch: 27\tTrain Acc: 0.42118\n",
            "Itr: 0\tEpoch: 28\tTrain Acc: 0.42626\tTest Acc: 0.4027\n",
            "[INFO] Saving soft labels at epoch 28\n",
            "Saving labels to .\\CIFAR100_NO_ZCA\\ConvNet\\../../soft_labels_epoch_28.pt\n",
            "2025-05-13 02:48:44.668753 Itr: 0\tEpoch: 28\tTrain Acc: 0.42626\n",
            "Itr: 0\tEpoch: 29\tTrain Acc: 0.43006\tTest Acc: 0.4195\n",
            "[INFO] Saving soft labels at epoch 29\n",
            "Saving labels to .\\CIFAR100_NO_ZCA\\ConvNet\\../../soft_labels_epoch_29.pt\n",
            "2025-05-13 02:49:13.416422 Itr: 0\tEpoch: 29\tTrain Acc: 0.43006\n",
            "Itr: 0\tEpoch: 30\tTrain Acc: 0.4394\tTest Acc: 0.4133\n",
            "[INFO] Saving soft labels at epoch 30\n",
            "Saving labels to .\\CIFAR100_NO_ZCA\\ConvNet\\../../soft_labels_epoch_30.pt\n",
            "2025-05-13 02:49:42.616331 Itr: 0\tEpoch: 30\tTrain Acc: 0.4394\n",
            "Itr: 0\tEpoch: 31\tTrain Acc: 0.44018\tTest Acc: 0.4254\n",
            "[INFO] Saving soft labels at epoch 31\n",
            "Saving labels to .\\CIFAR100_NO_ZCA\\ConvNet\\../../soft_labels_epoch_31.pt\n",
            "2025-05-13 02:50:12.234964 Itr: 0\tEpoch: 31\tTrain Acc: 0.44018\n",
            "Itr: 0\tEpoch: 32\tTrain Acc: 0.4472\tTest Acc: 0.4212\n",
            "[INFO] Saving soft labels at epoch 32\n",
            "Saving labels to .\\CIFAR100_NO_ZCA\\ConvNet\\../../soft_labels_epoch_32.pt\n",
            "2025-05-13 02:50:41.771561 Itr: 0\tEpoch: 32\tTrain Acc: 0.4472\n",
            "Itr: 0\tEpoch: 33\tTrain Acc: 0.45124\tTest Acc: 0.4313\n",
            "[INFO] Saving soft labels at epoch 33\n",
            "Saving labels to .\\CIFAR100_NO_ZCA\\ConvNet\\../../soft_labels_epoch_33.pt\n",
            "2025-05-13 02:51:10.565421 Itr: 0\tEpoch: 33\tTrain Acc: 0.45124\n",
            "Itr: 0\tEpoch: 34\tTrain Acc: 0.46128\tTest Acc: 0.4393\n",
            "[INFO] Saving soft labels at epoch 34\n",
            "Saving labels to .\\CIFAR100_NO_ZCA\\ConvNet\\../../soft_labels_epoch_34.pt\n",
            "2025-05-13 02:51:39.864757 Itr: 0\tEpoch: 34\tTrain Acc: 0.46128\n",
            "Itr: 0\tEpoch: 35\tTrain Acc: 0.4625\tTest Acc: 0.4363\n",
            "[INFO] Saving soft labels at epoch 35\n",
            "Saving labels to .\\CIFAR100_NO_ZCA\\ConvNet\\../../soft_labels_epoch_35.pt\n",
            "2025-05-13 02:52:08.763561 Itr: 0\tEpoch: 35\tTrain Acc: 0.4625\n",
            "Itr: 0\tEpoch: 36\tTrain Acc: 0.46672\tTest Acc: 0.4408\n",
            "[INFO] Saving soft labels at epoch 36\n",
            "Saving labels to .\\CIFAR100_NO_ZCA\\ConvNet\\../../soft_labels_epoch_36.pt\n",
            "2025-05-13 02:52:38.540589 Itr: 0\tEpoch: 36\tTrain Acc: 0.46672\n",
            "Itr: 0\tEpoch: 37\tTrain Acc: 0.47282\tTest Acc: 0.4404\n",
            "[INFO] Saving soft labels at epoch 37\n",
            "Saving labels to .\\CIFAR100_NO_ZCA\\ConvNet\\../../soft_labels_epoch_37.pt\n",
            "2025-05-13 02:53:07.790588 Itr: 0\tEpoch: 37\tTrain Acc: 0.47282\n",
            "Itr: 0\tEpoch: 38\tTrain Acc: 0.47464\tTest Acc: 0.439\n",
            "[INFO] Saving soft labels at epoch 38\n",
            "Saving labels to .\\CIFAR100_NO_ZCA\\ConvNet\\../../soft_labels_epoch_38.pt\n",
            "2025-05-13 02:53:36.323909 Itr: 0\tEpoch: 38\tTrain Acc: 0.47464\n",
            "Itr: 0\tEpoch: 39\tTrain Acc: 0.4791\tTest Acc: 0.4317\n",
            "[INFO] Saving soft labels at epoch 39\n",
            "Saving labels to .\\CIFAR100_NO_ZCA\\ConvNet\\../../soft_labels_epoch_39.pt\n",
            "2025-05-13 02:54:06.742160 Itr: 0\tEpoch: 39\tTrain Acc: 0.4791\n",
            "Itr: 0\tEpoch: 40\tTrain Acc: 0.48766\tTest Acc: 0.4408\n",
            "[INFO] Saving soft labels at epoch 40\n",
            "Saving labels to .\\CIFAR100_NO_ZCA\\ConvNet\\../../soft_labels_epoch_40.pt\n",
            "2025-05-13 02:54:35.703686 Itr: 0\tEpoch: 40\tTrain Acc: 0.48766\n",
            "Itr: 0\tEpoch: 41\tTrain Acc: 0.49138\tTest Acc: 0.4359\n",
            "[INFO] Saving soft labels at epoch 41\n",
            "Saving labels to .\\CIFAR100_NO_ZCA\\ConvNet\\../../soft_labels_epoch_41.pt\n",
            "2025-05-13 02:55:04.540357 Itr: 0\tEpoch: 41\tTrain Acc: 0.49138\n",
            "Itr: 0\tEpoch: 42\tTrain Acc: 0.4891\tTest Acc: 0.4646\n",
            "[INFO] Saving soft labels at epoch 42\n",
            "Saving labels to .\\CIFAR100_NO_ZCA\\ConvNet\\../../soft_labels_epoch_42.pt\n",
            "2025-05-13 02:55:33.831187 Itr: 0\tEpoch: 42\tTrain Acc: 0.4891\n",
            "Itr: 0\tEpoch: 43\tTrain Acc: 0.49832\tTest Acc: 0.4578\n",
            "[INFO] Saving soft labels at epoch 43\n",
            "Saving labels to .\\CIFAR100_NO_ZCA\\ConvNet\\../../soft_labels_epoch_43.pt\n",
            "2025-05-13 02:56:03.228942 Itr: 0\tEpoch: 43\tTrain Acc: 0.49832\n",
            "Itr: 0\tEpoch: 44\tTrain Acc: 0.50146\tTest Acc: 0.4618\n",
            "[INFO] Saving soft labels at epoch 44\n",
            "Saving labels to .\\CIFAR100_NO_ZCA\\ConvNet\\../../soft_labels_epoch_44.pt\n",
            "2025-05-13 02:56:33.261845 Itr: 0\tEpoch: 44\tTrain Acc: 0.50146\n",
            "Itr: 0\tEpoch: 45\tTrain Acc: 0.51016\tTest Acc: 0.4621\n",
            "[INFO] Saving soft labels at epoch 45\n",
            "Saving labels to .\\CIFAR100_NO_ZCA\\ConvNet\\../../soft_labels_epoch_45.pt\n",
            "2025-05-13 02:57:03.301669 Itr: 0\tEpoch: 45\tTrain Acc: 0.51016\n",
            "Itr: 0\tEpoch: 46\tTrain Acc: 0.50592\tTest Acc: 0.4492\n",
            "[INFO] Saving soft labels at epoch 46\n",
            "Saving labels to .\\CIFAR100_NO_ZCA\\ConvNet\\../../soft_labels_epoch_46.pt\n",
            "2025-05-13 02:57:31.639566 Itr: 0\tEpoch: 46\tTrain Acc: 0.50592\n",
            "Itr: 0\tEpoch: 47\tTrain Acc: 0.51844\tTest Acc: 0.4633\n",
            "[INFO] Saving soft labels at epoch 47\n",
            "Saving labels to .\\CIFAR100_NO_ZCA\\ConvNet\\../../soft_labels_epoch_47.pt\n",
            "2025-05-13 02:58:01.277236 Itr: 0\tEpoch: 47\tTrain Acc: 0.51844\n",
            "Itr: 0\tEpoch: 48\tTrain Acc: 0.51678\tTest Acc: 0.4733\n",
            "[INFO] Saving soft labels at epoch 48\n",
            "Saving labels to .\\CIFAR100_NO_ZCA\\ConvNet\\../../soft_labels_epoch_48.pt\n",
            "2025-05-13 02:58:29.882125 Itr: 0\tEpoch: 48\tTrain Acc: 0.51678\n",
            "Itr: 0\tEpoch: 49\tTrain Acc: 0.51786\tTest Acc: 0.475\n",
            "[INFO] Saving soft labels at epoch 49\n",
            "Saving labels to .\\CIFAR100_NO_ZCA\\ConvNet\\../../soft_labels_epoch_49.pt\n",
            "2025-05-13 02:58:59.980350 Itr: 0\tEpoch: 49\tTrain Acc: 0.51786\n",
            "Itr: 0\tEpoch: 50\tTrain Acc: 0.52714\tTest Acc: 0.4773\n",
            "[INFO] Saving soft labels at epoch 50\n",
            "Saving labels to .\\CIFAR100_NO_ZCA\\ConvNet\\../../soft_labels_epoch_50.pt\n",
            "2025-05-13 02:59:29.210369 Itr: 0\tEpoch: 50\tTrain Acc: 0.52714\n",
            "Itr: 0\tEpoch: 51\tTrain Acc: 0.5212\tTest Acc: 0.4694\n",
            "[INFO] Saving soft labels at epoch 51\n",
            "Saving labels to .\\CIFAR100_NO_ZCA\\ConvNet\\../../soft_labels_epoch_51.pt\n",
            "2025-05-13 03:00:01.224289 Itr: 0\tEpoch: 51\tTrain Acc: 0.5212\n",
            "Itr: 0\tEpoch: 52\tTrain Acc: 0.52702\tTest Acc: 0.481\n",
            "[INFO] Saving soft labels at epoch 52\n",
            "Saving labels to .\\CIFAR100_NO_ZCA\\ConvNet\\../../soft_labels_epoch_52.pt\n",
            "2025-05-13 03:00:41.683559 Itr: 0\tEpoch: 52\tTrain Acc: 0.52702\n",
            "Itr: 0\tEpoch: 53\tTrain Acc: 0.5362\tTest Acc: 0.4786\n",
            "[INFO] Saving soft labels at epoch 53\n",
            "Saving labels to .\\CIFAR100_NO_ZCA\\ConvNet\\../../soft_labels_epoch_53.pt\n",
            "2025-05-13 03:01:10.386323 Itr: 0\tEpoch: 53\tTrain Acc: 0.5362\n",
            "Itr: 0\tEpoch: 54\tTrain Acc: 0.53664\tTest Acc: 0.4835\n",
            "[INFO] Saving soft labels at epoch 54\n",
            "Saving labels to .\\CIFAR100_NO_ZCA\\ConvNet\\../../soft_labels_epoch_54.pt\n",
            "2025-05-13 03:01:38.758325 Itr: 0\tEpoch: 54\tTrain Acc: 0.53664\n",
            "Itr: 0\tEpoch: 55\tTrain Acc: 0.54146\tTest Acc: 0.4806\n",
            "[INFO] Saving soft labels at epoch 55\n",
            "Saving labels to .\\CIFAR100_NO_ZCA\\ConvNet\\../../soft_labels_epoch_55.pt\n",
            "2025-05-13 03:02:07.329987 Itr: 0\tEpoch: 55\tTrain Acc: 0.54146\n",
            "Itr: 0\tEpoch: 56\tTrain Acc: 0.54814\tTest Acc: 0.4852\n",
            "[INFO] Saving soft labels at epoch 56\n",
            "Saving labels to .\\CIFAR100_NO_ZCA\\ConvNet\\../../soft_labels_epoch_56.pt\n",
            "2025-05-13 03:02:37.280458 Itr: 0\tEpoch: 56\tTrain Acc: 0.54814\n",
            "Itr: 0\tEpoch: 57\tTrain Acc: 0.5495\tTest Acc: 0.4781\n",
            "[INFO] Saving soft labels at epoch 57\n",
            "Saving labels to .\\CIFAR100_NO_ZCA\\ConvNet\\../../soft_labels_epoch_57.pt\n",
            "2025-05-13 03:03:05.757237 Itr: 0\tEpoch: 57\tTrain Acc: 0.5495\n",
            "Itr: 0\tEpoch: 58\tTrain Acc: 0.5531\tTest Acc: 0.4732\n",
            "[INFO] Saving soft labels at epoch 58\n",
            "Saving labels to .\\CIFAR100_NO_ZCA\\ConvNet\\../../soft_labels_epoch_58.pt\n",
            "2025-05-13 03:03:34.364045 Itr: 0\tEpoch: 58\tTrain Acc: 0.5531\n",
            "Itr: 0\tEpoch: 59\tTrain Acc: 0.55498\tTest Acc: 0.4666\n",
            "[INFO] Saving soft labels at epoch 59\n",
            "Saving labels to .\\CIFAR100_NO_ZCA\\ConvNet\\../../soft_labels_epoch_59.pt\n",
            "2025-05-13 03:04:03.964346 Itr: 0\tEpoch: 59\tTrain Acc: 0.55498\n",
            "Itr: 0\tEpoch: 60\tTrain Acc: 0.55466\tTest Acc: 0.4679\n",
            "[INFO] Saving soft labels at epoch 60\n",
            "Saving labels to .\\CIFAR100_NO_ZCA\\ConvNet\\../../soft_labels_epoch_60.pt\n",
            "2025-05-13 03:04:33.314622 Itr: 0\tEpoch: 60\tTrain Acc: 0.55466\n",
            "Itr: 0\tEpoch: 61\tTrain Acc: 0.55766\tTest Acc: 0.491\n",
            "[INFO] Saving soft labels at epoch 61\n",
            "Saving labels to .\\CIFAR100_NO_ZCA\\ConvNet\\../../soft_labels_epoch_61.pt\n",
            "2025-05-13 03:05:04.240447 Itr: 0\tEpoch: 61\tTrain Acc: 0.55766\n",
            "Itr: 0\tEpoch: 62\tTrain Acc: 0.56716\tTest Acc: 0.4904\n",
            "[INFO] Saving soft labels at epoch 62\n",
            "Saving labels to .\\CIFAR100_NO_ZCA\\ConvNet\\../../soft_labels_epoch_62.pt\n",
            "2025-05-13 03:05:32.685878 Itr: 0\tEpoch: 62\tTrain Acc: 0.56716\n",
            "Itr: 0\tEpoch: 63\tTrain Acc: 0.57094\tTest Acc: 0.4717\n",
            "[INFO] Saving soft labels at epoch 63\n",
            "Saving labels to .\\CIFAR100_NO_ZCA\\ConvNet\\../../soft_labels_epoch_63.pt\n",
            "2025-05-13 03:06:03.513886 Itr: 0\tEpoch: 63\tTrain Acc: 0.57094\n",
            "Itr: 0\tEpoch: 64\tTrain Acc: 0.56816\tTest Acc: 0.4917\n",
            "[INFO] Saving soft labels at epoch 64\n",
            "Saving labels to .\\CIFAR100_NO_ZCA\\ConvNet\\../../soft_labels_epoch_64.pt\n",
            "2025-05-13 03:06:35.592095 Itr: 0\tEpoch: 64\tTrain Acc: 0.56816\n",
            "Itr: 0\tEpoch: 65\tTrain Acc: 0.57586\tTest Acc: 0.4864\n",
            "[INFO] Saving soft labels at epoch 65\n",
            "Saving labels to .\\CIFAR100_NO_ZCA\\ConvNet\\../../soft_labels_epoch_65.pt\n",
            "2025-05-13 03:07:05.079951 Itr: 0\tEpoch: 65\tTrain Acc: 0.57586\n",
            "Itr: 0\tEpoch: 66\tTrain Acc: 0.57656\tTest Acc: 0.4896\n",
            "[INFO] Saving soft labels at epoch 66\n",
            "Saving labels to .\\CIFAR100_NO_ZCA\\ConvNet\\../../soft_labels_epoch_66.pt\n",
            "2025-05-13 03:07:33.428710 Itr: 0\tEpoch: 66\tTrain Acc: 0.57656\n",
            "Itr: 0\tEpoch: 67\tTrain Acc: 0.58462\tTest Acc: 0.4916\n",
            "[INFO] Saving soft labels at epoch 67\n",
            "Saving labels to .\\CIFAR100_NO_ZCA\\ConvNet\\../../soft_labels_epoch_67.pt\n",
            "2025-05-13 03:08:03.966420 Itr: 0\tEpoch: 67\tTrain Acc: 0.58462\n",
            "Itr: 0\tEpoch: 68\tTrain Acc: 0.57964\tTest Acc: 0.4877\n",
            "[INFO] Saving soft labels at epoch 68\n",
            "Saving labels to .\\CIFAR100_NO_ZCA\\ConvNet\\../../soft_labels_epoch_68.pt\n",
            "2025-05-13 03:08:35.631865 Itr: 0\tEpoch: 68\tTrain Acc: 0.57964\n",
            "Itr: 0\tEpoch: 69\tTrain Acc: 0.5783\tTest Acc: 0.4856\n",
            "[INFO] Saving soft labels at epoch 69\n",
            "Saving labels to .\\CIFAR100_NO_ZCA\\ConvNet\\../../soft_labels_epoch_69.pt\n",
            "2025-05-13 03:09:05.375118 Itr: 0\tEpoch: 69\tTrain Acc: 0.5783\n",
            "Itr: 0\tEpoch: 70\tTrain Acc: 0.58782\tTest Acc: 0.4902\n",
            "[INFO] Saving soft labels at epoch 70\n",
            "Saving labels to .\\CIFAR100_NO_ZCA\\ConvNet\\../../soft_labels_epoch_70.pt\n",
            "2025-05-13 03:09:33.824149 Itr: 0\tEpoch: 70\tTrain Acc: 0.58782\n",
            "Itr: 0\tEpoch: 71\tTrain Acc: 0.58632\tTest Acc: 0.5057\n",
            "[INFO] Saving soft labels at epoch 71\n",
            "Saving labels to .\\CIFAR100_NO_ZCA\\ConvNet\\../../soft_labels_epoch_71.pt\n",
            "2025-05-13 03:10:03.662252 Itr: 0\tEpoch: 71\tTrain Acc: 0.58632\n",
            "Itr: 0\tEpoch: 72\tTrain Acc: 0.5939\tTest Acc: 0.5032\n",
            "[INFO] Saving soft labels at epoch 72\n",
            "Saving labels to .\\CIFAR100_NO_ZCA\\ConvNet\\../../soft_labels_epoch_72.pt\n",
            "2025-05-13 03:10:32.842169 Itr: 0\tEpoch: 72\tTrain Acc: 0.5939\n",
            "Itr: 0\tEpoch: 73\tTrain Acc: 0.59394\tTest Acc: 0.4971\n",
            "[INFO] Saving soft labels at epoch 73\n",
            "Saving labels to .\\CIFAR100_NO_ZCA\\ConvNet\\../../soft_labels_epoch_73.pt\n",
            "2025-05-13 03:11:02.830550 Itr: 0\tEpoch: 73\tTrain Acc: 0.59394\n",
            "Itr: 0\tEpoch: 74\tTrain Acc: 0.59382\tTest Acc: 0.5017\n",
            "[INFO] Saving soft labels at epoch 74\n",
            "Saving labels to .\\CIFAR100_NO_ZCA\\ConvNet\\../../soft_labels_epoch_74.pt\n",
            "2025-05-13 03:11:31.793834 Itr: 0\tEpoch: 74\tTrain Acc: 0.59382\n",
            "Itr: 0\tEpoch: 75\tTrain Acc: 0.59964\tTest Acc: 0.4793\n",
            "[INFO] Saving soft labels at epoch 75\n",
            "Saving labels to .\\CIFAR100_NO_ZCA\\ConvNet\\../../soft_labels_epoch_75.pt\n",
            "2025-05-13 03:12:00.777563 Itr: 0\tEpoch: 75\tTrain Acc: 0.59964\n",
            "Itr: 0\tEpoch: 76\tTrain Acc: 0.59578\tTest Acc: 0.5071\n",
            "[INFO] Saving soft labels at epoch 76\n",
            "Saving labels to .\\CIFAR100_NO_ZCA\\ConvNet\\../../soft_labels_epoch_76.pt\n",
            "2025-05-13 03:12:29.459544 Itr: 0\tEpoch: 76\tTrain Acc: 0.59578\n",
            "Itr: 0\tEpoch: 77\tTrain Acc: 0.6022\tTest Acc: 0.49\n",
            "[INFO] Saving soft labels at epoch 77\n",
            "Saving labels to .\\CIFAR100_NO_ZCA\\ConvNet\\../../soft_labels_epoch_77.pt\n",
            "2025-05-13 03:12:59.567654 Itr: 0\tEpoch: 77\tTrain Acc: 0.6022\n",
            "Itr: 0\tEpoch: 78\tTrain Acc: 0.61134\tTest Acc: 0.4959\n",
            "[INFO] Saving soft labels at epoch 78\n",
            "Saving labels to .\\CIFAR100_NO_ZCA\\ConvNet\\../../soft_labels_epoch_78.pt\n",
            "2025-05-13 03:13:28.642973 Itr: 0\tEpoch: 78\tTrain Acc: 0.61134\n",
            "Itr: 0\tEpoch: 79\tTrain Acc: 0.60888\tTest Acc: 0.4965\n",
            "[INFO] Saving soft labels at epoch 79\n",
            "Saving labels to .\\CIFAR100_NO_ZCA\\ConvNet\\../../soft_labels_epoch_79.pt\n",
            "2025-05-13 03:13:58.517438 Itr: 0\tEpoch: 79\tTrain Acc: 0.60888\n",
            "Itr: 0\tEpoch: 80\tTrain Acc: 0.60816\tTest Acc: 0.5073\n",
            "[INFO] Saving soft labels at epoch 80\n",
            "Saving labels to .\\CIFAR100_NO_ZCA\\ConvNet\\../../soft_labels_epoch_80.pt\n",
            "2025-05-13 03:14:27.733727 Itr: 0\tEpoch: 80\tTrain Acc: 0.60816\n",
            "Itr: 0\tEpoch: 81\tTrain Acc: 0.61276\tTest Acc: 0.5073\n",
            "[INFO] Saving soft labels at epoch 81\n",
            "Saving labels to .\\CIFAR100_NO_ZCA\\ConvNet\\../../soft_labels_epoch_81.pt\n",
            "2025-05-13 03:14:58.181977 Itr: 0\tEpoch: 81\tTrain Acc: 0.61276\n",
            "Itr: 0\tEpoch: 82\tTrain Acc: 0.60978\tTest Acc: 0.4769\n",
            "[INFO] Saving soft labels at epoch 82\n",
            "Saving labels to .\\CIFAR100_NO_ZCA\\ConvNet\\../../soft_labels_epoch_82.pt\n",
            "2025-05-13 03:15:39.353676 Itr: 0\tEpoch: 82\tTrain Acc: 0.60978\n",
            "Itr: 0\tEpoch: 83\tTrain Acc: 0.61998\tTest Acc: 0.517\n",
            "[INFO] Saving soft labels at epoch 83\n",
            "Saving labels to .\\CIFAR100_NO_ZCA\\ConvNet\\../../soft_labels_epoch_83.pt\n",
            "2025-05-13 03:16:09.138908 Itr: 0\tEpoch: 83\tTrain Acc: 0.61998\n",
            "Itr: 0\tEpoch: 84\tTrain Acc: 0.62738\tTest Acc: 0.502\n",
            "[INFO] Saving soft labels at epoch 84\n",
            "Saving labels to .\\CIFAR100_NO_ZCA\\ConvNet\\../../soft_labels_epoch_84.pt\n",
            "2025-05-13 03:16:38.516684 Itr: 0\tEpoch: 84\tTrain Acc: 0.62738\n",
            "Itr: 0\tEpoch: 85\tTrain Acc: 0.6226\tTest Acc: 0.4993\n",
            "[INFO] Saving soft labels at epoch 85\n",
            "Saving labels to .\\CIFAR100_NO_ZCA\\ConvNet\\../../soft_labels_epoch_85.pt\n",
            "2025-05-13 03:17:07.850587 Itr: 0\tEpoch: 85\tTrain Acc: 0.6226\n",
            "Itr: 0\tEpoch: 86\tTrain Acc: 0.6237\tTest Acc: 0.5044\n",
            "[INFO] Saving soft labels at epoch 86\n",
            "Saving labels to .\\CIFAR100_NO_ZCA\\ConvNet\\../../soft_labels_epoch_86.pt\n",
            "2025-05-13 03:17:36.793342 Itr: 0\tEpoch: 86\tTrain Acc: 0.6237\n",
            "Itr: 0\tEpoch: 87\tTrain Acc: 0.63138\tTest Acc: 0.5004\n",
            "[INFO] Saving soft labels at epoch 87\n",
            "Saving labels to .\\CIFAR100_NO_ZCA\\ConvNet\\../../soft_labels_epoch_87.pt\n",
            "2025-05-13 03:18:07.478461 Itr: 0\tEpoch: 87\tTrain Acc: 0.63138\n",
            "Itr: 0\tEpoch: 88\tTrain Acc: 0.62716\tTest Acc: 0.5065\n",
            "[INFO] Saving soft labels at epoch 88\n",
            "Saving labels to .\\CIFAR100_NO_ZCA\\ConvNet\\../../soft_labels_epoch_88.pt\n",
            "2025-05-13 03:18:35.478384 Itr: 0\tEpoch: 88\tTrain Acc: 0.62716\n",
            "Itr: 0\tEpoch: 89\tTrain Acc: 0.6318\tTest Acc: 0.5139\n",
            "[INFO] Saving soft labels at epoch 89\n",
            "Saving labels to .\\CIFAR100_NO_ZCA\\ConvNet\\../../soft_labels_epoch_89.pt\n",
            "2025-05-13 03:19:04.714388 Itr: 0\tEpoch: 89\tTrain Acc: 0.6318\n",
            "Itr: 0\tEpoch: 90\tTrain Acc: 0.63594\tTest Acc: 0.5068\n",
            "[INFO] Saving soft labels at epoch 90\n",
            "Saving labels to .\\CIFAR100_NO_ZCA\\ConvNet\\../../soft_labels_epoch_90.pt\n",
            "2025-05-13 03:19:33.866849 Itr: 0\tEpoch: 90\tTrain Acc: 0.63594\n",
            "Itr: 0\tEpoch: 91\tTrain Acc: 0.64224\tTest Acc: 0.5127\n",
            "[INFO] Saving soft labels at epoch 91\n",
            "Saving labels to .\\CIFAR100_NO_ZCA\\ConvNet\\../../soft_labels_epoch_91.pt\n",
            "2025-05-13 03:20:04.024943 Itr: 0\tEpoch: 91\tTrain Acc: 0.64224\n",
            "Itr: 0\tEpoch: 92\tTrain Acc: 0.6403\tTest Acc: 0.5013\n",
            "[INFO] Saving soft labels at epoch 92\n",
            "Saving labels to .\\CIFAR100_NO_ZCA\\ConvNet\\../../soft_labels_epoch_92.pt\n",
            "2025-05-13 03:20:33.250516 Itr: 0\tEpoch: 92\tTrain Acc: 0.6403\n",
            "Itr: 0\tEpoch: 93\tTrain Acc: 0.64806\tTest Acc: 0.5081\n",
            "[INFO] Saving soft labels at epoch 93\n",
            "Saving labels to .\\CIFAR100_NO_ZCA\\ConvNet\\../../soft_labels_epoch_93.pt\n",
            "2025-05-13 03:21:02.664983 Itr: 0\tEpoch: 93\tTrain Acc: 0.64806\n",
            "Itr: 0\tEpoch: 94\tTrain Acc: 0.64294\tTest Acc: 0.5189\n",
            "[INFO] Saving soft labels at epoch 94\n",
            "Saving labels to .\\CIFAR100_NO_ZCA\\ConvNet\\../../soft_labels_epoch_94.pt\n",
            "2025-05-13 03:21:31.441445 Itr: 0\tEpoch: 94\tTrain Acc: 0.64294\n",
            "Itr: 0\tEpoch: 95\tTrain Acc: 0.63954\tTest Acc: 0.521\n",
            "[INFO] Saving soft labels at epoch 95\n",
            "Saving labels to .\\CIFAR100_NO_ZCA\\ConvNet\\../../soft_labels_epoch_95.pt\n",
            "2025-05-13 03:22:01.880039 Itr: 0\tEpoch: 95\tTrain Acc: 0.63954\n",
            "Itr: 0\tEpoch: 96\tTrain Acc: 0.64946\tTest Acc: 0.5195\n",
            "[INFO] Saving soft labels at epoch 96\n",
            "Saving labels to .\\CIFAR100_NO_ZCA\\ConvNet\\../../soft_labels_epoch_96.pt\n",
            "2025-05-13 03:22:30.988616 Itr: 0\tEpoch: 96\tTrain Acc: 0.64946\n",
            "Itr: 0\tEpoch: 97\tTrain Acc: 0.64402\tTest Acc: 0.5128\n",
            "[INFO] Saving soft labels at epoch 97\n",
            "Saving labels to .\\CIFAR100_NO_ZCA\\ConvNet\\../../soft_labels_epoch_97.pt\n",
            "2025-05-13 03:23:01.386441 Itr: 0\tEpoch: 97\tTrain Acc: 0.64402\n",
            "Itr: 0\tEpoch: 98\tTrain Acc: 0.64944\tTest Acc: 0.5115\n",
            "[INFO] Saving soft labels at epoch 98\n",
            "Saving labels to .\\CIFAR100_NO_ZCA\\ConvNet\\../../soft_labels_epoch_98.pt\n",
            "2025-05-13 03:23:29.884304 Itr: 0\tEpoch: 98\tTrain Acc: 0.64944\n",
            "Itr: 0\tEpoch: 99\tTrain Acc: 0.65264\tTest Acc: 0.5187\n",
            "[INFO] Saving soft labels at epoch 99\n",
            "Saving labels to .\\CIFAR100_NO_ZCA\\ConvNet\\../../soft_labels_epoch_99.pt\n",
            "2025-05-13 03:23:59.575702 Itr: 0\tEpoch: 99\tTrain Acc: 0.65264\n",
            "Itr: 0\tEpoch: 100\tTrain Acc: 0.65596\tTest Acc: 0.5172\n",
            "[INFO] Saving soft labels at epoch 100\n",
            "Saving labels to .\\CIFAR100_NO_ZCA\\ConvNet\\../../soft_labels_epoch_100.pt\n",
            "2025-05-13 03:24:28.910372 Itr: 0\tEpoch: 100\tTrain Acc: 0.65596\n",
            "Itr: 0\tEpoch: 101\tTrain Acc: 0.66266\tTest Acc: 0.5278\n",
            "[INFO] Saving soft labels at epoch 101\n",
            "Saving labels to .\\CIFAR100_NO_ZCA\\ConvNet\\../../soft_labels_epoch_101.pt\n",
            "2025-05-13 03:24:57.320863 Itr: 0\tEpoch: 101\tTrain Acc: 0.66266\n",
            "Itr: 0\tEpoch: 102\tTrain Acc: 0.6529\tTest Acc: 0.5196\n",
            "[INFO] Saving soft labels at epoch 102\n",
            "Saving labels to .\\CIFAR100_NO_ZCA\\ConvNet\\../../soft_labels_epoch_102.pt\n",
            "2025-05-13 03:25:26.106541 Itr: 0\tEpoch: 102\tTrain Acc: 0.6529\n",
            "Itr: 0\tEpoch: 103\tTrain Acc: 0.66464\tTest Acc: 0.5254\n",
            "[INFO] Saving soft labels at epoch 103\n",
            "Saving labels to .\\CIFAR100_NO_ZCA\\ConvNet\\../../soft_labels_epoch_103.pt\n",
            "2025-05-13 03:25:54.856553 Itr: 0\tEpoch: 103\tTrain Acc: 0.66464\n",
            "Itr: 0\tEpoch: 104\tTrain Acc: 0.6688\tTest Acc: 0.517\n",
            "[INFO] Saving soft labels at epoch 104\n",
            "Saving labels to .\\CIFAR100_NO_ZCA\\ConvNet\\../../soft_labels_epoch_104.pt\n",
            "2025-05-13 03:26:26.302418 Itr: 0\tEpoch: 104\tTrain Acc: 0.6688\n",
            "Itr: 0\tEpoch: 105\tTrain Acc: 0.66502\tTest Acc: 0.5218\n",
            "[INFO] Saving soft labels at epoch 105\n",
            "Saving labels to .\\CIFAR100_NO_ZCA\\ConvNet\\../../soft_labels_epoch_105.pt\n",
            "2025-05-13 03:26:55.456869 Itr: 0\tEpoch: 105\tTrain Acc: 0.66502\n",
            "Itr: 0\tEpoch: 106\tTrain Acc: 0.67158\tTest Acc: 0.5244\n",
            "[INFO] Saving soft labels at epoch 106\n",
            "Saving labels to .\\CIFAR100_NO_ZCA\\ConvNet\\../../soft_labels_epoch_106.pt\n",
            "2025-05-13 03:27:25.092976 Itr: 0\tEpoch: 106\tTrain Acc: 0.67158\n",
            "Itr: 0\tEpoch: 107\tTrain Acc: 0.67118\tTest Acc: 0.5222\n",
            "[INFO] Saving soft labels at epoch 107\n",
            "Saving labels to .\\CIFAR100_NO_ZCA\\ConvNet\\../../soft_labels_epoch_107.pt\n",
            "2025-05-13 03:27:55.761985 Itr: 0\tEpoch: 107\tTrain Acc: 0.67118\n",
            "Itr: 0\tEpoch: 108\tTrain Acc: 0.67088\tTest Acc: 0.5226\n",
            "[INFO] Saving soft labels at epoch 108\n",
            "Saving labels to .\\CIFAR100_NO_ZCA\\ConvNet\\../../soft_labels_epoch_108.pt\n",
            "2025-05-13 03:28:25.941873 Itr: 0\tEpoch: 108\tTrain Acc: 0.67088\n",
            "Itr: 0\tEpoch: 109\tTrain Acc: 0.67292\tTest Acc: 0.5188\n",
            "[INFO] Saving soft labels at epoch 109\n",
            "Saving labels to .\\CIFAR100_NO_ZCA\\ConvNet\\../../soft_labels_epoch_109.pt\n",
            "2025-05-13 03:28:56.191080 Itr: 0\tEpoch: 109\tTrain Acc: 0.67292\n",
            "Itr: 0\tEpoch: 110\tTrain Acc: 0.67532\tTest Acc: 0.5193\n",
            "[INFO] Saving soft labels at epoch 110\n",
            "Saving labels to .\\CIFAR100_NO_ZCA\\ConvNet\\../../soft_labels_epoch_110.pt\n",
            "2025-05-13 03:29:27.052171 Itr: 0\tEpoch: 110\tTrain Acc: 0.67532\n",
            "Itr: 0\tEpoch: 111\tTrain Acc: 0.6706\tTest Acc: 0.528\n",
            "[INFO] Saving soft labels at epoch 111\n",
            "Saving labels to .\\CIFAR100_NO_ZCA\\ConvNet\\../../soft_labels_epoch_111.pt\n",
            "2025-05-13 03:29:57.037139 Itr: 0\tEpoch: 111\tTrain Acc: 0.6706\n",
            "Itr: 0\tEpoch: 112\tTrain Acc: 0.68256\tTest Acc: 0.5266\n",
            "[INFO] Saving soft labels at epoch 112\n",
            "Saving labels to .\\CIFAR100_NO_ZCA\\ConvNet\\../../soft_labels_epoch_112.pt\n",
            "2025-05-13 03:30:40.574294 Itr: 0\tEpoch: 112\tTrain Acc: 0.68256\n",
            "Itr: 0\tEpoch: 113\tTrain Acc: 0.68156\tTest Acc: 0.5058\n",
            "[INFO] Saving soft labels at epoch 113\n",
            "Saving labels to .\\CIFAR100_NO_ZCA\\ConvNet\\../../soft_labels_epoch_113.pt\n",
            "2025-05-13 03:31:09.042297 Itr: 0\tEpoch: 113\tTrain Acc: 0.68156\n",
            "Itr: 0\tEpoch: 114\tTrain Acc: 0.67872\tTest Acc: 0.5176\n",
            "[INFO] Saving soft labels at epoch 114\n",
            "Saving labels to .\\CIFAR100_NO_ZCA\\ConvNet\\../../soft_labels_epoch_114.pt\n",
            "2025-05-13 03:31:38.187811 Itr: 0\tEpoch: 114\tTrain Acc: 0.67872\n",
            "Itr: 0\tEpoch: 115\tTrain Acc: 0.677\tTest Acc: 0.5303\n",
            "[INFO] Saving soft labels at epoch 115\n",
            "Saving labels to .\\CIFAR100_NO_ZCA\\ConvNet\\../../soft_labels_epoch_115.pt\n",
            "2025-05-13 03:32:06.900102 Itr: 0\tEpoch: 115\tTrain Acc: 0.677\n",
            "Itr: 0\tEpoch: 116\tTrain Acc: 0.6844\tTest Acc: 0.5276\n",
            "[INFO] Saving soft labels at epoch 116\n",
            "Saving labels to .\\CIFAR100_NO_ZCA\\ConvNet\\../../soft_labels_epoch_116.pt\n",
            "2025-05-13 03:32:36.000118 Itr: 0\tEpoch: 116\tTrain Acc: 0.6844\n",
            "Itr: 0\tEpoch: 117\tTrain Acc: 0.67744\tTest Acc: 0.5117\n",
            "[INFO] Saving soft labels at epoch 117\n",
            "Saving labels to .\\CIFAR100_NO_ZCA\\ConvNet\\../../soft_labels_epoch_117.pt\n",
            "2025-05-13 03:33:05.830023 Itr: 0\tEpoch: 117\tTrain Acc: 0.67744\n",
            "Itr: 0\tEpoch: 118\tTrain Acc: 0.68632\tTest Acc: 0.5171\n",
            "[INFO] Saving soft labels at epoch 118\n",
            "Saving labels to .\\CIFAR100_NO_ZCA\\ConvNet\\../../soft_labels_epoch_118.pt\n",
            "2025-05-13 03:33:34.829599 Itr: 0\tEpoch: 118\tTrain Acc: 0.68632\n",
            "Itr: 0\tEpoch: 119\tTrain Acc: 0.6878\tTest Acc: 0.5228\n",
            "[INFO] Saving soft labels at epoch 119\n",
            "Saving labels to .\\CIFAR100_NO_ZCA\\ConvNet\\../../soft_labels_epoch_119.pt\n",
            "2025-05-13 03:34:04.367055 Itr: 0\tEpoch: 119\tTrain Acc: 0.6878\n",
            "Itr: 0\tEpoch: 120\tTrain Acc: 0.6868\tTest Acc: 0.5128\n",
            "[INFO] Saving soft labels at epoch 120\n",
            "Saving labels to .\\CIFAR100_NO_ZCA\\ConvNet\\../../soft_labels_epoch_120.pt\n",
            "2025-05-13 03:34:34.327562 Itr: 0\tEpoch: 120\tTrain Acc: 0.6868\n",
            "Itr: 0\tEpoch: 121\tTrain Acc: 0.68892\tTest Acc: 0.5266\n",
            "[INFO] Saving soft labels at epoch 121\n",
            "Saving labels to .\\CIFAR100_NO_ZCA\\ConvNet\\../../soft_labels_epoch_121.pt\n",
            "2025-05-13 03:35:04.174979 Itr: 0\tEpoch: 121\tTrain Acc: 0.68892\n",
            "Itr: 0\tEpoch: 122\tTrain Acc: 0.69246\tTest Acc: 0.5217\n",
            "[INFO] Saving soft labels at epoch 122\n",
            "Saving labels to .\\CIFAR100_NO_ZCA\\ConvNet\\../../soft_labels_epoch_122.pt\n",
            "2025-05-13 03:35:33.124657 Itr: 0\tEpoch: 122\tTrain Acc: 0.69246\n",
            "Itr: 0\tEpoch: 123\tTrain Acc: 0.69736\tTest Acc: 0.527\n",
            "[INFO] Saving soft labels at epoch 123\n",
            "Saving labels to .\\CIFAR100_NO_ZCA\\ConvNet\\../../soft_labels_epoch_123.pt\n",
            "2025-05-13 03:36:02.698180 Itr: 0\tEpoch: 123\tTrain Acc: 0.69736\n",
            "Itr: 0\tEpoch: 124\tTrain Acc: 0.69506\tTest Acc: 0.5389\n",
            "[INFO] Saving soft labels at epoch 124\n",
            "Saving labels to .\\CIFAR100_NO_ZCA\\ConvNet\\../../soft_labels_epoch_124.pt\n",
            "2025-05-13 03:36:35.116500 Itr: 0\tEpoch: 124\tTrain Acc: 0.69506\n",
            "Itr: 0\tEpoch: 125\tTrain Acc: 0.69724\tTest Acc: 0.5303\n",
            "[INFO] Saving soft labels at epoch 125\n",
            "Saving labels to .\\CIFAR100_NO_ZCA\\ConvNet\\../../soft_labels_epoch_125.pt\n",
            "2025-05-13 03:37:04.438341 Itr: 0\tEpoch: 125\tTrain Acc: 0.69724\n",
            "Itr: 0\tEpoch: 126\tTrain Acc: 0.69716\tTest Acc: 0.5273\n",
            "[INFO] Saving soft labels at epoch 126\n",
            "Saving labels to .\\CIFAR100_NO_ZCA\\ConvNet\\../../soft_labels_epoch_126.pt\n",
            "2025-05-13 03:37:33.877455 Itr: 0\tEpoch: 126\tTrain Acc: 0.69716\n",
            "Itr: 0\tEpoch: 127\tTrain Acc: 0.70258\tTest Acc: 0.5286\n",
            "[INFO] Saving soft labels at epoch 127\n",
            "Saving labels to .\\CIFAR100_NO_ZCA\\ConvNet\\../../soft_labels_epoch_127.pt\n",
            "2025-05-13 03:38:03.083445 Itr: 0\tEpoch: 127\tTrain Acc: 0.70258\n",
            "Itr: 0\tEpoch: 128\tTrain Acc: 0.71278\tTest Acc: 0.5311\n",
            "[INFO] Saving soft labels at epoch 128\n",
            "Saving labels to .\\CIFAR100_NO_ZCA\\ConvNet\\../../soft_labels_epoch_128.pt\n",
            "2025-05-13 03:38:32.882737 Itr: 0\tEpoch: 128\tTrain Acc: 0.71278\n",
            "Itr: 0\tEpoch: 129\tTrain Acc: 0.71886\tTest Acc: 0.5246\n",
            "[INFO] Saving soft labels at epoch 129\n",
            "Saving labels to .\\CIFAR100_NO_ZCA\\ConvNet\\../../soft_labels_epoch_129.pt\n",
            "2025-05-13 03:39:02.100358 Itr: 0\tEpoch: 129\tTrain Acc: 0.71886\n",
            "Itr: 0\tEpoch: 130\tTrain Acc: 0.70586\tTest Acc: 0.5183\n",
            "[INFO] Saving soft labels at epoch 130\n",
            "Saving labels to .\\CIFAR100_NO_ZCA\\ConvNet\\../../soft_labels_epoch_130.pt\n",
            "2025-05-13 03:39:32.483191 Itr: 0\tEpoch: 130\tTrain Acc: 0.70586\n",
            "Itr: 0\tEpoch: 131\tTrain Acc: 0.7079\tTest Acc: 0.5314\n",
            "[INFO] Saving soft labels at epoch 131\n",
            "Saving labels to .\\CIFAR100_NO_ZCA\\ConvNet\\../../soft_labels_epoch_131.pt\n",
            "2025-05-13 03:40:02.372825 Itr: 0\tEpoch: 131\tTrain Acc: 0.7079\n",
            "Itr: 0\tEpoch: 132\tTrain Acc: 0.71196\tTest Acc: 0.5187\n",
            "[INFO] Saving soft labels at epoch 132\n",
            "Saving labels to .\\CIFAR100_NO_ZCA\\ConvNet\\../../soft_labels_epoch_132.pt\n",
            "2025-05-13 03:40:32.690322 Itr: 0\tEpoch: 132\tTrain Acc: 0.71196\n",
            "Itr: 0\tEpoch: 133\tTrain Acc: 0.71656\tTest Acc: 0.5365\n",
            "[INFO] Saving soft labels at epoch 133\n",
            "Saving labels to .\\CIFAR100_NO_ZCA\\ConvNet\\../../soft_labels_epoch_133.pt\n",
            "2025-05-13 03:41:03.032865 Itr: 0\tEpoch: 133\tTrain Acc: 0.71656\n",
            "Itr: 0\tEpoch: 134\tTrain Acc: 0.70828\tTest Acc: 0.5279\n",
            "[INFO] Saving soft labels at epoch 134\n",
            "Saving labels to .\\CIFAR100_NO_ZCA\\ConvNet\\../../soft_labels_epoch_134.pt\n",
            "2025-05-13 03:41:32.213782 Itr: 0\tEpoch: 134\tTrain Acc: 0.70828\n",
            "Itr: 0\tEpoch: 135\tTrain Acc: 0.71312\tTest Acc: 0.5195\n",
            "[INFO] Saving soft labels at epoch 135\n",
            "Saving labels to .\\CIFAR100_NO_ZCA\\ConvNet\\../../soft_labels_epoch_135.pt\n",
            "2025-05-13 03:42:02.042596 Itr: 0\tEpoch: 135\tTrain Acc: 0.71312\n",
            "Itr: 0\tEpoch: 136\tTrain Acc: 0.70958\tTest Acc: 0.5292\n",
            "[INFO] Saving soft labels at epoch 136\n",
            "Saving labels to .\\CIFAR100_NO_ZCA\\ConvNet\\../../soft_labels_epoch_136.pt\n",
            "2025-05-13 03:42:31.423398 Itr: 0\tEpoch: 136\tTrain Acc: 0.70958\n",
            "Itr: 0\tEpoch: 137\tTrain Acc: 0.71862\tTest Acc: 0.5315\n",
            "[INFO] Saving soft labels at epoch 137\n",
            "Saving labels to .\\CIFAR100_NO_ZCA\\ConvNet\\../../soft_labels_epoch_137.pt\n",
            "2025-05-13 03:43:01.283995 Itr: 0\tEpoch: 137\tTrain Acc: 0.71862\n",
            "Itr: 0\tEpoch: 138\tTrain Acc: 0.7091\tTest Acc: 0.5299\n",
            "[INFO] Saving soft labels at epoch 138\n",
            "Saving labels to .\\CIFAR100_NO_ZCA\\ConvNet\\../../soft_labels_epoch_138.pt\n",
            "2025-05-13 03:43:30.000276 Itr: 0\tEpoch: 138\tTrain Acc: 0.7091\n",
            "Itr: 0\tEpoch: 139\tTrain Acc: 0.72278\tTest Acc: 0.4875\n",
            "[INFO] Saving soft labels at epoch 139\n",
            "Saving labels to .\\CIFAR100_NO_ZCA\\ConvNet\\../../soft_labels_epoch_139.pt\n",
            "2025-05-13 03:44:00.489615 Itr: 0\tEpoch: 139\tTrain Acc: 0.72278\n",
            "Itr: 0\tEpoch: 140\tTrain Acc: 0.72544\tTest Acc: 0.5294\n",
            "[INFO] Saving soft labels at epoch 140\n",
            "Saving labels to .\\CIFAR100_NO_ZCA\\ConvNet\\../../soft_labels_epoch_140.pt\n",
            "2025-05-13 03:44:29.945645 Itr: 0\tEpoch: 140\tTrain Acc: 0.72544\n",
            "Itr: 0\tEpoch: 141\tTrain Acc: 0.73004\tTest Acc: 0.5312\n",
            "[INFO] Saving soft labels at epoch 141\n",
            "Saving labels to .\\CIFAR100_NO_ZCA\\ConvNet\\../../soft_labels_epoch_141.pt\n",
            "2025-05-13 03:45:00.152875 Itr: 0\tEpoch: 141\tTrain Acc: 0.73004\n",
            "Itr: 0\tEpoch: 142\tTrain Acc: 0.72404\tTest Acc: 0.5358\n",
            "[INFO] Saving soft labels at epoch 142\n",
            "Saving labels to .\\CIFAR100_NO_ZCA\\ConvNet\\../../soft_labels_epoch_142.pt\n",
            "2025-05-13 03:45:43.396434 Itr: 0\tEpoch: 142\tTrain Acc: 0.72404\n",
            "Itr: 0\tEpoch: 143\tTrain Acc: 0.73132\tTest Acc: 0.5244\n",
            "[INFO] Saving soft labels at epoch 143\n",
            "Saving labels to .\\CIFAR100_NO_ZCA\\ConvNet\\../../soft_labels_epoch_143.pt\n",
            "2025-05-13 03:46:12.604621 Itr: 0\tEpoch: 143\tTrain Acc: 0.73132\n",
            "Itr: 0\tEpoch: 144\tTrain Acc: 0.72552\tTest Acc: 0.5161\n",
            "[INFO] Saving soft labels at epoch 144\n",
            "Saving labels to .\\CIFAR100_NO_ZCA\\ConvNet\\../../soft_labels_epoch_144.pt\n",
            "2025-05-13 03:46:42.148238 Itr: 0\tEpoch: 144\tTrain Acc: 0.72552\n",
            "Itr: 0\tEpoch: 145\tTrain Acc: 0.72188\tTest Acc: 0.5368\n",
            "[INFO] Saving soft labels at epoch 145\n",
            "Saving labels to .\\CIFAR100_NO_ZCA\\ConvNet\\../../soft_labels_epoch_145.pt\n",
            "2025-05-13 03:47:11.184214 Itr: 0\tEpoch: 145\tTrain Acc: 0.72188\n",
            "Itr: 0\tEpoch: 146\tTrain Acc: 0.72828\tTest Acc: 0.5352\n",
            "[INFO] Saving soft labels at epoch 146\n",
            "Saving labels to .\\CIFAR100_NO_ZCA\\ConvNet\\../../soft_labels_epoch_146.pt\n",
            "2025-05-13 03:47:41.704942 Itr: 0\tEpoch: 146\tTrain Acc: 0.72828\n",
            "Itr: 0\tEpoch: 147\tTrain Acc: 0.735\tTest Acc: 0.5267\n",
            "[INFO] Saving soft labels at epoch 147\n",
            "Saving labels to .\\CIFAR100_NO_ZCA\\ConvNet\\../../soft_labels_epoch_147.pt\n",
            "2025-05-13 03:48:15.061787 Itr: 0\tEpoch: 147\tTrain Acc: 0.735\n",
            "Itr: 0\tEpoch: 148\tTrain Acc: 0.7349\tTest Acc: 0.5272\n",
            "[INFO] Saving soft labels at epoch 148\n",
            "Saving labels to .\\CIFAR100_NO_ZCA\\ConvNet\\../../soft_labels_epoch_148.pt\n",
            "2025-05-13 03:48:43.885984 Itr: 0\tEpoch: 148\tTrain Acc: 0.7349\n",
            "Itr: 0\tEpoch: 149\tTrain Acc: 0.73208\tTest Acc: 0.5269\n",
            "[INFO] Saving soft labels at epoch 149\n",
            "Saving labels to .\\CIFAR100_NO_ZCA\\ConvNet\\../../soft_labels_epoch_149.pt\n",
            "2025-05-13 03:49:12.530501 Itr: 0\tEpoch: 149\tTrain Acc: 0.73208\n",
            "Itr: 0\tEpoch: 150\tTrain Acc: 0.731\tTest Acc: 0.5385\n",
            "[INFO] Saving soft labels at epoch 150\n",
            "Saving labels to .\\CIFAR100_NO_ZCA\\ConvNet\\../../soft_labels_epoch_150.pt\n",
            "2025-05-13 03:49:41.494357 Itr: 0\tEpoch: 150\tTrain Acc: 0.731\n",
            "Itr: 0\tEpoch: 151\tTrain Acc: 0.72548\tTest Acc: 0.522\n",
            "[INFO] Saving soft labels at epoch 151\n",
            "Saving labels to .\\CIFAR100_NO_ZCA\\ConvNet\\../../soft_labels_epoch_151.pt\n",
            "2025-05-13 03:50:10.028696 Itr: 0\tEpoch: 151\tTrain Acc: 0.72548\n",
            "Itr: 0\tEpoch: 152\tTrain Acc: 0.731\tTest Acc: 0.5299\n",
            "[INFO] Saving soft labels at epoch 152\n",
            "Saving labels to .\\CIFAR100_NO_ZCA\\ConvNet\\../../soft_labels_epoch_152.pt\n",
            "2025-05-13 03:50:39.034424 Itr: 0\tEpoch: 152\tTrain Acc: 0.731\n",
            "Itr: 0\tEpoch: 153\tTrain Acc: 0.7412\tTest Acc: 0.5387\n",
            "[INFO] Saving soft labels at epoch 153\n",
            "Saving labels to .\\CIFAR100_NO_ZCA\\ConvNet\\../../soft_labels_epoch_153.pt\n",
            "2025-05-13 03:51:07.447959 Itr: 0\tEpoch: 153\tTrain Acc: 0.7412\n",
            "Itr: 0\tEpoch: 154\tTrain Acc: 0.72806\tTest Acc: 0.5278\n",
            "[INFO] Saving soft labels at epoch 154\n",
            "Saving labels to .\\CIFAR100_NO_ZCA\\ConvNet\\../../soft_labels_epoch_154.pt\n",
            "2025-05-13 03:51:36.103599 Itr: 0\tEpoch: 154\tTrain Acc: 0.72806\n",
            "Itr: 0\tEpoch: 155\tTrain Acc: 0.73456\tTest Acc: 0.5333\n",
            "[INFO] Saving soft labels at epoch 155\n",
            "Saving labels to .\\CIFAR100_NO_ZCA\\ConvNet\\../../soft_labels_epoch_155.pt\n",
            "2025-05-13 03:52:05.015145 Itr: 0\tEpoch: 155\tTrain Acc: 0.73456\n",
            "Itr: 0\tEpoch: 156\tTrain Acc: 0.74942\tTest Acc: 0.5337\n",
            "[INFO] Saving soft labels at epoch 156\n",
            "Saving labels to .\\CIFAR100_NO_ZCA\\ConvNet\\../../soft_labels_epoch_156.pt\n",
            "2025-05-13 03:52:36.349809 Itr: 0\tEpoch: 156\tTrain Acc: 0.74942\n",
            "Itr: 0\tEpoch: 157\tTrain Acc: 0.74768\tTest Acc: 0.5297\n",
            "[INFO] Saving soft labels at epoch 157\n",
            "Saving labels to .\\CIFAR100_NO_ZCA\\ConvNet\\../../soft_labels_epoch_157.pt\n",
            "2025-05-13 03:53:07.385425 Itr: 0\tEpoch: 157\tTrain Acc: 0.74768\n",
            "Itr: 0\tEpoch: 158\tTrain Acc: 0.73304\tTest Acc: 0.5275\n",
            "[INFO] Saving soft labels at epoch 158\n",
            "Saving labels to .\\CIFAR100_NO_ZCA\\ConvNet\\../../soft_labels_epoch_158.pt\n",
            "2025-05-13 03:53:37.590134 Itr: 0\tEpoch: 158\tTrain Acc: 0.73304\n",
            "Itr: 0\tEpoch: 159\tTrain Acc: 0.74338\tTest Acc: 0.5295\n",
            "[INFO] Saving soft labels at epoch 159\n",
            "Saving labels to .\\CIFAR100_NO_ZCA\\ConvNet\\../../soft_labels_epoch_159.pt\n",
            "2025-05-13 03:54:06.299051 Itr: 0\tEpoch: 159\tTrain Acc: 0.74338\n",
            "Itr: 0\tEpoch: 160\tTrain Acc: 0.74436\tTest Acc: 0.5328\n",
            "[INFO] Saving soft labels at epoch 160\n",
            "Saving labels to .\\CIFAR100_NO_ZCA\\ConvNet\\../../soft_labels_epoch_160.pt\n",
            "2025-05-13 03:54:35.813514 Itr: 0\tEpoch: 160\tTrain Acc: 0.74436\n",
            "Itr: 0\tEpoch: 161\tTrain Acc: 0.74614\tTest Acc: 0.535\n",
            "[INFO] Saving soft labels at epoch 161\n",
            "Saving labels to .\\CIFAR100_NO_ZCA\\ConvNet\\../../soft_labels_epoch_161.pt\n",
            "2025-05-13 03:55:03.540794 Itr: 0\tEpoch: 161\tTrain Acc: 0.74614\n",
            "Itr: 0\tEpoch: 162\tTrain Acc: 0.74436\tTest Acc: 0.5316\n",
            "[INFO] Saving soft labels at epoch 162\n",
            "Saving labels to .\\CIFAR100_NO_ZCA\\ConvNet\\../../soft_labels_epoch_162.pt\n",
            "2025-05-13 03:55:31.687605 Itr: 0\tEpoch: 162\tTrain Acc: 0.74436\n",
            "Itr: 0\tEpoch: 163\tTrain Acc: 0.73984\tTest Acc: 0.5293\n",
            "[INFO] Saving soft labels at epoch 163\n",
            "Saving labels to .\\CIFAR100_NO_ZCA\\ConvNet\\../../soft_labels_epoch_163.pt\n",
            "2025-05-13 03:56:01.617482 Itr: 0\tEpoch: 163\tTrain Acc: 0.73984\n",
            "Itr: 0\tEpoch: 164\tTrain Acc: 0.74868\tTest Acc: 0.5351\n",
            "[INFO] Saving soft labels at epoch 164\n",
            "Saving labels to .\\CIFAR100_NO_ZCA\\ConvNet\\../../soft_labels_epoch_164.pt\n",
            "2025-05-13 03:56:33.066996 Itr: 0\tEpoch: 164\tTrain Acc: 0.74868\n",
            "Itr: 0\tEpoch: 165\tTrain Acc: 0.74526\tTest Acc: 0.5276\n",
            "[INFO] Saving soft labels at epoch 165\n",
            "Saving labels to .\\CIFAR100_NO_ZCA\\ConvNet\\../../soft_labels_epoch_165.pt\n",
            "2025-05-13 03:57:03.508652 Itr: 0\tEpoch: 165\tTrain Acc: 0.74526\n",
            "Itr: 0\tEpoch: 166\tTrain Acc: 0.75812\tTest Acc: 0.5373\n",
            "[INFO] Saving soft labels at epoch 166\n",
            "Saving labels to .\\CIFAR100_NO_ZCA\\ConvNet\\../../soft_labels_epoch_166.pt\n",
            "2025-05-13 03:57:32.231271 Itr: 0\tEpoch: 166\tTrain Acc: 0.75812\n",
            "Itr: 0\tEpoch: 167\tTrain Acc: 0.75994\tTest Acc: 0.5311\n",
            "[INFO] Saving soft labels at epoch 167\n",
            "Saving labels to .\\CIFAR100_NO_ZCA\\ConvNet\\../../soft_labels_epoch_167.pt\n",
            "2025-05-13 03:58:02.722891 Itr: 0\tEpoch: 167\tTrain Acc: 0.75994\n",
            "Itr: 0\tEpoch: 168\tTrain Acc: 0.75576\tTest Acc: 0.5374\n",
            "[INFO] Saving soft labels at epoch 168\n",
            "Saving labels to .\\CIFAR100_NO_ZCA\\ConvNet\\../../soft_labels_epoch_168.pt\n",
            "2025-05-13 03:58:32.718110 Itr: 0\tEpoch: 168\tTrain Acc: 0.75576\n",
            "Itr: 0\tEpoch: 169\tTrain Acc: 0.75942\tTest Acc: 0.5291\n",
            "[INFO] Saving soft labels at epoch 169\n",
            "Saving labels to .\\CIFAR100_NO_ZCA\\ConvNet\\../../soft_labels_epoch_169.pt\n",
            "2025-05-13 03:59:02.722222 Itr: 0\tEpoch: 169\tTrain Acc: 0.75942\n",
            "Itr: 0\tEpoch: 170\tTrain Acc: 0.74564\tTest Acc: 0.5281\n",
            "[INFO] Saving soft labels at epoch 170\n",
            "Saving labels to .\\CIFAR100_NO_ZCA\\ConvNet\\../../soft_labels_epoch_170.pt\n",
            "2025-05-13 03:59:31.653290 Itr: 0\tEpoch: 170\tTrain Acc: 0.74564\n",
            "Itr: 0\tEpoch: 171\tTrain Acc: 0.75738\tTest Acc: 0.5375\n",
            "[INFO] Saving soft labels at epoch 171\n",
            "Saving labels to .\\CIFAR100_NO_ZCA\\ConvNet\\../../soft_labels_epoch_171.pt\n",
            "2025-05-13 04:00:01.695408 Itr: 0\tEpoch: 171\tTrain Acc: 0.75738\n",
            "Itr: 0\tEpoch: 172\tTrain Acc: 0.74952\tTest Acc: 0.54\n",
            "[INFO] Saving soft labels at epoch 172\n",
            "Saving labels to .\\CIFAR100_NO_ZCA\\ConvNet\\../../soft_labels_epoch_172.pt\n",
            "2025-05-13 04:00:44.568112 Itr: 0\tEpoch: 172\tTrain Acc: 0.74952\n",
            "Itr: 0\tEpoch: 173\tTrain Acc: 0.75914\tTest Acc: 0.5343\n",
            "[INFO] Saving soft labels at epoch 173\n",
            "Saving labels to .\\CIFAR100_NO_ZCA\\ConvNet\\../../soft_labels_epoch_173.pt\n",
            "2025-05-13 04:01:13.642196 Itr: 0\tEpoch: 173\tTrain Acc: 0.75914\n",
            "Itr: 0\tEpoch: 174\tTrain Acc: 0.7641\tTest Acc: 0.5353\n",
            "[INFO] Saving soft labels at epoch 174\n",
            "Saving labels to .\\CIFAR100_NO_ZCA\\ConvNet\\../../soft_labels_epoch_174.pt\n",
            "2025-05-13 04:01:42.448902 Itr: 0\tEpoch: 174\tTrain Acc: 0.7641\n",
            "Itr: 0\tEpoch: 175\tTrain Acc: 0.7747\tTest Acc: 0.5406\n",
            "[INFO] Saving soft labels at epoch 175\n",
            "Saving labels to .\\CIFAR100_NO_ZCA\\ConvNet\\../../soft_labels_epoch_175.pt\n",
            "2025-05-13 04:02:11.212248 Itr: 0\tEpoch: 175\tTrain Acc: 0.7747\n",
            "Itr: 0\tEpoch: 176\tTrain Acc: 0.76356\tTest Acc: 0.5382\n",
            "[INFO] Saving soft labels at epoch 176\n",
            "Saving labels to .\\CIFAR100_NO_ZCA\\ConvNet\\../../soft_labels_epoch_176.pt\n",
            "2025-05-13 04:02:40.101398 Itr: 0\tEpoch: 176\tTrain Acc: 0.76356\n",
            "Itr: 0\tEpoch: 177\tTrain Acc: 0.76608\tTest Acc: 0.5269\n",
            "[INFO] Saving soft labels at epoch 177\n",
            "Saving labels to .\\CIFAR100_NO_ZCA\\ConvNet\\../../soft_labels_epoch_177.pt\n",
            "2025-05-13 04:03:09.010133 Itr: 0\tEpoch: 177\tTrain Acc: 0.76608\n",
            "Itr: 0\tEpoch: 178\tTrain Acc: 0.76158\tTest Acc: 0.5293\n",
            "[INFO] Saving soft labels at epoch 178\n",
            "Saving labels to .\\CIFAR100_NO_ZCA\\ConvNet\\../../soft_labels_epoch_178.pt\n",
            "2025-05-13 04:03:38.422146 Itr: 0\tEpoch: 178\tTrain Acc: 0.76158\n",
            "Itr: 0\tEpoch: 179\tTrain Acc: 0.77376\tTest Acc: 0.5419\n",
            "[INFO] Saving soft labels at epoch 179\n",
            "Saving labels to .\\CIFAR100_NO_ZCA\\ConvNet\\../../soft_labels_epoch_179.pt\n",
            "2025-05-13 04:04:08.147498 Itr: 0\tEpoch: 179\tTrain Acc: 0.77376\n",
            "Itr: 0\tEpoch: 180\tTrain Acc: 0.77472\tTest Acc: 0.5319\n",
            "[INFO] Saving soft labels at epoch 180\n",
            "Saving labels to .\\CIFAR100_NO_ZCA\\ConvNet\\../../soft_labels_epoch_180.pt\n",
            "2025-05-13 04:04:38.723895 Itr: 0\tEpoch: 180\tTrain Acc: 0.77472\n",
            "Itr: 0\tEpoch: 181\tTrain Acc: 0.76872\tTest Acc: 0.5327\n",
            "[INFO] Saving soft labels at epoch 181\n",
            "Saving labels to .\\CIFAR100_NO_ZCA\\ConvNet\\../../soft_labels_epoch_181.pt\n",
            "2025-05-13 04:05:07.573358 Itr: 0\tEpoch: 181\tTrain Acc: 0.76872\n",
            "Itr: 0\tEpoch: 182\tTrain Acc: 0.76854\tTest Acc: 0.5285\n",
            "[INFO] Saving soft labels at epoch 182\n",
            "Saving labels to .\\CIFAR100_NO_ZCA\\ConvNet\\../../soft_labels_epoch_182.pt\n",
            "2025-05-13 04:05:36.095975 Itr: 0\tEpoch: 182\tTrain Acc: 0.76854\n",
            "Itr: 0\tEpoch: 183\tTrain Acc: 0.775\tTest Acc: 0.5336\n",
            "[INFO] Saving soft labels at epoch 183\n",
            "Saving labels to .\\CIFAR100_NO_ZCA\\ConvNet\\../../soft_labels_epoch_183.pt\n",
            "2025-05-13 04:06:04.828395 Itr: 0\tEpoch: 183\tTrain Acc: 0.775\n",
            "Itr: 0\tEpoch: 184\tTrain Acc: 0.76652\tTest Acc: 0.5247\n",
            "[INFO] Saving soft labels at epoch 184\n",
            "Saving labels to .\\CIFAR100_NO_ZCA\\ConvNet\\../../soft_labels_epoch_184.pt\n",
            "2025-05-13 04:06:35.727911 Itr: 0\tEpoch: 184\tTrain Acc: 0.76652\n",
            "Itr: 0\tEpoch: 185\tTrain Acc: 0.7707\tTest Acc: 0.5345\n",
            "[INFO] Saving soft labels at epoch 185\n",
            "Saving labels to .\\CIFAR100_NO_ZCA\\ConvNet\\../../soft_labels_epoch_185.pt\n",
            "2025-05-13 04:07:04.895728 Itr: 0\tEpoch: 185\tTrain Acc: 0.7707\n",
            "Itr: 0\tEpoch: 186\tTrain Acc: 0.76848\tTest Acc: 0.5422\n",
            "[INFO] Saving soft labels at epoch 186\n",
            "Saving labels to .\\CIFAR100_NO_ZCA\\ConvNet\\../../soft_labels_epoch_186.pt\n",
            "2025-05-13 04:07:33.843619 Itr: 0\tEpoch: 186\tTrain Acc: 0.76848\n",
            "Itr: 0\tEpoch: 187\tTrain Acc: 0.78148\tTest Acc: 0.5363\n",
            "[INFO] Saving soft labels at epoch 187\n",
            "Saving labels to .\\CIFAR100_NO_ZCA\\ConvNet\\../../soft_labels_epoch_187.pt\n",
            "2025-05-13 04:08:04.699176 Itr: 0\tEpoch: 187\tTrain Acc: 0.78148\n",
            "Itr: 0\tEpoch: 188\tTrain Acc: 0.77488\tTest Acc: 0.5345\n",
            "[INFO] Saving soft labels at epoch 188\n",
            "Saving labels to .\\CIFAR100_NO_ZCA\\ConvNet\\../../soft_labels_epoch_188.pt\n",
            "2025-05-13 04:08:33.094889 Itr: 0\tEpoch: 188\tTrain Acc: 0.77488\n",
            "Itr: 0\tEpoch: 189\tTrain Acc: 0.78026\tTest Acc: 0.5316\n",
            "[INFO] Saving soft labels at epoch 189\n",
            "Saving labels to .\\CIFAR100_NO_ZCA\\ConvNet\\../../soft_labels_epoch_189.pt\n",
            "2025-05-13 04:09:02.602476 Itr: 0\tEpoch: 189\tTrain Acc: 0.78026\n",
            "Itr: 0\tEpoch: 190\tTrain Acc: 0.779\tTest Acc: 0.5239\n",
            "[INFO] Saving soft labels at epoch 190\n",
            "Saving labels to .\\CIFAR100_NO_ZCA\\ConvNet\\../../soft_labels_epoch_190.pt\n",
            "2025-05-13 04:09:32.298358 Itr: 0\tEpoch: 190\tTrain Acc: 0.779\n",
            "Itr: 0\tEpoch: 191\tTrain Acc: 0.78384\tTest Acc: 0.5424\n",
            "[INFO] Saving soft labels at epoch 191\n",
            "Saving labels to .\\CIFAR100_NO_ZCA\\ConvNet\\../../soft_labels_epoch_191.pt\n",
            "2025-05-13 04:10:03.273917 Itr: 0\tEpoch: 191\tTrain Acc: 0.78384\n",
            "Itr: 0\tEpoch: 192\tTrain Acc: 0.78702\tTest Acc: 0.5455\n",
            "[INFO] Saving soft labels at epoch 192\n",
            "Saving labels to .\\CIFAR100_NO_ZCA\\ConvNet\\../../soft_labels_epoch_192.pt\n",
            "2025-05-13 04:10:33.395178 Itr: 0\tEpoch: 192\tTrain Acc: 0.78702\n",
            "Itr: 0\tEpoch: 193\tTrain Acc: 0.78358\tTest Acc: 0.5361\n",
            "[INFO] Saving soft labels at epoch 193\n",
            "Saving labels to .\\CIFAR100_NO_ZCA\\ConvNet\\../../soft_labels_epoch_193.pt\n",
            "2025-05-13 04:11:04.279852 Itr: 0\tEpoch: 193\tTrain Acc: 0.78358\n",
            "Itr: 0\tEpoch: 194\tTrain Acc: 0.77718\tTest Acc: 0.5302\n",
            "[INFO] Saving soft labels at epoch 194\n",
            "Saving labels to .\\CIFAR100_NO_ZCA\\ConvNet\\../../soft_labels_epoch_194.pt\n",
            "2025-05-13 04:11:33.434315 Itr: 0\tEpoch: 194\tTrain Acc: 0.77718\n",
            "Itr: 0\tEpoch: 195\tTrain Acc: 0.78796\tTest Acc: 0.5411\n",
            "[INFO] Saving soft labels at epoch 195\n",
            "Saving labels to .\\CIFAR100_NO_ZCA\\ConvNet\\../../soft_labels_epoch_195.pt\n",
            "2025-05-13 04:12:03.810336 Itr: 0\tEpoch: 195\tTrain Acc: 0.78796\n",
            "Itr: 0\tEpoch: 196\tTrain Acc: 0.78078\tTest Acc: 0.5407\n",
            "[INFO] Saving soft labels at epoch 196\n",
            "Saving labels to .\\CIFAR100_NO_ZCA\\ConvNet\\../../soft_labels_epoch_196.pt\n",
            "2025-05-13 04:12:32.798470 Itr: 0\tEpoch: 196\tTrain Acc: 0.78078\n",
            "Itr: 0\tEpoch: 197\tTrain Acc: 0.78548\tTest Acc: 0.5241\n",
            "[INFO] Saving soft labels at epoch 197\n",
            "Saving labels to .\\CIFAR100_NO_ZCA\\ConvNet\\../../soft_labels_epoch_197.pt\n",
            "2025-05-13 04:13:03.816705 Itr: 0\tEpoch: 197\tTrain Acc: 0.78548\n",
            "Itr: 0\tEpoch: 198\tTrain Acc: 0.78184\tTest Acc: 0.5364\n",
            "[INFO] Saving soft labels at epoch 198\n",
            "Saving labels to .\\CIFAR100_NO_ZCA\\ConvNet\\../../soft_labels_epoch_198.pt\n",
            "2025-05-13 04:13:32.180726 Itr: 0\tEpoch: 198\tTrain Acc: 0.78184\n",
            "Itr: 0\tEpoch: 199\tTrain Acc: 0.79144\tTest Acc: 0.5328\n",
            "[INFO] Saving soft labels at epoch 199\n",
            "Saving labels to .\\CIFAR100_NO_ZCA\\ConvNet\\../../soft_labels_epoch_199.pt\n",
            "2025-05-13 04:14:02.190547 Itr: 0\tEpoch: 199\tTrain Acc: 0.79144\n",
            "Itr: 0\tEpoch: 200\tTrain Acc: 0.7908\tTest Acc: 0.5401\n",
            "[INFO] Saving soft labels at epoch 200\n",
            "Saving labels to .\\CIFAR100_NO_ZCA\\ConvNet\\../../soft_labels_epoch_200.pt\n",
            "2025-05-13 04:14:31.376218 Itr: 0\tEpoch: 200\tTrain Acc: 0.7908\n",
            "Itr: 0\tEpoch: 201\tTrain Acc: 0.78562\tTest Acc: 0.5349\n",
            "[INFO] Saving soft labels at epoch 201\n",
            "Saving labels to .\\CIFAR100_NO_ZCA\\ConvNet\\../../soft_labels_epoch_201.pt\n",
            "2025-05-13 04:15:01.803329 Itr: 0\tEpoch: 201\tTrain Acc: 0.78562\n",
            "Itr: 0\tEpoch: 202\tTrain Acc: 0.79174\tTest Acc: 0.541\n",
            "[INFO] Saving soft labels at epoch 202\n",
            "Saving labels to .\\CIFAR100_NO_ZCA\\ConvNet\\../../soft_labels_epoch_202.pt\n",
            "2025-05-13 04:15:44.619711 Itr: 0\tEpoch: 202\tTrain Acc: 0.79174\n",
            "Itr: 0\tEpoch: 203\tTrain Acc: 0.79352\tTest Acc: 0.5331\n",
            "[INFO] Saving soft labels at epoch 203\n",
            "Saving labels to .\\CIFAR100_NO_ZCA\\ConvNet\\../../soft_labels_epoch_203.pt\n",
            "2025-05-13 04:16:14.444386 Itr: 0\tEpoch: 203\tTrain Acc: 0.79352\n",
            "Itr: 0\tEpoch: 204\tTrain Acc: 0.78614\tTest Acc: 0.5355\n",
            "[INFO] Saving soft labels at epoch 204\n",
            "Saving labels to .\\CIFAR100_NO_ZCA\\ConvNet\\../../soft_labels_epoch_204.pt\n",
            "2025-05-13 04:16:43.993059 Itr: 0\tEpoch: 204\tTrain Acc: 0.78614\n",
            "Itr: 0\tEpoch: 205\tTrain Acc: 0.79548\tTest Acc: 0.5447\n",
            "[INFO] Saving soft labels at epoch 205\n",
            "Saving labels to .\\CIFAR100_NO_ZCA\\ConvNet\\../../soft_labels_epoch_205.pt\n",
            "2025-05-13 04:17:13.455414 Itr: 0\tEpoch: 205\tTrain Acc: 0.79548\n",
            "Itr: 0\tEpoch: 206\tTrain Acc: 0.7859\tTest Acc: 0.5345\n",
            "[INFO] Saving soft labels at epoch 206\n",
            "Saving labels to .\\CIFAR100_NO_ZCA\\ConvNet\\../../soft_labels_epoch_206.pt\n",
            "2025-05-13 04:17:43.652412 Itr: 0\tEpoch: 206\tTrain Acc: 0.7859\n",
            "Itr: 0\tEpoch: 207\tTrain Acc: 0.78098\tTest Acc: 0.535\n",
            "[INFO] Saving soft labels at epoch 207\n",
            "Saving labels to .\\CIFAR100_NO_ZCA\\ConvNet\\../../soft_labels_epoch_207.pt\n",
            "2025-05-13 04:18:14.020918 Itr: 0\tEpoch: 207\tTrain Acc: 0.78098\n",
            "Itr: 0\tEpoch: 208\tTrain Acc: 0.78406\tTest Acc: 0.536\n",
            "[INFO] Saving soft labels at epoch 208\n",
            "Saving labels to .\\CIFAR100_NO_ZCA\\ConvNet\\../../soft_labels_epoch_208.pt\n",
            "2025-05-13 04:18:43.725012 Itr: 0\tEpoch: 208\tTrain Acc: 0.78406\n",
            "Itr: 0\tEpoch: 209\tTrain Acc: 0.7956\tTest Acc: 0.5389\n",
            "[INFO] Saving soft labels at epoch 209\n",
            "Saving labels to .\\CIFAR100_NO_ZCA\\ConvNet\\../../soft_labels_epoch_209.pt\n",
            "2025-05-13 04:19:12.300323 Itr: 0\tEpoch: 209\tTrain Acc: 0.7956\n",
            "Itr: 0\tEpoch: 210\tTrain Acc: 0.79094\tTest Acc: 0.5361\n",
            "[INFO] Saving soft labels at epoch 210\n",
            "Saving labels to .\\CIFAR100_NO_ZCA\\ConvNet\\../../soft_labels_epoch_210.pt\n",
            "2025-05-13 04:19:41.282250 Itr: 0\tEpoch: 210\tTrain Acc: 0.79094\n",
            "Itr: 0\tEpoch: 211\tTrain Acc: 0.79908\tTest Acc: 0.547\n",
            "[INFO] Saving soft labels at epoch 211\n",
            "Saving labels to .\\CIFAR100_NO_ZCA\\ConvNet\\../../soft_labels_epoch_211.pt\n",
            "2025-05-13 04:20:10.065581 Itr: 0\tEpoch: 211\tTrain Acc: 0.79908\n",
            "Itr: 0\tEpoch: 212\tTrain Acc: 0.8019\tTest Acc: 0.5397\n",
            "[INFO] Saving soft labels at epoch 212\n",
            "Saving labels to .\\CIFAR100_NO_ZCA\\ConvNet\\../../soft_labels_epoch_212.pt\n",
            "2025-05-13 04:20:39.009262 Itr: 0\tEpoch: 212\tTrain Acc: 0.8019\n",
            "Itr: 0\tEpoch: 213\tTrain Acc: 0.80156\tTest Acc: 0.5432\n",
            "[INFO] Saving soft labels at epoch 213\n",
            "Saving labels to .\\CIFAR100_NO_ZCA\\ConvNet\\../../soft_labels_epoch_213.pt\n",
            "2025-05-13 04:21:08.047262 Itr: 0\tEpoch: 213\tTrain Acc: 0.80156\n",
            "Itr: 0\tEpoch: 214\tTrain Acc: 0.79662\tTest Acc: 0.5364\n",
            "[INFO] Saving soft labels at epoch 214\n",
            "Saving labels to .\\CIFAR100_NO_ZCA\\ConvNet\\../../soft_labels_epoch_214.pt\n",
            "2025-05-13 04:21:37.032853 Itr: 0\tEpoch: 214\tTrain Acc: 0.79662\n",
            "Itr: 0\tEpoch: 215\tTrain Acc: 0.80922\tTest Acc: 0.5263\n",
            "[INFO] Saving soft labels at epoch 215\n",
            "Saving labels to .\\CIFAR100_NO_ZCA\\ConvNet\\../../soft_labels_epoch_215.pt\n",
            "2025-05-13 04:22:05.862646 Itr: 0\tEpoch: 215\tTrain Acc: 0.80922\n",
            "Itr: 0\tEpoch: 216\tTrain Acc: 0.79456\tTest Acc: 0.5351\n",
            "[INFO] Saving soft labels at epoch 216\n",
            "Saving labels to .\\CIFAR100_NO_ZCA\\ConvNet\\../../soft_labels_epoch_216.pt\n",
            "2025-05-13 04:22:35.833313 Itr: 0\tEpoch: 216\tTrain Acc: 0.79456\n",
            "Itr: 0\tEpoch: 217\tTrain Acc: 0.80706\tTest Acc: 0.5357\n",
            "[INFO] Saving soft labels at epoch 217\n",
            "Saving labels to .\\CIFAR100_NO_ZCA\\ConvNet\\../../soft_labels_epoch_217.pt\n",
            "2025-05-13 04:23:08.469396 Itr: 0\tEpoch: 217\tTrain Acc: 0.80706\n",
            "Itr: 0\tEpoch: 218\tTrain Acc: 0.8054\tTest Acc: 0.5351\n",
            "[INFO] Saving soft labels at epoch 218\n",
            "Saving labels to .\\CIFAR100_NO_ZCA\\ConvNet\\../../soft_labels_epoch_218.pt\n",
            "2025-05-13 04:23:37.984413 Itr: 0\tEpoch: 218\tTrain Acc: 0.8054\n",
            "Itr: 0\tEpoch: 219\tTrain Acc: 0.79998\tTest Acc: 0.5442\n",
            "[INFO] Saving soft labels at epoch 219\n",
            "Saving labels to .\\CIFAR100_NO_ZCA\\ConvNet\\../../soft_labels_epoch_219.pt\n",
            "2025-05-13 04:24:06.294987 Itr: 0\tEpoch: 219\tTrain Acc: 0.79998\n",
            "Itr: 0\tEpoch: 220\tTrain Acc: 0.80172\tTest Acc: 0.535\n",
            "[INFO] Saving soft labels at epoch 220\n",
            "Saving labels to .\\CIFAR100_NO_ZCA\\ConvNet\\../../soft_labels_epoch_220.pt\n",
            "2025-05-13 04:24:35.004806 Itr: 0\tEpoch: 220\tTrain Acc: 0.80172\n",
            "Itr: 0\tEpoch: 221\tTrain Acc: 0.80424\tTest Acc: 0.5223\n",
            "[INFO] Saving soft labels at epoch 221\n",
            "Saving labels to .\\CIFAR100_NO_ZCA\\ConvNet\\../../soft_labels_epoch_221.pt\n",
            "2025-05-13 04:25:04.197591 Itr: 0\tEpoch: 221\tTrain Acc: 0.80424\n",
            "Itr: 0\tEpoch: 222\tTrain Acc: 0.80962\tTest Acc: 0.5442\n",
            "[INFO] Saving soft labels at epoch 222\n",
            "Saving labels to .\\CIFAR100_NO_ZCA\\ConvNet\\../../soft_labels_epoch_222.pt\n",
            "2025-05-13 04:25:33.125376 Itr: 0\tEpoch: 222\tTrain Acc: 0.80962\n",
            "Itr: 0\tEpoch: 223\tTrain Acc: 0.81538\tTest Acc: 0.5303\n",
            "[INFO] Saving soft labels at epoch 223\n",
            "Saving labels to .\\CIFAR100_NO_ZCA\\ConvNet\\../../soft_labels_epoch_223.pt\n",
            "2025-05-13 04:26:02.535903 Itr: 0\tEpoch: 223\tTrain Acc: 0.81538\n",
            "Itr: 0\tEpoch: 224\tTrain Acc: 0.80364\tTest Acc: 0.5424\n",
            "[INFO] Saving soft labels at epoch 224\n",
            "Saving labels to .\\CIFAR100_NO_ZCA\\ConvNet\\../../soft_labels_epoch_224.pt\n",
            "2025-05-13 04:26:35.669446 Itr: 0\tEpoch: 224\tTrain Acc: 0.80364\n",
            "Itr: 0\tEpoch: 225\tTrain Acc: 0.80858\tTest Acc: 0.537\n",
            "[INFO] Saving soft labels at epoch 225\n",
            "Saving labels to .\\CIFAR100_NO_ZCA\\ConvNet\\../../soft_labels_epoch_225.pt\n",
            "2025-05-13 04:27:05.151961 Itr: 0\tEpoch: 225\tTrain Acc: 0.80858\n",
            "Itr: 0\tEpoch: 226\tTrain Acc: 0.80346\tTest Acc: 0.5329\n",
            "[INFO] Saving soft labels at epoch 226\n",
            "Saving labels to .\\CIFAR100_NO_ZCA\\ConvNet\\../../soft_labels_epoch_226.pt\n",
            "2025-05-13 04:27:33.767618 Itr: 0\tEpoch: 226\tTrain Acc: 0.80346\n",
            "Itr: 0\tEpoch: 227\tTrain Acc: 0.81054\tTest Acc: 0.5223\n",
            "[INFO] Saving soft labels at epoch 227\n",
            "Saving labels to .\\CIFAR100_NO_ZCA\\ConvNet\\../../soft_labels_epoch_227.pt\n",
            "2025-05-13 04:28:04.826850 Itr: 0\tEpoch: 227\tTrain Acc: 0.81054\n",
            "Itr: 0\tEpoch: 228\tTrain Acc: 0.80246\tTest Acc: 0.5345\n",
            "[INFO] Saving soft labels at epoch 228\n",
            "Saving labels to .\\CIFAR100_NO_ZCA\\ConvNet\\../../soft_labels_epoch_228.pt\n",
            "2025-05-13 04:28:33.935305 Itr: 0\tEpoch: 228\tTrain Acc: 0.80246\n",
            "Itr: 0\tEpoch: 229\tTrain Acc: 0.81478\tTest Acc: 0.5424\n",
            "[INFO] Saving soft labels at epoch 229\n",
            "Saving labels to .\\CIFAR100_NO_ZCA\\ConvNet\\../../soft_labels_epoch_229.pt\n",
            "2025-05-13 04:29:04.101766 Itr: 0\tEpoch: 229\tTrain Acc: 0.81478\n",
            "Itr: 0\tEpoch: 230\tTrain Acc: 0.8122\tTest Acc: 0.5485\n",
            "[INFO] Saving soft labels at epoch 230\n",
            "Saving labels to .\\CIFAR100_NO_ZCA\\ConvNet\\../../soft_labels_epoch_230.pt\n",
            "2025-05-13 04:29:33.411333 Itr: 0\tEpoch: 230\tTrain Acc: 0.8122\n",
            "Itr: 0\tEpoch: 231\tTrain Acc: 0.812\tTest Acc: 0.5494\n",
            "[INFO] Saving soft labels at epoch 231\n",
            "Saving labels to .\\CIFAR100_NO_ZCA\\ConvNet\\../../soft_labels_epoch_231.pt\n",
            "2025-05-13 04:30:02.897796 Itr: 0\tEpoch: 231\tTrain Acc: 0.812\n",
            "Itr: 0\tEpoch: 232\tTrain Acc: 0.81698\tTest Acc: 0.5353\n",
            "[INFO] Saving soft labels at epoch 232\n",
            "Saving labels to .\\CIFAR100_NO_ZCA\\ConvNet\\../../soft_labels_epoch_232.pt\n",
            "2025-05-13 04:30:45.706157 Itr: 0\tEpoch: 232\tTrain Acc: 0.81698\n",
            "Itr: 0\tEpoch: 233\tTrain Acc: 0.81842\tTest Acc: 0.5435\n",
            "[INFO] Saving soft labels at epoch 233\n",
            "Saving labels to .\\CIFAR100_NO_ZCA\\ConvNet\\../../soft_labels_epoch_233.pt\n",
            "2025-05-13 04:31:14.401507 Itr: 0\tEpoch: 233\tTrain Acc: 0.81842\n",
            "Itr: 0\tEpoch: 234\tTrain Acc: 0.80386\tTest Acc: 0.542\n",
            "[INFO] Saving soft labels at epoch 234\n",
            "Saving labels to .\\CIFAR100_NO_ZCA\\ConvNet\\../../soft_labels_epoch_234.pt\n",
            "2025-05-13 04:31:43.879451 Itr: 0\tEpoch: 234\tTrain Acc: 0.80386\n",
            "Itr: 0\tEpoch: 235\tTrain Acc: 0.81508\tTest Acc: 0.5381\n",
            "[INFO] Saving soft labels at epoch 235\n",
            "Saving labels to .\\CIFAR100_NO_ZCA\\ConvNet\\../../soft_labels_epoch_235.pt\n",
            "2025-05-13 04:32:13.082094 Itr: 0\tEpoch: 235\tTrain Acc: 0.81508\n",
            "Itr: 0\tEpoch: 236\tTrain Acc: 0.81174\tTest Acc: 0.5271\n",
            "[INFO] Saving soft labels at epoch 236\n",
            "Saving labels to .\\CIFAR100_NO_ZCA\\ConvNet\\../../soft_labels_epoch_236.pt\n",
            "2025-05-13 04:32:42.858707 Itr: 0\tEpoch: 236\tTrain Acc: 0.81174\n",
            "Itr: 0\tEpoch: 237\tTrain Acc: 0.81016\tTest Acc: 0.5415\n",
            "[INFO] Saving soft labels at epoch 237\n",
            "Saving labels to .\\CIFAR100_NO_ZCA\\ConvNet\\../../soft_labels_epoch_237.pt\n",
            "2025-05-13 04:33:12.063192 Itr: 0\tEpoch: 237\tTrain Acc: 0.81016\n",
            "Itr: 0\tEpoch: 238\tTrain Acc: 0.80572\tTest Acc: 0.5355\n",
            "[INFO] Saving soft labels at epoch 238\n",
            "Saving labels to .\\CIFAR100_NO_ZCA\\ConvNet\\../../soft_labels_epoch_238.pt\n",
            "2025-05-13 04:33:41.026766 Itr: 0\tEpoch: 238\tTrain Acc: 0.80572\n",
            "Itr: 0\tEpoch: 239\tTrain Acc: 0.8175\tTest Acc: 0.5407\n",
            "[INFO] Saving soft labels at epoch 239\n",
            "Saving labels to .\\CIFAR100_NO_ZCA\\ConvNet\\../../soft_labels_epoch_239.pt\n",
            "2025-05-13 04:34:11.021439 Itr: 0\tEpoch: 239\tTrain Acc: 0.8175\n",
            "Itr: 0\tEpoch: 240\tTrain Acc: 0.81898\tTest Acc: 0.541\n",
            "[INFO] Saving soft labels at epoch 240\n",
            "Saving labels to .\\CIFAR100_NO_ZCA\\ConvNet\\../../soft_labels_epoch_240.pt\n",
            "2025-05-13 04:34:40.253910 Itr: 0\tEpoch: 240\tTrain Acc: 0.81898\n",
            "Itr: 0\tEpoch: 241\tTrain Acc: 0.819\tTest Acc: 0.543\n",
            "[INFO] Saving soft labels at epoch 241\n",
            "Saving labels to .\\CIFAR100_NO_ZCA\\ConvNet\\../../soft_labels_epoch_241.pt\n",
            "2025-05-13 04:35:09.213498 Itr: 0\tEpoch: 241\tTrain Acc: 0.819\n",
            "Itr: 0\tEpoch: 242\tTrain Acc: 0.82266\tTest Acc: 0.5427\n",
            "[INFO] Saving soft labels at epoch 242\n",
            "Saving labels to .\\CIFAR100_NO_ZCA\\ConvNet\\../../soft_labels_epoch_242.pt\n",
            "2025-05-13 04:35:38.157155 Itr: 0\tEpoch: 242\tTrain Acc: 0.82266\n",
            "Itr: 0\tEpoch: 243\tTrain Acc: 0.82548\tTest Acc: 0.5394\n",
            "[INFO] Saving soft labels at epoch 243\n",
            "Saving labels to .\\CIFAR100_NO_ZCA\\ConvNet\\../../soft_labels_epoch_243.pt\n",
            "2025-05-13 04:36:07.162354 Itr: 0\tEpoch: 243\tTrain Acc: 0.82548\n",
            "Itr: 0\tEpoch: 244\tTrain Acc: 0.8152\tTest Acc: 0.5327\n",
            "[INFO] Saving soft labels at epoch 244\n",
            "Saving labels to .\\CIFAR100_NO_ZCA\\ConvNet\\../../soft_labels_epoch_244.pt\n",
            "2025-05-13 04:36:36.745253 Itr: 0\tEpoch: 244\tTrain Acc: 0.8152\n",
            "Itr: 0\tEpoch: 245\tTrain Acc: 0.82168\tTest Acc: 0.5384\n",
            "[INFO] Saving soft labels at epoch 245\n",
            "Saving labels to .\\CIFAR100_NO_ZCA\\ConvNet\\../../soft_labels_epoch_245.pt\n",
            "2025-05-13 04:37:05.390730 Itr: 0\tEpoch: 245\tTrain Acc: 0.82168\n",
            "Itr: 0\tEpoch: 246\tTrain Acc: 0.81488\tTest Acc: 0.5393\n",
            "[INFO] Saving soft labels at epoch 246\n",
            "Saving labels to .\\CIFAR100_NO_ZCA\\ConvNet\\../../soft_labels_epoch_246.pt\n",
            "2025-05-13 04:37:35.044001 Itr: 0\tEpoch: 246\tTrain Acc: 0.81488\n",
            "Itr: 0\tEpoch: 247\tTrain Acc: 0.8235\tTest Acc: 0.5415\n",
            "[INFO] Saving soft labels at epoch 247\n",
            "Saving labels to .\\CIFAR100_NO_ZCA\\ConvNet\\../../soft_labels_epoch_247.pt\n",
            "2025-05-13 04:38:05.038168 Itr: 0\tEpoch: 247\tTrain Acc: 0.8235\n",
            "Itr: 0\tEpoch: 248\tTrain Acc: 0.81776\tTest Acc: 0.5346\n",
            "[INFO] Saving soft labels at epoch 248\n",
            "Saving labels to .\\CIFAR100_NO_ZCA\\ConvNet\\../../soft_labels_epoch_248.pt\n",
            "2025-05-13 04:38:34.234606 Itr: 0\tEpoch: 248\tTrain Acc: 0.81776\n",
            "Itr: 0\tEpoch: 249\tTrain Acc: 0.83004\tTest Acc: 0.532\n",
            "[INFO] Saving soft labels at epoch 249\n",
            "Saving labels to .\\CIFAR100_NO_ZCA\\ConvNet\\../../soft_labels_epoch_249.pt\n",
            "2025-05-13 04:39:04.355627 Itr: 0\tEpoch: 249\tTrain Acc: 0.83004\n",
            "Itr: 0\tEpoch: 250\tTrain Acc: 0.80448\tTest Acc: 0.5338\n",
            "[INFO] Saving soft labels at epoch 250\n",
            "Saving labels to .\\CIFAR100_NO_ZCA\\ConvNet\\../../soft_labels_epoch_250.pt\n",
            "2025-05-13 04:39:33.586339 Itr: 0\tEpoch: 250\tTrain Acc: 0.80448\n",
            "Itr: 0\tEpoch: 251\tTrain Acc: 0.82596\tTest Acc: 0.5477\n",
            "[INFO] Saving soft labels at epoch 251\n",
            "Saving labels to .\\CIFAR100_NO_ZCA\\ConvNet\\../../soft_labels_epoch_251.pt\n",
            "2025-05-13 04:40:04.379621 Itr: 0\tEpoch: 251\tTrain Acc: 0.82596\n",
            "Itr: 0\tEpoch: 252\tTrain Acc: 0.82848\tTest Acc: 0.5313\n",
            "[INFO] Saving soft labels at epoch 252\n",
            "Saving labels to .\\CIFAR100_NO_ZCA\\ConvNet\\../../soft_labels_epoch_252.pt\n",
            "2025-05-13 04:40:34.322886 Itr: 0\tEpoch: 252\tTrain Acc: 0.82848\n",
            "Itr: 0\tEpoch: 253\tTrain Acc: 0.82978\tTest Acc: 0.5496\n",
            "[INFO] Saving soft labels at epoch 253\n",
            "Saving labels to .\\CIFAR100_NO_ZCA\\ConvNet\\../../soft_labels_epoch_253.pt\n",
            "2025-05-13 04:41:03.944193 Itr: 0\tEpoch: 253\tTrain Acc: 0.82978\n",
            "Itr: 0\tEpoch: 254\tTrain Acc: 0.81856\tTest Acc: 0.5394\n",
            "[INFO] Saving soft labels at epoch 254\n",
            "Saving labels to .\\CIFAR100_NO_ZCA\\ConvNet\\../../soft_labels_epoch_254.pt\n",
            "2025-05-13 04:41:33.337114 Itr: 0\tEpoch: 254\tTrain Acc: 0.81856\n",
            "Itr: 0\tEpoch: 255\tTrain Acc: 0.82336\tTest Acc: 0.5421\n",
            "[INFO] Saving soft labels at epoch 255\n",
            "Saving labels to .\\CIFAR100_NO_ZCA\\ConvNet\\../../soft_labels_epoch_255.pt\n",
            "2025-05-13 04:42:03.517155 Itr: 0\tEpoch: 255\tTrain Acc: 0.82336\n",
            "Itr: 0\tEpoch: 256\tTrain Acc: 0.83342\tTest Acc: 0.539\n",
            "[INFO] Saving soft labels at epoch 256\n",
            "Saving labels to .\\CIFAR100_NO_ZCA\\ConvNet\\../../soft_labels_epoch_256.pt\n",
            "2025-05-13 04:42:32.469676 Itr: 0\tEpoch: 256\tTrain Acc: 0.83342\n",
            "Itr: 0\tEpoch: 257\tTrain Acc: 0.81934\tTest Acc: 0.5275\n",
            "[INFO] Saving soft labels at epoch 257\n",
            "Saving labels to .\\CIFAR100_NO_ZCA\\ConvNet\\../../soft_labels_epoch_257.pt\n",
            "2025-05-13 04:43:03.273534 Itr: 0\tEpoch: 257\tTrain Acc: 0.81934\n",
            "Itr: 0\tEpoch: 258\tTrain Acc: 0.80938\tTest Acc: 0.5335\n",
            "[INFO] Saving soft labels at epoch 258\n",
            "Saving labels to .\\CIFAR100_NO_ZCA\\ConvNet\\../../soft_labels_epoch_258.pt\n",
            "2025-05-13 04:43:31.435910 Itr: 0\tEpoch: 258\tTrain Acc: 0.80938\n",
            "Itr: 0\tEpoch: 259\tTrain Acc: 0.82632\tTest Acc: 0.5463\n",
            "[INFO] Saving soft labels at epoch 259\n",
            "Saving labels to .\\CIFAR100_NO_ZCA\\ConvNet\\../../soft_labels_epoch_259.pt\n",
            "2025-05-13 04:44:00.758690 Itr: 0\tEpoch: 259\tTrain Acc: 0.82632\n",
            "Itr: 0\tEpoch: 260\tTrain Acc: 0.82114\tTest Acc: 0.5412\n",
            "[INFO] Saving soft labels at epoch 260\n",
            "Saving labels to .\\CIFAR100_NO_ZCA\\ConvNet\\../../soft_labels_epoch_260.pt\n",
            "2025-05-13 04:44:29.874197 Itr: 0\tEpoch: 260\tTrain Acc: 0.82114\n",
            "Itr: 0\tEpoch: 261\tTrain Acc: 0.83372\tTest Acc: 0.5267\n",
            "[INFO] Saving soft labels at epoch 261\n",
            "Saving labels to .\\CIFAR100_NO_ZCA\\ConvNet\\../../soft_labels_epoch_261.pt\n",
            "2025-05-13 04:45:01.964195 Itr: 0\tEpoch: 261\tTrain Acc: 0.83372\n",
            "Itr: 0\tEpoch: 262\tTrain Acc: 0.82696\tTest Acc: 0.5467\n",
            "[INFO] Saving soft labels at epoch 262\n",
            "Saving labels to .\\CIFAR100_NO_ZCA\\ConvNet\\../../soft_labels_epoch_262.pt\n",
            "2025-05-13 04:45:33.688434 Itr: 0\tEpoch: 262\tTrain Acc: 0.82696\n",
            "Itr: 0\tEpoch: 263\tTrain Acc: 0.82602\tTest Acc: 0.5428\n",
            "[INFO] Saving soft labels at epoch 263\n",
            "Saving labels to .\\CIFAR100_NO_ZCA\\ConvNet\\../../soft_labels_epoch_263.pt\n",
            "2025-05-13 04:46:16.899237 Itr: 0\tEpoch: 263\tTrain Acc: 0.82602\n",
            "Itr: 0\tEpoch: 264\tTrain Acc: 0.8283\tTest Acc: 0.5289\n",
            "[INFO] Saving soft labels at epoch 264\n",
            "Saving labels to .\\CIFAR100_NO_ZCA\\ConvNet\\../../soft_labels_epoch_264.pt\n",
            "2025-05-13 04:46:46.859358 Itr: 0\tEpoch: 264\tTrain Acc: 0.8283\n",
            "Itr: 0\tEpoch: 265\tTrain Acc: 0.83444\tTest Acc: 0.5468\n",
            "[INFO] Saving soft labels at epoch 265\n",
            "Saving labels to .\\CIFAR100_NO_ZCA\\ConvNet\\../../soft_labels_epoch_265.pt\n",
            "2025-05-13 04:47:15.906564 Itr: 0\tEpoch: 265\tTrain Acc: 0.83444\n",
            "Itr: 0\tEpoch: 266\tTrain Acc: 0.82346\tTest Acc: 0.5378\n",
            "[INFO] Saving soft labels at epoch 266\n",
            "Saving labels to .\\CIFAR100_NO_ZCA\\ConvNet\\../../soft_labels_epoch_266.pt\n",
            "2025-05-13 04:47:45.840919 Itr: 0\tEpoch: 266\tTrain Acc: 0.82346\n",
            "Itr: 0\tEpoch: 267\tTrain Acc: 0.82508\tTest Acc: 0.5403\n",
            "[INFO] Saving soft labels at epoch 267\n",
            "Saving labels to .\\CIFAR100_NO_ZCA\\ConvNet\\../../soft_labels_epoch_267.pt\n",
            "2025-05-13 04:48:14.422830 Itr: 0\tEpoch: 267\tTrain Acc: 0.82508\n",
            "Itr: 0\tEpoch: 268\tTrain Acc: 0.82662\tTest Acc: 0.5359\n",
            "[INFO] Saving soft labels at epoch 268\n",
            "Saving labels to .\\CIFAR100_NO_ZCA\\ConvNet\\../../soft_labels_epoch_268.pt\n",
            "2025-05-13 04:48:47.584920 Itr: 0\tEpoch: 268\tTrain Acc: 0.82662\n",
            "Itr: 0\tEpoch: 269\tTrain Acc: 0.83612\tTest Acc: 0.5451\n",
            "[INFO] Saving soft labels at epoch 269\n",
            "Saving labels to .\\CIFAR100_NO_ZCA\\ConvNet\\../../soft_labels_epoch_269.pt\n",
            "2025-05-13 04:49:16.588996 Itr: 0\tEpoch: 269\tTrain Acc: 0.83612\n",
            "Itr: 0\tEpoch: 270\tTrain Acc: 0.83096\tTest Acc: 0.5454\n",
            "[INFO] Saving soft labels at epoch 270\n",
            "Saving labels to .\\CIFAR100_NO_ZCA\\ConvNet\\../../soft_labels_epoch_270.pt\n",
            "2025-05-13 04:49:46.288508 Itr: 0\tEpoch: 270\tTrain Acc: 0.83096\n",
            "Itr: 0\tEpoch: 271\tTrain Acc: 0.8236\tTest Acc: 0.5318\n",
            "[INFO] Saving soft labels at epoch 271\n",
            "Saving labels to .\\CIFAR100_NO_ZCA\\ConvNet\\../../soft_labels_epoch_271.pt\n",
            "2025-05-13 04:50:14.778070 Itr: 0\tEpoch: 271\tTrain Acc: 0.8236\n",
            "Itr: 0\tEpoch: 272\tTrain Acc: 0.8296\tTest Acc: 0.5381\n",
            "[INFO] Saving soft labels at epoch 272\n",
            "Saving labels to .\\CIFAR100_NO_ZCA\\ConvNet\\../../soft_labels_epoch_272.pt\n",
            "2025-05-13 04:50:44.359173 Itr: 0\tEpoch: 272\tTrain Acc: 0.8296\n",
            "Itr: 0\tEpoch: 273\tTrain Acc: 0.83978\tTest Acc: 0.5286\n",
            "[INFO] Saving soft labels at epoch 273\n",
            "Saving labels to .\\CIFAR100_NO_ZCA\\ConvNet\\../../soft_labels_epoch_273.pt\n",
            "2025-05-13 04:51:13.765039 Itr: 0\tEpoch: 273\tTrain Acc: 0.83978\n",
            "Itr: 0\tEpoch: 274\tTrain Acc: 0.83534\tTest Acc: 0.5469\n",
            "[INFO] Saving soft labels at epoch 274\n",
            "Saving labels to .\\CIFAR100_NO_ZCA\\ConvNet\\../../soft_labels_epoch_274.pt\n",
            "2025-05-13 04:51:45.116344 Itr: 0\tEpoch: 274\tTrain Acc: 0.83534\n",
            "Itr: 0\tEpoch: 275\tTrain Acc: 0.84056\tTest Acc: 0.5374\n",
            "[INFO] Saving soft labels at epoch 275\n",
            "Saving labels to .\\CIFAR100_NO_ZCA\\ConvNet\\../../soft_labels_epoch_275.pt\n",
            "2025-05-13 04:52:38.959565 Itr: 0\tEpoch: 275\tTrain Acc: 0.84056\n",
            "Itr: 0\tEpoch: 276\tTrain Acc: 0.83628\tTest Acc: 0.5233\n",
            "[INFO] Saving soft labels at epoch 276\n",
            "Saving labels to .\\CIFAR100_NO_ZCA\\ConvNet\\../../soft_labels_epoch_276.pt\n",
            "2025-05-13 04:53:08.541610 Itr: 0\tEpoch: 276\tTrain Acc: 0.83628\n",
            "Itr: 0\tEpoch: 277\tTrain Acc: 0.83496\tTest Acc: 0.5463\n",
            "[INFO] Saving soft labels at epoch 277\n",
            "Saving labels to .\\CIFAR100_NO_ZCA\\ConvNet\\../../soft_labels_epoch_277.pt\n",
            "2025-05-13 04:53:37.340360 Itr: 0\tEpoch: 277\tTrain Acc: 0.83496\n",
            "Itr: 0\tEpoch: 278\tTrain Acc: 0.83728\tTest Acc: 0.5514\n",
            "[INFO] Saving soft labels at epoch 278\n",
            "Saving labels to .\\CIFAR100_NO_ZCA\\ConvNet\\../../soft_labels_epoch_278.pt\n",
            "2025-05-13 04:54:06.900232 Itr: 0\tEpoch: 278\tTrain Acc: 0.83728\n",
            "Itr: 0\tEpoch: 279\tTrain Acc: 0.83062\tTest Acc: 0.5317\n",
            "[INFO] Saving soft labels at epoch 279\n",
            "Saving labels to .\\CIFAR100_NO_ZCA\\ConvNet\\../../soft_labels_epoch_279.pt\n",
            "2025-05-13 04:54:35.820113 Itr: 0\tEpoch: 279\tTrain Acc: 0.83062\n",
            "Itr: 0\tEpoch: 280\tTrain Acc: 0.82906\tTest Acc: 0.5486\n",
            "[INFO] Saving soft labels at epoch 280\n",
            "Saving labels to .\\CIFAR100_NO_ZCA\\ConvNet\\../../soft_labels_epoch_280.pt\n",
            "2025-05-13 04:55:05.311362 Itr: 0\tEpoch: 280\tTrain Acc: 0.82906\n",
            "Itr: 0\tEpoch: 281\tTrain Acc: 0.83222\tTest Acc: 0.5545\n",
            "[INFO] Saving soft labels at epoch 281\n",
            "Saving labels to .\\CIFAR100_NO_ZCA\\ConvNet\\../../soft_labels_epoch_281.pt\n",
            "2025-05-13 04:55:32.656242 Itr: 0\tEpoch: 281\tTrain Acc: 0.83222\n",
            "Itr: 0\tEpoch: 282\tTrain Acc: 0.83326\tTest Acc: 0.5477\n",
            "[INFO] Saving soft labels at epoch 282\n",
            "Saving labels to .\\CIFAR100_NO_ZCA\\ConvNet\\../../soft_labels_epoch_282.pt\n",
            "2025-05-13 04:56:02.868405 Itr: 0\tEpoch: 282\tTrain Acc: 0.83326\n",
            "Itr: 0\tEpoch: 283\tTrain Acc: 0.83994\tTest Acc: 0.5385\n",
            "[INFO] Saving soft labels at epoch 283\n",
            "Saving labels to .\\CIFAR100_NO_ZCA\\ConvNet\\../../soft_labels_epoch_283.pt\n",
            "2025-05-13 04:56:35.155115 Itr: 0\tEpoch: 283\tTrain Acc: 0.83994\n",
            "Itr: 0\tEpoch: 284\tTrain Acc: 0.83608\tTest Acc: 0.5419\n",
            "[INFO] Saving soft labels at epoch 284\n",
            "Saving labels to .\\CIFAR100_NO_ZCA\\ConvNet\\../../soft_labels_epoch_284.pt\n",
            "2025-05-13 04:57:04.997480 Itr: 0\tEpoch: 284\tTrain Acc: 0.83608\n",
            "Itr: 0\tEpoch: 285\tTrain Acc: 0.83846\tTest Acc: 0.5484\n",
            "[INFO] Saving soft labels at epoch 285\n",
            "Saving labels to .\\CIFAR100_NO_ZCA\\ConvNet\\../../soft_labels_epoch_285.pt\n",
            "2025-05-13 04:57:34.163131 Itr: 0\tEpoch: 285\tTrain Acc: 0.83846\n",
            "Itr: 0\tEpoch: 286\tTrain Acc: 0.84554\tTest Acc: 0.5454\n",
            "[INFO] Saving soft labels at epoch 286\n",
            "Saving labels to .\\CIFAR100_NO_ZCA\\ConvNet\\../../soft_labels_epoch_286.pt\n",
            "2025-05-13 04:58:06.071216 Itr: 0\tEpoch: 286\tTrain Acc: 0.84554\n",
            "Itr: 0\tEpoch: 287\tTrain Acc: 0.83564\tTest Acc: 0.5441\n",
            "[INFO] Saving soft labels at epoch 287\n",
            "Saving labels to .\\CIFAR100_NO_ZCA\\ConvNet\\../../soft_labels_epoch_287.pt\n",
            "2025-05-13 04:58:35.434351 Itr: 0\tEpoch: 287\tTrain Acc: 0.83564\n",
            "Itr: 0\tEpoch: 288\tTrain Acc: 0.84082\tTest Acc: 0.5347\n",
            "[INFO] Saving soft labels at epoch 288\n",
            "Saving labels to .\\CIFAR100_NO_ZCA\\ConvNet\\../../soft_labels_epoch_288.pt\n",
            "2025-05-13 04:59:05.206153 Itr: 0\tEpoch: 288\tTrain Acc: 0.84082\n",
            "Itr: 0\tEpoch: 289\tTrain Acc: 0.8453\tTest Acc: 0.5449\n",
            "[INFO] Saving soft labels at epoch 289\n",
            "Saving labels to .\\CIFAR100_NO_ZCA\\ConvNet\\../../soft_labels_epoch_289.pt\n",
            "2025-05-13 04:59:35.179612 Itr: 0\tEpoch: 289\tTrain Acc: 0.8453\n",
            "Itr: 0\tEpoch: 290\tTrain Acc: 0.8389\tTest Acc: 0.5505\n",
            "[INFO] Saving soft labels at epoch 290\n",
            "Saving labels to .\\CIFAR100_NO_ZCA\\ConvNet\\../../soft_labels_epoch_290.pt\n",
            "2025-05-13 05:00:05.929072 Itr: 0\tEpoch: 290\tTrain Acc: 0.8389\n",
            "Itr: 0\tEpoch: 291\tTrain Acc: 0.83296\tTest Acc: 0.5387\n",
            "[INFO] Saving soft labels at epoch 291\n",
            "Saving labels to .\\CIFAR100_NO_ZCA\\ConvNet\\../../soft_labels_epoch_291.pt\n",
            "2025-05-13 05:00:35.956547 Itr: 0\tEpoch: 291\tTrain Acc: 0.83296\n",
            "Itr: 0\tEpoch: 292\tTrain Acc: 0.85066\tTest Acc: 0.5374\n",
            "[INFO] Saving soft labels at epoch 292\n",
            "Saving labels to .\\CIFAR100_NO_ZCA\\ConvNet\\../../soft_labels_epoch_292.pt\n",
            "2025-05-13 05:01:17.301631 Itr: 0\tEpoch: 292\tTrain Acc: 0.85066\n",
            "Itr: 0\tEpoch: 293\tTrain Acc: 0.83224\tTest Acc: 0.5429\n",
            "[INFO] Saving soft labels at epoch 293\n",
            "Saving labels to .\\CIFAR100_NO_ZCA\\ConvNet\\../../soft_labels_epoch_293.pt\n",
            "2025-05-13 05:01:46.081684 Itr: 0\tEpoch: 293\tTrain Acc: 0.83224\n",
            "Itr: 0\tEpoch: 294\tTrain Acc: 0.8331\tTest Acc: 0.5471\n",
            "[INFO] Saving soft labels at epoch 294\n",
            "Saving labels to .\\CIFAR100_NO_ZCA\\ConvNet\\../../soft_labels_epoch_294.pt\n",
            "2025-05-13 05:02:14.846534 Itr: 0\tEpoch: 294\tTrain Acc: 0.8331\n",
            "Itr: 0\tEpoch: 295\tTrain Acc: 0.8374\tTest Acc: 0.5452\n",
            "[INFO] Saving soft labels at epoch 295\n",
            "Saving labels to .\\CIFAR100_NO_ZCA\\ConvNet\\../../soft_labels_epoch_295.pt\n",
            "2025-05-13 05:02:44.328119 Itr: 0\tEpoch: 295\tTrain Acc: 0.8374\n",
            "Itr: 0\tEpoch: 296\tTrain Acc: 0.8447\tTest Acc: 0.5426\n",
            "[INFO] Saving soft labels at epoch 296\n",
            "Saving labels to .\\CIFAR100_NO_ZCA\\ConvNet\\../../soft_labels_epoch_296.pt\n",
            "2025-05-13 05:03:13.779121 Itr: 0\tEpoch: 296\tTrain Acc: 0.8447\n",
            "Itr: 0\tEpoch: 297\tTrain Acc: 0.84504\tTest Acc: 0.5424\n",
            "[INFO] Saving soft labels at epoch 297\n",
            "Saving labels to .\\CIFAR100_NO_ZCA\\ConvNet\\../../soft_labels_epoch_297.pt\n",
            "2025-05-13 05:03:43.326273 Itr: 0\tEpoch: 297\tTrain Acc: 0.84504\n",
            "Itr: 0\tEpoch: 298\tTrain Acc: 0.8432\tTest Acc: 0.5394\n",
            "[INFO] Saving soft labels at epoch 298\n",
            "Saving labels to .\\CIFAR100_NO_ZCA\\ConvNet\\../../soft_labels_epoch_298.pt\n",
            "2025-05-13 05:04:12.921365 Itr: 0\tEpoch: 298\tTrain Acc: 0.8432\n",
            "Itr: 0\tEpoch: 299\tTrain Acc: 0.84326\tTest Acc: 0.5374\n",
            "[INFO] Saving soft labels at epoch 299\n",
            "Saving labels to .\\CIFAR100_NO_ZCA\\ConvNet\\../../soft_labels_epoch_299.pt\n",
            "2025-05-13 05:04:41.962513 Itr: 0\tEpoch: 299\tTrain Acc: 0.84326\n",
            "Saving .\\CIFAR100_NO_ZCA\\ConvNet\\replay_buffer_0.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "  0%|          | 0/50000 [00:00<?, ?it/s]\n",
            "  0%|          | 195/50000 [00:00<00:25, 1931.16it/s]\n",
            "  1%|          | 389/50000 [00:00<00:26, 1853.02it/s]\n",
            "  1%|1         | 575/50000 [00:00<00:28, 1756.09it/s]\n",
            "  2%|1         | 752/50000 [00:00<00:28, 1728.97it/s]\n",
            "  2%|1         | 934/50000 [00:00<00:27, 1755.63it/s]\n",
            "  2%|2         | 1113/50000 [00:00<00:27, 1761.94it/s]\n",
            "  3%|2         | 1290/50000 [00:00<00:27, 1745.77it/s]\n",
            "  3%|2         | 1466/50000 [00:00<00:27, 1749.43it/s]\n",
            "  3%|3         | 1642/50000 [00:00<00:28, 1722.29it/s]\n",
            "  4%|3         | 1815/50000 [00:01<00:28, 1708.49it/s]\n",
            "  4%|3         | 1988/50000 [00:01<00:28, 1709.94it/s]\n",
            "  4%|4         | 2162/50000 [00:01<00:27, 1718.44it/s]\n",
            "  5%|4         | 2334/50000 [00:01<00:28, 1696.13it/s]\n",
            "  5%|5         | 2504/50000 [00:01<00:28, 1690.67it/s]\n",
            "  5%|5         | 2677/50000 [00:01<00:27, 1700.33it/s]\n",
            "  6%|5         | 2849/50000 [00:01<00:27, 1702.47it/s]\n",
            "  6%|6         | 3020/50000 [00:01<00:27, 1703.61it/s]\n",
            "  6%|6         | 3191/50000 [00:01<00:27, 1699.85it/s]\n",
            "  7%|6         | 3362/50000 [00:01<00:27, 1678.53it/s]\n",
            "  7%|7         | 3530/50000 [00:02<00:27, 1667.51it/s]\n",
            "  7%|7         | 3700/50000 [00:02<00:27, 1675.86it/s]\n",
            "  8%|7         | 3870/50000 [00:02<00:27, 1681.32it/s]\n",
            "  8%|8         | 4039/50000 [00:02<00:27, 1670.54it/s]\n",
            "  8%|8         | 4208/50000 [00:02<00:27, 1675.34it/s]\n",
            "  9%|8         | 4378/50000 [00:02<00:27, 1682.48it/s]\n",
            "  9%|9         | 4551/50000 [00:02<00:26, 1693.92it/s]\n",
            "  9%|9         | 4724/50000 [00:02<00:26, 1701.27it/s]\n",
            " 10%|9         | 4898/50000 [00:02<00:26, 1708.64it/s]\n",
            " 10%|#         | 5069/50000 [00:02<00:26, 1700.89it/s]\n",
            " 10%|#         | 5245/50000 [00:03<00:26, 1715.11it/s]\n",
            " 11%|#         | 5417/50000 [00:03<00:25, 1716.50it/s]\n",
            " 11%|#1        | 5591/50000 [00:03<00:25, 1718.64it/s]\n",
            " 12%|#1        | 5763/50000 [00:03<00:26, 1683.57it/s]\n",
            " 12%|#1        | 5937/50000 [00:03<00:25, 1695.50it/s]\n",
            " 12%|#2        | 6110/50000 [00:03<00:25, 1703.13it/s]\n",
            " 13%|#2        | 6287/50000 [00:03<00:25, 1718.28it/s]\n",
            " 13%|#2        | 6468/50000 [00:03<00:24, 1744.39it/s]\n",
            " 13%|#3        | 6680/50000 [00:03<00:23, 1851.71it/s]\n",
            " 14%|#3        | 6866/50000 [00:03<00:23, 1815.25it/s]\n",
            " 14%|#4        | 7048/50000 [00:04<00:24, 1764.51it/s]\n",
            " 14%|#4        | 7225/50000 [00:04<00:24, 1740.37it/s]\n",
            " 15%|#4        | 7400/50000 [00:04<00:24, 1723.56it/s]\n",
            " 15%|#5        | 7573/50000 [00:04<00:24, 1716.06it/s]\n",
            " 15%|#5        | 7745/50000 [00:04<00:24, 1715.16it/s]\n",
            " 16%|#5        | 7917/50000 [00:04<00:24, 1702.37it/s]\n",
            " 16%|#6        | 8089/50000 [00:04<00:24, 1706.23it/s]\n",
            " 17%|#6        | 8266/50000 [00:04<00:24, 1721.41it/s]\n",
            " 17%|#6        | 8441/50000 [00:04<00:24, 1728.38it/s]\n",
            " 17%|#7        | 8619/50000 [00:05<00:23, 1740.41it/s]\n",
            " 18%|#7        | 8799/50000 [00:05<00:23, 1754.44it/s]\n",
            " 18%|#7        | 8975/50000 [00:05<00:23, 1731.98it/s]\n",
            " 18%|#8        | 9149/50000 [00:05<00:23, 1730.20it/s]\n",
            " 19%|#8        | 9323/50000 [00:05<00:23, 1709.10it/s]\n",
            " 19%|#8        | 9495/50000 [00:05<00:23, 1709.99it/s]\n",
            " 19%|#9        | 9668/50000 [00:05<00:23, 1712.60it/s]\n",
            " 20%|#9        | 9840/50000 [00:05<00:24, 1661.23it/s]\n",
            " 20%|##        | 10007/50000 [00:05<00:24, 1641.86it/s]\n",
            " 20%|##        | 10179/50000 [00:05<00:23, 1662.98it/s]\n",
            " 21%|##        | 10359/50000 [00:06<00:23, 1701.58it/s]\n",
            " 21%|##1       | 10533/50000 [00:06<00:23, 1709.11it/s]\n",
            " 21%|##1       | 10705/50000 [00:06<00:23, 1698.82it/s]\n",
            " 22%|##1       | 10876/50000 [00:06<00:23, 1691.94it/s]\n",
            " 22%|##2       | 11046/50000 [00:06<00:23, 1683.05it/s]\n",
            " 22%|##2       | 11219/50000 [00:06<00:22, 1694.88it/s]\n",
            " 23%|##2       | 11397/50000 [00:06<00:22, 1715.36it/s]\n",
            " 23%|##3       | 11569/50000 [00:06<00:22, 1711.61it/s]\n",
            " 23%|##3       | 11741/50000 [00:06<00:22, 1711.58it/s]\n",
            " 24%|##3       | 11918/50000 [00:06<00:22, 1721.82it/s]\n",
            " 24%|##4       | 12091/50000 [00:07<00:21, 1723.82it/s]\n",
            " 25%|##4       | 12268/50000 [00:07<00:21, 1733.43it/s]\n",
            " 25%|##4       | 12442/50000 [00:07<00:21, 1713.88it/s]\n",
            " 25%|##5       | 12614/50000 [00:07<00:21, 1710.66it/s]\n",
            " 26%|##5       | 12786/50000 [00:07<00:21, 1709.96it/s]\n",
            " 26%|##5       | 12958/50000 [00:07<00:21, 1711.15it/s]\n",
            " 26%|##6       | 13130/50000 [00:07<00:21, 1712.28it/s]\n",
            " 27%|##6       | 13307/50000 [00:07<00:21, 1726.00it/s]\n",
            " 27%|##6       | 13480/50000 [00:07<00:21, 1722.57it/s]\n",
            " 27%|##7       | 13653/50000 [00:07<00:21, 1701.30it/s]\n",
            " 28%|##7       | 13825/50000 [00:08<00:21, 1703.57it/s]\n",
            " 28%|##7       | 13996/50000 [00:08<00:21, 1692.59it/s]\n",
            " 28%|##8       | 14166/50000 [00:08<00:21, 1692.72it/s]\n",
            " 29%|##8       | 14336/50000 [00:08<00:21, 1665.98it/s]\n",
            " 29%|##9       | 14505/50000 [00:08<00:21, 1668.64it/s]\n",
            " 29%|##9       | 14682/50000 [00:08<00:20, 1693.91it/s]\n",
            " 30%|##9       | 14860/50000 [00:08<00:20, 1716.61it/s]\n",
            " 30%|###       | 15036/50000 [00:08<00:20, 1724.70it/s]\n",
            " 30%|###       | 15209/50000 [00:08<00:20, 1724.03it/s]\n",
            " 31%|###       | 15382/50000 [00:08<00:20, 1722.49it/s]\n",
            " 31%|###1      | 15560/50000 [00:09<00:19, 1734.83it/s]\n",
            " 31%|###1      | 15734/50000 [00:09<00:19, 1732.22it/s]\n",
            " 32%|###1      | 15908/50000 [00:09<00:19, 1723.20it/s]\n",
            " 32%|###2      | 16081/50000 [00:09<00:20, 1692.44it/s]\n",
            " 33%|###2      | 16251/50000 [00:09<00:20, 1686.16it/s]\n",
            " 33%|###2      | 16425/50000 [00:09<00:19, 1697.07it/s]\n",
            " 33%|###3      | 16596/50000 [00:09<00:19, 1698.80it/s]\n",
            " 34%|###3      | 16771/50000 [00:09<00:19, 1708.92it/s]\n",
            " 34%|###3      | 16942/50000 [00:09<00:19, 1708.15it/s]\n",
            " 34%|###4      | 17113/50000 [00:09<00:19, 1702.87it/s]\n",
            " 35%|###4      | 17287/50000 [00:10<00:19, 1709.08it/s]\n",
            " 35%|###4      | 17462/50000 [00:10<00:18, 1718.33it/s]\n",
            " 35%|###5      | 17635/50000 [00:10<00:18, 1716.88it/s]\n",
            " 36%|###5      | 17807/50000 [00:10<00:18, 1694.38it/s]\n",
            " 36%|###5      | 17977/50000 [00:10<00:18, 1692.09it/s]\n",
            " 36%|###6      | 18150/50000 [00:10<00:18, 1702.22it/s]\n",
            " 37%|###6      | 18321/50000 [00:10<00:18, 1704.42it/s]\n",
            " 37%|###6      | 18496/50000 [00:10<00:18, 1714.37it/s]\n",
            " 37%|###7      | 18668/50000 [00:10<00:18, 1710.50it/s]\n",
            " 38%|###7      | 18840/50000 [00:11<00:18, 1709.53it/s]\n",
            " 38%|###8      | 19011/50000 [00:11<00:18, 1686.70it/s]\n",
            " 38%|###8      | 19184/50000 [00:11<00:18, 1695.57it/s]\n",
            " 39%|###8      | 19357/50000 [00:11<00:17, 1704.22it/s]\n",
            " 39%|###9      | 19528/50000 [00:11<00:18, 1664.34it/s]\n",
            " 39%|###9      | 19695/50000 [00:11<00:18, 1661.65it/s]\n",
            " 40%|###9      | 19862/50000 [00:11<00:18, 1661.77it/s]\n",
            " 40%|####      | 20035/50000 [00:11<00:17, 1679.59it/s]\n",
            " 40%|####      | 20208/50000 [00:11<00:17, 1690.14it/s]\n",
            " 41%|####      | 20378/50000 [00:11<00:17, 1676.91it/s]\n",
            " 41%|####1     | 20548/50000 [00:12<00:17, 1680.44it/s]\n",
            " 41%|####1     | 20717/50000 [00:12<00:17, 1677.71it/s]\n",
            " 42%|####1     | 20888/50000 [00:12<00:17, 1685.52it/s]\n",
            " 42%|####2     | 21059/50000 [00:12<00:17, 1688.76it/s]\n",
            " 42%|####2     | 21228/50000 [00:12<00:17, 1683.16it/s]\n",
            " 43%|####2     | 21397/50000 [00:12<00:17, 1664.85it/s]\n",
            " 43%|####3     | 21567/50000 [00:12<00:17, 1671.56it/s]\n",
            " 43%|####3     | 21737/50000 [00:12<00:16, 1674.82it/s]\n",
            " 44%|####3     | 21914/50000 [00:12<00:16, 1697.05it/s]\n",
            " 44%|####4     | 22084/50000 [00:12<00:16, 1680.25it/s]\n",
            " 45%|####4     | 22258/50000 [00:13<00:16, 1694.23it/s]\n",
            " 45%|####4     | 22428/50000 [00:13<00:16, 1688.03it/s]\n",
            " 45%|####5     | 22599/50000 [00:13<00:16, 1692.15it/s]\n",
            " 46%|####5     | 22775/50000 [00:13<00:15, 1708.62it/s]\n",
            " 46%|####5     | 22946/50000 [00:13<00:15, 1704.14it/s]\n",
            " 46%|####6     | 23117/50000 [00:13<00:16, 1680.17it/s]\n",
            " 47%|####6     | 23293/50000 [00:13<00:15, 1703.03it/s]\n",
            " 47%|####6     | 23469/50000 [00:13<00:15, 1716.53it/s]\n",
            " 47%|####7     | 23642/50000 [00:13<00:15, 1716.46it/s]\n",
            " 48%|####7     | 23820/50000 [00:13<00:15, 1734.61it/s]\n",
            " 48%|####7     | 23994/50000 [00:14<00:14, 1734.53it/s]\n",
            " 48%|####8     | 24168/50000 [00:14<00:14, 1731.44it/s]\n",
            " 49%|####8     | 24342/50000 [00:14<00:14, 1724.87it/s]\n",
            " 49%|####9     | 24515/50000 [00:14<00:14, 1720.98it/s]\n",
            " 49%|####9     | 24688/50000 [00:14<00:14, 1700.28it/s]\n",
            " 50%|####9     | 24859/50000 [00:14<00:15, 1671.86it/s]\n",
            " 50%|#####     | 25036/50000 [00:14<00:14, 1700.31it/s]\n",
            " 50%|#####     | 25212/50000 [00:14<00:14, 1714.56it/s]\n",
            " 51%|#####     | 25386/50000 [00:14<00:14, 1716.64it/s]\n",
            " 51%|#####1    | 25558/50000 [00:14<00:14, 1707.74it/s]\n",
            " 51%|#####1    | 25729/50000 [00:15<00:14, 1680.36it/s]\n",
            " 52%|#####1    | 25901/50000 [00:15<00:14, 1689.68it/s]\n",
            " 52%|#####2    | 26074/50000 [00:15<00:14, 1697.69it/s]\n",
            " 52%|#####2    | 26244/50000 [00:15<00:14, 1684.90it/s]\n",
            " 53%|#####2    | 26413/50000 [00:15<00:14, 1663.58it/s]\n",
            " 53%|#####3    | 26580/50000 [00:15<00:24, 955.97it/s] \n",
            " 54%|#####3    | 26753/50000 [00:15<00:21, 1104.48it/s]\n",
            " 54%|#####3    | 26929/50000 [00:16<00:18, 1245.49it/s]\n",
            " 54%|#####4    | 27100/50000 [00:16<00:16, 1353.32it/s]\n",
            " 55%|#####4    | 27271/50000 [00:16<00:15, 1443.21it/s]\n",
            " 55%|#####4    | 27441/50000 [00:16<00:14, 1510.06it/s]\n",
            " 55%|#####5    | 27610/50000 [00:16<00:14, 1558.50it/s]\n",
            " 56%|#####5    | 27785/50000 [00:16<00:13, 1610.65it/s]\n",
            " 56%|#####5    | 27954/50000 [00:16<00:13, 1622.88it/s]\n",
            " 56%|#####6    | 28127/50000 [00:16<00:13, 1653.67it/s]\n",
            " 57%|#####6    | 28301/50000 [00:16<00:12, 1672.10it/s]\n",
            " 57%|#####6    | 28474/50000 [00:16<00:12, 1687.56it/s]\n",
            " 57%|#####7    | 28651/50000 [00:17<00:12, 1707.65it/s]\n",
            " 58%|#####7    | 28824/50000 [00:17<00:12, 1710.08it/s]\n",
            " 58%|#####8    | 29000/50000 [00:17<00:12, 1723.55it/s]\n",
            " 58%|#####8    | 29173/50000 [00:17<00:12, 1718.50it/s]\n",
            " 59%|#####8    | 29346/50000 [00:17<00:12, 1706.51it/s]\n",
            " 59%|#####9    | 29517/50000 [00:17<00:12, 1698.13it/s]\n",
            " 59%|#####9    | 29688/50000 [00:17<00:11, 1695.28it/s]\n",
            " 60%|#####9    | 29858/50000 [00:17<00:11, 1692.32it/s]\n",
            " 60%|######    | 30033/50000 [00:17<00:11, 1705.87it/s]\n",
            " 60%|######    | 30212/50000 [00:17<00:11, 1725.92it/s]\n",
            " 61%|######    | 30385/50000 [00:18<00:11, 1718.53it/s]\n",
            " 61%|######1   | 30557/50000 [00:18<00:11, 1717.34it/s]\n",
            " 61%|######1   | 30729/50000 [00:18<00:11, 1694.60it/s]\n",
            " 62%|######1   | 30899/50000 [00:18<00:11, 1691.85it/s]\n",
            " 62%|######2   | 31069/50000 [00:18<00:11, 1692.81it/s]\n",
            " 62%|######2   | 31246/50000 [00:18<00:10, 1712.70it/s]\n",
            " 63%|######2   | 31418/50000 [00:18<00:10, 1701.38it/s]\n",
            " 63%|######3   | 31589/50000 [00:18<00:10, 1703.04it/s]\n",
            " 64%|######3   | 31763/50000 [00:18<00:10, 1713.57it/s]\n",
            " 64%|######3   | 31938/50000 [00:18<00:10, 1723.55it/s]\n",
            " 64%|######4   | 32111/50000 [00:19<00:10, 1720.07it/s]\n",
            " 65%|######4   | 32284/50000 [00:19<00:10, 1712.43it/s]\n",
            " 65%|######4   | 32456/50000 [00:19<00:10, 1709.11it/s]\n",
            " 65%|######5   | 32627/50000 [00:19<00:10, 1705.57it/s]\n",
            " 66%|######5   | 32798/50000 [00:19<00:10, 1706.30it/s]\n",
            " 66%|######5   | 32969/50000 [00:19<00:09, 1706.98it/s]\n",
            " 66%|######6   | 33142/50000 [00:19<00:09, 1707.79it/s]\n",
            " 67%|######6   | 33314/50000 [00:19<00:09, 1710.28it/s]\n",
            " 67%|######6   | 33486/50000 [00:19<00:09, 1712.20it/s]\n",
            " 67%|######7   | 33658/50000 [00:19<00:09, 1709.92it/s]\n",
            " 68%|######7   | 33829/50000 [00:20<00:09, 1701.21it/s]\n",
            " 68%|######8   | 34001/50000 [00:20<00:09, 1701.75it/s]\n",
            " 68%|######8   | 34174/50000 [00:20<00:09, 1707.42it/s]\n",
            " 69%|######8   | 34346/50000 [00:20<00:09, 1710.28it/s]\n",
            " 69%|######9   | 34518/50000 [00:20<00:09, 1706.01it/s]\n",
            " 69%|######9   | 34689/50000 [00:20<00:08, 1703.43it/s]\n",
            " 70%|######9   | 34860/50000 [00:20<00:09, 1678.16it/s]\n",
            " 70%|#######   | 35028/50000 [00:20<00:08, 1677.04it/s]\n",
            " 70%|#######   | 35200/50000 [00:20<00:08, 1686.70it/s]\n",
            " 71%|#######   | 35369/50000 [00:20<00:09, 1619.54it/s]\n",
            " 71%|#######1  | 35532/50000 [00:21<00:09, 1527.34it/s]\n",
            " 71%|#######1  | 35686/50000 [00:21<00:10, 1370.07it/s]\n",
            " 72%|#######1  | 35827/50000 [00:21<00:10, 1349.38it/s]\n",
            " 72%|#######1  | 35965/50000 [00:21<00:10, 1307.39it/s]\n",
            " 72%|#######2  | 36098/50000 [00:21<00:11, 1255.24it/s]\n",
            " 72%|#######2  | 36234/50000 [00:21<00:10, 1272.08it/s]\n",
            " 73%|#######2  | 36363/50000 [00:21<00:10, 1257.37it/s]\n",
            " 73%|#######3  | 36510/50000 [00:21<00:10, 1314.37it/s]\n",
            " 73%|#######3  | 36666/50000 [00:22<00:09, 1381.12it/s]\n",
            " 74%|#######3  | 36876/50000 [00:22<00:08, 1585.81it/s]\n",
            " 74%|#######4  | 37098/50000 [00:22<00:07, 1765.69it/s]\n",
            " 75%|#######4  | 37276/50000 [00:22<00:07, 1734.77it/s]\n",
            " 75%|#######4  | 37451/50000 [00:22<00:07, 1725.06it/s]\n",
            " 75%|#######5  | 37625/50000 [00:22<00:07, 1718.64it/s]\n",
            " 76%|#######5  | 37798/50000 [00:22<00:07, 1713.67it/s]\n",
            " 76%|#######5  | 37970/50000 [00:22<00:07, 1692.10it/s]\n",
            " 76%|#######6  | 38140/50000 [00:22<00:07, 1668.40it/s]\n",
            " 77%|#######6  | 38316/50000 [00:22<00:06, 1692.19it/s]\n",
            " 77%|#######6  | 38486/50000 [00:23<00:06, 1682.68it/s]\n",
            " 77%|#######7  | 38657/50000 [00:23<00:06, 1688.30it/s]\n",
            " 78%|#######7  | 38830/50000 [00:23<00:06, 1693.45it/s]\n",
            " 78%|#######8  | 39000/50000 [00:23<00:06, 1687.58it/s]\n",
            " 78%|#######8  | 39169/50000 [00:23<00:06, 1688.23it/s]\n",
            " 79%|#######8  | 39347/50000 [00:23<00:06, 1710.89it/s]\n",
            " 79%|#######9  | 39519/50000 [00:23<00:06, 1710.00it/s]\n",
            " 79%|#######9  | 39694/50000 [00:23<00:05, 1719.38it/s]\n",
            " 80%|#######9  | 39866/50000 [00:23<00:05, 1710.26it/s]\n",
            " 80%|########  | 40038/50000 [00:23<00:05, 1678.08it/s]\n",
            " 80%|########  | 40210/50000 [00:24<00:05, 1689.94it/s]\n",
            " 81%|########  | 40384/50000 [00:24<00:05, 1700.35it/s]\n",
            " 81%|########1 | 40556/50000 [00:24<00:05, 1701.54it/s]\n",
            " 81%|########1 | 40729/50000 [00:24<00:05, 1706.09it/s]\n",
            " 82%|########1 | 40900/50000 [00:24<00:05, 1705.75it/s]\n",
            " 82%|########2 | 41072/50000 [00:24<00:05, 1707.52it/s]\n",
            " 82%|########2 | 41246/50000 [00:24<00:05, 1712.38it/s]\n",
            " 83%|########2 | 41418/50000 [00:24<00:05, 1692.81it/s]\n",
            " 83%|########3 | 41592/50000 [00:24<00:04, 1702.73it/s]\n",
            " 84%|########3 | 41763/50000 [00:24<00:04, 1678.84it/s]\n",
            " 84%|########3 | 41938/50000 [00:25<00:04, 1699.49it/s]\n",
            " 84%|########4 | 42116/50000 [00:25<00:04, 1720.86it/s]\n",
            " 85%|########4 | 42293/50000 [00:25<00:04, 1730.79it/s]\n",
            " 85%|########4 | 42467/50000 [00:25<00:04, 1729.30it/s]\n",
            " 85%|########5 | 42640/50000 [00:25<00:04, 1727.28it/s]\n",
            " 86%|########5 | 42814/50000 [00:25<00:04, 1728.08it/s]\n",
            " 86%|########5 | 42987/50000 [00:25<00:04, 1717.21it/s]\n",
            " 86%|########6 | 43161/50000 [00:25<00:03, 1720.40it/s]\n",
            " 87%|########6 | 43334/50000 [00:25<00:03, 1713.86it/s]\n",
            " 87%|########7 | 43507/50000 [00:25<00:03, 1715.34it/s]\n",
            " 87%|########7 | 43682/50000 [00:26<00:03, 1721.53it/s]\n",
            " 88%|########7 | 43857/50000 [00:26<00:03, 1728.69it/s]\n",
            " 88%|########8 | 44030/50000 [00:26<00:03, 1709.00it/s]\n",
            " 88%|########8 | 44201/50000 [00:26<00:03, 1700.39it/s]\n",
            " 89%|########8 | 44372/50000 [00:26<00:03, 1696.89it/s]\n",
            " 89%|########9 | 44545/50000 [00:26<00:03, 1703.72it/s]\n",
            " 89%|########9 | 44716/50000 [00:26<00:03, 1649.54it/s]\n",
            " 90%|########9 | 44885/50000 [00:26<00:03, 1658.71it/s]\n",
            " 90%|######### | 45057/50000 [00:26<00:02, 1672.15it/s]\n",
            " 90%|######### | 45230/50000 [00:27<00:02, 1686.09it/s]\n",
            " 91%|######### | 45401/50000 [00:27<00:02, 1691.84it/s]\n",
            " 91%|#########1| 45580/50000 [00:27<00:02, 1716.65it/s]\n",
            " 92%|#########1| 45752/50000 [00:27<00:02, 1712.45it/s]\n",
            " 92%|#########1| 45926/50000 [00:27<00:02, 1716.61it/s]\n",
            " 92%|#########2| 46098/50000 [00:27<00:02, 1713.24it/s]\n",
            " 93%|#########2| 46271/50000 [00:27<00:02, 1715.66it/s]\n",
            " 93%|#########2| 46443/50000 [00:27<00:02, 1711.18it/s]\n",
            " 93%|#########3| 46615/50000 [00:27<00:01, 1700.59it/s]\n",
            " 94%|#########3| 46786/50000 [00:27<00:01, 1703.05it/s]\n",
            " 94%|#########3| 46957/50000 [00:28<00:01, 1703.33it/s]\n",
            " 94%|#########4| 47128/50000 [00:28<00:01, 1701.08it/s]\n",
            " 95%|#########4| 47299/50000 [00:28<00:01, 1699.42it/s]\n",
            " 95%|#########4| 47469/50000 [00:28<00:01, 1692.36it/s]\n",
            " 95%|#########5| 47642/50000 [00:28<00:01, 1700.47it/s]\n",
            " 96%|#########5| 47816/50000 [00:28<00:01, 1710.90it/s]\n",
            " 96%|#########5| 47995/50000 [00:28<00:01, 1732.23it/s]\n",
            " 96%|#########6| 48170/50000 [00:28<00:01, 1736.93it/s]\n",
            " 97%|#########6| 48346/50000 [00:28<00:00, 1742.62it/s]\n",
            " 97%|#########7| 48521/50000 [00:28<00:00, 1731.54it/s]\n",
            " 97%|#########7| 48698/50000 [00:29<00:00, 1738.95it/s]\n",
            " 98%|#########7| 48872/50000 [00:29<00:00, 1718.66it/s]\n",
            " 98%|#########8| 49045/50000 [00:29<00:00, 1720.88it/s]\n",
            " 98%|#########8| 49218/50000 [00:29<00:00, 1721.46it/s]\n",
            " 99%|#########8| 49391/50000 [00:29<00:00, 1717.42it/s]\n",
            " 99%|#########9| 49563/50000 [00:29<00:00, 1701.06it/s]\n",
            " 99%|#########9| 49743/50000 [00:29<00:00, 1730.36it/s]\n",
            "100%|#########9| 49919/50000 [00:29<00:00, 1736.50it/s]\n",
            "100%|##########| 50000/50000 [00:29<00:00, 1679.16it/s]\n",
            "\n",
            "0it [00:00, ?it/s]\n",
            "50000it [00:00, 1768226.51it/s]\n",
            "C:\\Users\\nv1n24\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\functional.py:534: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\TensorShape.cpp:3596.)\n",
            "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.chdir(r\"C:\\Users\\nv1n24\\OneDrive - University of Southampton\\PhD\\Taught Modules\\COMP6258\\GroupWork\\no-distillation\\softlabel\")"
      ],
      "metadata": {
        "id": "G9x-N3ldPxgq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "print(os.getcwd())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BBkcmftjPw-X",
        "outputId": "eaefc916-535e-40b9-9389-214ef930697b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "C:\\Users\\nv1n24\\OneDrive - University of Southampton\\PhD\\Taught Modules\\COMP6258\\GroupWork\\no-distillation\\softlabel\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import subprocess\n",
        "import sys\n",
        "\n",
        "runs = [\n",
        "    (5, 24),     # IPC 5, epoch 24\n",
        " (10, 57),    # IPC 10, epoch 57\n",
        " (50, 125),   # IPC 50, epoch 125\n",
        " (100, 125),  # IPC 100, epoch 125\n",
        "           ]\n",
        "\n",
        "\n",
        "probabilities = [0.8, 0.9, 0.95, 1]\n",
        "\n",
        "\n",
        "for ipc, epoch in runs:\n",
        "  for prob in probabilities:\n",
        "    command = [\n",
        "            \"python\", \"nodistill_GaussandShuffle.py\",\n",
        "            \"--dataset=CIFAR100\",\n",
        "            f\"--ipc={ipc}\",\n",
        "            \"--expt_type=add_noise\",\n",
        "            \"--teacher_label\",\n",
        "            f\"--max_expert_epoch={epoch}\",\n",
        "            \"--lr_net=1.e-02\",\n",
        "            \"--expert_path=../train_expert\",\n",
        "            \"--data_path=/content/cifar100\",\n",
        "            f\"--true_class_probability={prob}\",\n",
        "            \"--student_model=ConvNet\",\n",
        "            \"--teacher_model=ConvNet\",\n",
        "            \"--epoch_eval_train=3000\",\n",
        "            \"--num_eval=1\"\n",
        "        ]\n",
        "    print(f\"\\n▶️ Running: IPC={ipc}, Expert Epoch={epoch}, probability:{prob} \\n\")\n",
        "    result = subprocess.run(command, capture_output=True, text=True)\n",
        "\n",
        "      # Print stdout and stderr\n",
        "    print(result.stdout)\n",
        "      #print(result.stderr)\n",
        "\n",
        "      # Save output to file\n",
        "    log_filename = f\"output_ipc{ipc}_epoch{epoch}_Noisy{prob}.txt\"\n",
        "    with open(log_filename, 'w') as f:\n",
        "      f.write(result.stdout)\n",
        "      f.write(\"\\n--- STDERR ---\\n\")\n",
        "      f.write(result.stderr)\n",
        "\n",
        "    print(f\"[✅] Output saved to {log_filename}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dkUvUuFCEcgd",
        "outputId": "f6c594c9-d920-458f-d74d-0670437bca20"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "▶️ Running: IPC=5, Expert Epoch=24, probability:0.8 \n",
            "\n",
            "CUDNN STATUS: True\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Hyper-parameters: \n",
            " {'dataset': 'CIFAR100', 'data_path': '/content/cifar100', 'expert_path': '../train_expert', 'ipc': 5, 'teacher_model': 'ConvNet', 'student_model': 'ConvNet', 'epoch_eval_train': 3000, 'optimizer': 'SGD', 'lr_net': 0.01, 'true_class_probability': 0.8, 'expt_type': 'add_noise', 'num_eval': 1, 'batch_train': 256, 'batch_real': 256, 'zca': False, 'load_checkpoint': None, 'selection_strategy': 'random', 'dsa': True, 'dsa_strategy': 'color_crop_cutout_flip_scale_rotate', 'teacher_label': True, 'max_expert_epoch': 24, 'min_expert_epoch': 1, 'ensemble': False, 'temp': 1.0, 'wandb': False, 'run_name': None, 'notes': 'No description', 'device': 'cuda', 'im_size': [32, 32], 'dc_aug_param': None, 'dsa_param': <utils.ParamDiffAug object at 0x000001B4C2D40890>, '_wandb': {}, 'zca_trans': None, 'distributed': False}\n",
            "Evaluation model:  ConvNet\n",
            "---------------Build label to index map--------------\n",
            "initialize synthetic data from random real images\n",
            "Expert Dir: ../train_expert\\CIFAR100_NO_ZCA\\ConvNet 2 buffers detected\n",
            "[2025-05-14 00:53:45] training begins\n",
            "-------------------------\n",
            "Experiment ID = 0\n",
            "loading file ../train_expert\\CIFAR100_NO_ZCA\\ConvNet\\replay_buffer_1.pt\n",
            "[INFO] Original labels saved to ../../labels_before_noise_IPC5_ExpEpoch24.pt\n",
            "[INFO] Noisy labels saved to ../../noisy_labels_top0.8_IPC5_ExpEpoch24.pt\n",
            "-------------------------\n",
            "Evaluation\n",
            "Experiment ID = 0, model_train = ConvNet, model_eval = ConvNet\n",
            "DSA augmentation strategy: \n",
            " color_crop_cutout_flip_scale_rotate\n",
            "DSA augmentation parameters: \n",
            " {'aug_mode': 'S', 'prob_flip': 0.5, 'ratio_scale': 1.2, 'ratio_rotate': 15.0, 'ratio_crop_pad': 0.125, 'ratio_cutout': 0.5, 'ratio_noise': 0.05, 'brightness': 1.0, 'saturation': 2.0, 'contrast': 0.5}\n",
            "[2025-05-14 00:54:26] Evaluate_00: epoch = 3000  train loss = 1.719459 train acc = 0.9980, test acc = 0.1214\n",
            "[2025-05-14 00:55:02] Evaluate_00: epoch = 3000  train loss = 1.516612 train acc = 1.0000, test acc = 0.1238\n",
            "[2025-05-14 00:55:37] Evaluate_00: epoch = 3000  train loss = 1.502394 train acc = 1.0000, test acc = 0.1201\n",
            "[2025-05-14 00:56:12] Evaluate_00: epoch = 3000  train loss = 1.541877 train acc = 1.0000, test acc = 0.1295\n",
            "[2025-05-14 00:56:44] Evaluate_00: epoch = 3000  train loss = 1.511894 train acc = 0.9980, test acc = 0.1274\n",
            "[2025-05-14 00:57:17] Evaluate_00: epoch = 3000  train loss = 1.534336 train acc = 1.0000, test acc = 0.1307\n",
            "[2025-05-14 00:57:53] Evaluate_00: epoch = 3000  train loss = 1.444987 train acc = 1.0000, test acc = 0.1293\n",
            "[2025-05-14 00:58:28] Evaluate_00: epoch = 3000  train loss = 1.460989 train acc = 1.0000, test acc = 0.1326\n",
            "[2025-05-14 00:59:00] Evaluate_00: epoch = 3000  train loss = 1.453647 train acc = 1.0000, test acc = 0.1322\n",
            "[2025-05-14 00:59:35] Evaluate_00: epoch = 3000  train loss = 1.486606 train acc = 1.0000, test acc = 0.1317\n",
            "[2025-05-14 01:00:11] Evaluate_00: epoch = 3000  train loss = 1.457884 train acc = 1.0000, test acc = 0.1327\n",
            "[2025-05-14 01:00:46] Evaluate_00: epoch = 3000  train loss = 1.454565 train acc = 1.0000, test acc = 0.1317\n",
            "[2025-05-14 01:01:21] Evaluate_00: epoch = 3000  train loss = 1.446901 train acc = 1.0000, test acc = 0.1331\n",
            "[2025-05-14 01:01:55] Evaluate_00: epoch = 3000  train loss = 1.438429 train acc = 1.0000, test acc = 0.1332\n",
            "[2025-05-14 01:02:31] Evaluate_00: epoch = 3000  train loss = 1.446793 train acc = 1.0000, test acc = 0.1326\n",
            "[2025-05-14 01:02:31] Evaluate_00: epoch = 3000 train time = 518 s train loss = 1.446793 train acc = 1.0000, test acc = 0.1326\n",
            "Evaluate 1 random ConvNet, mean = 0.1332 std = 0.0000\n",
            "-------------------------\n",
            "\n",
            "[✅] Output saved to output_ipc5_epoch24_Noisy0.8.txt\n",
            "\n",
            "▶️ Running: IPC=5, Expert Epoch=24, probability:0.9 \n",
            "\n",
            "CUDNN STATUS: True\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Hyper-parameters: \n",
            " {'dataset': 'CIFAR100', 'data_path': '/content/cifar100', 'expert_path': '../train_expert', 'ipc': 5, 'teacher_model': 'ConvNet', 'student_model': 'ConvNet', 'epoch_eval_train': 3000, 'optimizer': 'SGD', 'lr_net': 0.01, 'true_class_probability': 0.9, 'expt_type': 'add_noise', 'num_eval': 1, 'batch_train': 256, 'batch_real': 256, 'zca': False, 'load_checkpoint': None, 'selection_strategy': 'random', 'dsa': True, 'dsa_strategy': 'color_crop_cutout_flip_scale_rotate', 'teacher_label': True, 'max_expert_epoch': 24, 'min_expert_epoch': 1, 'ensemble': False, 'temp': 1.0, 'wandb': False, 'run_name': None, 'notes': 'No description', 'device': 'cuda', 'im_size': [32, 32], 'dc_aug_param': None, 'dsa_param': <utils.ParamDiffAug object at 0x0000029B78212450>, '_wandb': {}, 'zca_trans': None, 'distributed': False}\n",
            "Evaluation model:  ConvNet\n",
            "---------------Build label to index map--------------\n",
            "initialize synthetic data from random real images\n",
            "Expert Dir: ../train_expert\\CIFAR100_NO_ZCA\\ConvNet 2 buffers detected\n",
            "[2025-05-14 01:03:17] training begins\n",
            "-------------------------\n",
            "Experiment ID = 0\n",
            "loading file ../train_expert\\CIFAR100_NO_ZCA\\ConvNet\\replay_buffer_1.pt\n",
            "[INFO] Original labels saved to ../../labels_before_noise_IPC5_ExpEpoch24.pt\n",
            "[INFO] Noisy labels saved to ../../noisy_labels_top0.9_IPC5_ExpEpoch24.pt\n",
            "-------------------------\n",
            "Evaluation\n",
            "Experiment ID = 0, model_train = ConvNet, model_eval = ConvNet\n",
            "DSA augmentation strategy: \n",
            " color_crop_cutout_flip_scale_rotate\n",
            "DSA augmentation parameters: \n",
            " {'aug_mode': 'S', 'prob_flip': 0.5, 'ratio_scale': 1.2, 'ratio_rotate': 15.0, 'ratio_crop_pad': 0.125, 'ratio_cutout': 0.5, 'ratio_noise': 0.05, 'brightness': 1.0, 'saturation': 2.0, 'contrast': 0.5}\n",
            "[2025-05-14 01:03:59] Evaluate_00: epoch = 3000  train loss = 1.118258 train acc = 0.9880, test acc = 0.1205\n",
            "[2025-05-14 01:04:35] Evaluate_00: epoch = 3000  train loss = 0.855068 train acc = 1.0000, test acc = 0.1216\n",
            "[2025-05-14 01:05:11] Evaluate_00: epoch = 3000  train loss = 0.946416 train acc = 0.9980, test acc = 0.1267\n",
            "[2025-05-14 01:05:42] Evaluate_00: epoch = 3000  train loss = 0.846338 train acc = 1.0000, test acc = 0.1280\n",
            "[2025-05-14 01:06:14] Evaluate_00: epoch = 3000  train loss = 0.837692 train acc = 0.9980, test acc = 0.1280\n",
            "[2025-05-14 01:06:46] Evaluate_00: epoch = 3000  train loss = 0.815150 train acc = 1.0000, test acc = 0.1265\n",
            "[2025-05-14 01:07:33] Evaluate_00: epoch = 3000  train loss = 0.819330 train acc = 1.0000, test acc = 0.1260\n",
            "[2025-05-14 01:08:06] Evaluate_00: epoch = 3000  train loss = 0.854756 train acc = 1.0000, test acc = 0.1284\n",
            "[2025-05-14 01:08:40] Evaluate_00: epoch = 3000  train loss = 0.839470 train acc = 1.0000, test acc = 0.1299\n",
            "[2025-05-14 01:09:27] Evaluate_00: epoch = 3000  train loss = 0.890022 train acc = 1.0000, test acc = 0.1287\n",
            "[2025-05-14 01:10:10] Evaluate_00: epoch = 3000  train loss = 0.810032 train acc = 1.0000, test acc = 0.1295\n",
            "[2025-05-14 01:10:46] Evaluate_00: epoch = 3000  train loss = 0.868066 train acc = 1.0000, test acc = 0.1287\n",
            "[2025-05-14 01:11:23] Evaluate_00: epoch = 3000  train loss = 0.856087 train acc = 1.0000, test acc = 0.1288\n",
            "[2025-05-14 01:11:58] Evaluate_00: epoch = 3000  train loss = 0.859905 train acc = 1.0000, test acc = 0.1281\n",
            "[2025-05-14 01:12:35] Evaluate_00: epoch = 3000  train loss = 0.848021 train acc = 1.0000, test acc = 0.1288\n",
            "[2025-05-14 01:12:35] Evaluate_00: epoch = 3000 train time = 550 s train loss = 0.848021 train acc = 1.0000, test acc = 0.1288\n",
            "Evaluate 1 random ConvNet, mean = 0.1299 std = 0.0000\n",
            "-------------------------\n",
            "\n",
            "[✅] Output saved to output_ipc5_epoch24_Noisy0.9.txt\n",
            "\n",
            "▶️ Running: IPC=5, Expert Epoch=24, probability:0.95 \n",
            "\n",
            "CUDNN STATUS: True\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Hyper-parameters: \n",
            " {'dataset': 'CIFAR100', 'data_path': '/content/cifar100', 'expert_path': '../train_expert', 'ipc': 5, 'teacher_model': 'ConvNet', 'student_model': 'ConvNet', 'epoch_eval_train': 3000, 'optimizer': 'SGD', 'lr_net': 0.01, 'true_class_probability': 0.95, 'expt_type': 'add_noise', 'num_eval': 1, 'batch_train': 256, 'batch_real': 256, 'zca': False, 'load_checkpoint': None, 'selection_strategy': 'random', 'dsa': True, 'dsa_strategy': 'color_crop_cutout_flip_scale_rotate', 'teacher_label': True, 'max_expert_epoch': 24, 'min_expert_epoch': 1, 'ensemble': False, 'temp': 1.0, 'wandb': False, 'run_name': None, 'notes': 'No description', 'device': 'cuda', 'im_size': [32, 32], 'dc_aug_param': None, 'dsa_param': <utils.ParamDiffAug object at 0x000002400B0B6ED0>, '_wandb': {}, 'zca_trans': None, 'distributed': False}\n",
            "Evaluation model:  ConvNet\n",
            "---------------Build label to index map--------------\n",
            "initialize synthetic data from random real images\n",
            "Expert Dir: ../train_expert\\CIFAR100_NO_ZCA\\ConvNet 2 buffers detected\n",
            "[2025-05-14 01:13:24] training begins\n",
            "-------------------------\n",
            "Experiment ID = 0\n",
            "loading file ../train_expert\\CIFAR100_NO_ZCA\\ConvNet\\replay_buffer_1.pt\n",
            "[INFO] Original labels saved to ../../labels_before_noise_IPC5_ExpEpoch24.pt\n",
            "[INFO] Noisy labels saved to ../../noisy_labels_top0.95_IPC5_ExpEpoch24.pt\n",
            "-------------------------\n",
            "Evaluation\n",
            "Experiment ID = 0, model_train = ConvNet, model_eval = ConvNet\n",
            "DSA augmentation strategy: \n",
            " color_crop_cutout_flip_scale_rotate\n",
            "DSA augmentation parameters: \n",
            " {'aug_mode': 'S', 'prob_flip': 0.5, 'ratio_scale': 1.2, 'ratio_rotate': 15.0, 'ratio_crop_pad': 0.125, 'ratio_cutout': 0.5, 'ratio_noise': 0.05, 'brightness': 1.0, 'saturation': 2.0, 'contrast': 0.5}\n",
            "[2025-05-14 01:14:08] Evaluate_00: epoch = 3000  train loss = 0.492925 train acc = 1.0000, test acc = 0.1185\n",
            "[2025-05-14 01:14:43] Evaluate_00: epoch = 3000  train loss = 0.536108 train acc = 0.9980, test acc = 0.1202\n",
            "[2025-05-14 01:15:19] Evaluate_00: epoch = 3000  train loss = 0.530375 train acc = 1.0000, test acc = 0.1245\n",
            "[2025-05-14 01:15:55] Evaluate_00: epoch = 3000  train loss = 0.478708 train acc = 1.0000, test acc = 0.1262\n",
            "[2025-05-14 01:16:33] Evaluate_00: epoch = 3000  train loss = 0.475756 train acc = 1.0000, test acc = 0.1274\n",
            "[2025-05-14 01:17:08] Evaluate_00: epoch = 3000  train loss = 0.514672 train acc = 1.0000, test acc = 0.1283\n",
            "[2025-05-14 01:17:45] Evaluate_00: epoch = 3000  train loss = 0.522578 train acc = 1.0000, test acc = 0.1282\n",
            "[2025-05-14 01:18:21] Evaluate_00: epoch = 3000  train loss = 0.468032 train acc = 1.0000, test acc = 0.1270\n",
            "[2025-05-14 01:19:00] Evaluate_00: epoch = 3000  train loss = 0.497454 train acc = 1.0000, test acc = 0.1271\n",
            "[2025-05-14 01:19:37] Evaluate_00: epoch = 3000  train loss = 0.526316 train acc = 1.0000, test acc = 0.1249\n",
            "[2025-05-14 01:20:16] Evaluate_00: epoch = 3000  train loss = 0.485984 train acc = 1.0000, test acc = 0.1274\n",
            "[2025-05-14 01:20:52] Evaluate_00: epoch = 3000  train loss = 0.466106 train acc = 1.0000, test acc = 0.1281\n",
            "[2025-05-14 01:21:29] Evaluate_00: epoch = 3000  train loss = 0.465963 train acc = 1.0000, test acc = 0.1263\n",
            "[2025-05-14 01:22:07] Evaluate_00: epoch = 3000  train loss = 0.490538 train acc = 1.0000, test acc = 0.1263\n",
            "[2025-05-14 01:22:54] Evaluate_00: epoch = 3000  train loss = 0.458354 train acc = 1.0000, test acc = 0.1260\n",
            "[2025-05-14 01:22:54] Evaluate_00: epoch = 3000 train time = 562 s train loss = 0.458354 train acc = 1.0000, test acc = 0.1260\n",
            "Evaluate 1 random ConvNet, mean = 0.1283 std = 0.0000\n",
            "-------------------------\n",
            "\n",
            "[✅] Output saved to output_ipc5_epoch24_Noisy0.95.txt\n",
            "\n",
            "▶️ Running: IPC=5, Expert Epoch=24, probability:1 \n",
            "\n",
            "CUDNN STATUS: True\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Hyper-parameters: \n",
            " {'dataset': 'CIFAR100', 'data_path': '/content/cifar100', 'expert_path': '../train_expert', 'ipc': 5, 'teacher_model': 'ConvNet', 'student_model': 'ConvNet', 'epoch_eval_train': 3000, 'optimizer': 'SGD', 'lr_net': 0.01, 'true_class_probability': 1.0, 'expt_type': 'add_noise', 'num_eval': 1, 'batch_train': 256, 'batch_real': 256, 'zca': False, 'load_checkpoint': None, 'selection_strategy': 'random', 'dsa': True, 'dsa_strategy': 'color_crop_cutout_flip_scale_rotate', 'teacher_label': True, 'max_expert_epoch': 24, 'min_expert_epoch': 1, 'ensemble': False, 'temp': 1.0, 'wandb': False, 'run_name': None, 'notes': 'No description', 'device': 'cuda', 'im_size': [32, 32], 'dc_aug_param': None, 'dsa_param': <utils.ParamDiffAug object at 0x000002A9DF1F6B90>, '_wandb': {}, 'zca_trans': None, 'distributed': False}\n",
            "Evaluation model:  ConvNet\n",
            "---------------Build label to index map--------------\n",
            "initialize synthetic data from random real images\n",
            "Expert Dir: ../train_expert\\CIFAR100_NO_ZCA\\ConvNet 2 buffers detected\n",
            "[2025-05-14 01:23:37] training begins\n",
            "-------------------------\n",
            "Experiment ID = 0\n",
            "loading file ../train_expert\\CIFAR100_NO_ZCA\\ConvNet\\replay_buffer_1.pt\n",
            "[INFO] Original labels saved to ../../labels_before_noise_IPC5_ExpEpoch24.pt\n",
            "[INFO] Noisy labels saved to ../../noisy_labels_top1.0_IPC5_ExpEpoch24.pt\n",
            "-------------------------\n",
            "Evaluation\n",
            "Experiment ID = 0, model_train = ConvNet, model_eval = ConvNet\n",
            "DSA augmentation strategy: \n",
            " color_crop_cutout_flip_scale_rotate\n",
            "DSA augmentation parameters: \n",
            " {'aug_mode': 'S', 'prob_flip': 0.5, 'ratio_scale': 1.2, 'ratio_rotate': 15.0, 'ratio_crop_pad': 0.125, 'ratio_cutout': 0.5, 'ratio_noise': 0.05, 'brightness': 1.0, 'saturation': 2.0, 'contrast': 0.5}\n",
            "[2025-05-14 01:24:17] Evaluate_00: epoch = 3000  train loss = 0.213028 train acc = 0.9820, test acc = 0.1221\n",
            "[2025-05-14 01:24:49] Evaluate_00: epoch = 3000  train loss = 0.059751 train acc = 0.9960, test acc = 0.1252\n",
            "[2025-05-14 01:25:21] Evaluate_00: epoch = 3000  train loss = 0.009575 train acc = 0.9980, test acc = 0.1264\n",
            "[2025-05-14 01:25:52] Evaluate_00: epoch = 3000  train loss = 0.009954 train acc = 1.0000, test acc = 0.1258\n",
            "[2025-05-14 01:26:27] Evaluate_00: epoch = 3000  train loss = 0.001504 train acc = 1.0000, test acc = 0.1276\n",
            "[2025-05-14 01:27:00] Evaluate_00: epoch = 3000  train loss = 0.013195 train acc = 1.0000, test acc = 0.1276\n",
            "[2025-05-14 01:27:32] Evaluate_00: epoch = 3000  train loss = 0.010425 train acc = 1.0000, test acc = 0.1293\n",
            "[2025-05-14 01:28:04] Evaluate_00: epoch = 3000  train loss = 0.006074 train acc = 1.0000, test acc = 0.1280\n",
            "[2025-05-14 01:28:36] Evaluate_00: epoch = 3000  train loss = 0.005778 train acc = 1.0000, test acc = 0.1277\n",
            "[2025-05-14 01:29:08] Evaluate_00: epoch = 3000  train loss = 0.013210 train acc = 1.0000, test acc = 0.1285\n",
            "[2025-05-14 01:29:40] Evaluate_00: epoch = 3000  train loss = 0.012004 train acc = 1.0000, test acc = 0.1278\n",
            "[2025-05-14 01:30:13] Evaluate_00: epoch = 3000  train loss = 0.010279 train acc = 1.0000, test acc = 0.1287\n",
            "[2025-05-14 01:30:48] Evaluate_00: epoch = 3000  train loss = 0.004682 train acc = 1.0000, test acc = 0.1285\n",
            "[2025-05-14 01:31:20] Evaluate_00: epoch = 3000  train loss = 0.007803 train acc = 1.0000, test acc = 0.1282\n",
            "[2025-05-14 01:31:52] Evaluate_00: epoch = 3000  train loss = 0.016210 train acc = 1.0000, test acc = 0.1270\n",
            "[2025-05-14 01:31:52] Evaluate_00: epoch = 3000 train time = 486 s train loss = 0.016210 train acc = 1.0000, test acc = 0.1270\n",
            "Evaluate 1 random ConvNet, mean = 0.1293 std = 0.0000\n",
            "-------------------------\n",
            "\n",
            "[✅] Output saved to output_ipc5_epoch24_Noisy1.txt\n",
            "\n",
            "▶️ Running: IPC=10, Expert Epoch=57, probability:0.8 \n",
            "\n",
            "CUDNN STATUS: True\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Hyper-parameters: \n",
            " {'dataset': 'CIFAR100', 'data_path': '/content/cifar100', 'expert_path': '../train_expert', 'ipc': 10, 'teacher_model': 'ConvNet', 'student_model': 'ConvNet', 'epoch_eval_train': 3000, 'optimizer': 'SGD', 'lr_net': 0.01, 'true_class_probability': 0.8, 'expt_type': 'add_noise', 'num_eval': 1, 'batch_train': 256, 'batch_real': 256, 'zca': False, 'load_checkpoint': None, 'selection_strategy': 'random', 'dsa': True, 'dsa_strategy': 'color_crop_cutout_flip_scale_rotate', 'teacher_label': True, 'max_expert_epoch': 57, 'min_expert_epoch': 1, 'ensemble': False, 'temp': 1.0, 'wandb': False, 'run_name': None, 'notes': 'No description', 'device': 'cuda', 'im_size': [32, 32], 'dc_aug_param': None, 'dsa_param': <utils.ParamDiffAug object at 0x00000261C8DFE490>, '_wandb': {}, 'zca_trans': None, 'distributed': False}\n",
            "Evaluation model:  ConvNet\n",
            "---------------Build label to index map--------------\n",
            "initialize synthetic data from random real images\n",
            "Expert Dir: ../train_expert\\CIFAR100_NO_ZCA\\ConvNet 2 buffers detected\n",
            "[2025-05-14 01:32:35] training begins\n",
            "-------------------------\n",
            "Experiment ID = 0\n",
            "loading file ../train_expert\\CIFAR100_NO_ZCA\\ConvNet\\replay_buffer_1.pt\n",
            "[INFO] Original labels saved to ../../labels_before_noise_IPC10_ExpEpoch57.pt\n",
            "[INFO] Noisy labels saved to ../../noisy_labels_top0.8_IPC10_ExpEpoch57.pt\n",
            "-------------------------\n",
            "Evaluation\n",
            "Experiment ID = 0, model_train = ConvNet, model_eval = ConvNet\n",
            "DSA augmentation strategy: \n",
            " color_crop_cutout_flip_scale_rotate\n",
            "DSA augmentation parameters: \n",
            " {'aug_mode': 'S', 'prob_flip': 0.5, 'ratio_scale': 1.2, 'ratio_rotate': 15.0, 'ratio_crop_pad': 0.125, 'ratio_cutout': 0.5, 'ratio_noise': 0.05, 'brightness': 1.0, 'saturation': 2.0, 'contrast': 0.5}\n",
            "[2025-05-14 01:33:34] Evaluate_00: epoch = 3000  train loss = 1.687019 train acc = 0.9830, test acc = 0.1725\n",
            "[2025-05-14 01:34:19] Evaluate_00: epoch = 3000  train loss = 1.556960 train acc = 0.9970, test acc = 0.1773\n",
            "[2025-05-14 01:35:06] Evaluate_00: epoch = 3000  train loss = 1.504202 train acc = 0.9990, test acc = 0.1747\n",
            "[2025-05-14 01:35:52] Evaluate_00: epoch = 3000  train loss = 1.510292 train acc = 1.0000, test acc = 0.1801\n",
            "[2025-05-14 01:36:38] Evaluate_00: epoch = 3000  train loss = 1.501287 train acc = 1.0000, test acc = 0.1820\n",
            "[2025-05-14 01:37:26] Evaluate_00: epoch = 3000  train loss = 1.476816 train acc = 1.0000, test acc = 0.1840\n",
            "[2025-05-14 01:38:24] Evaluate_00: epoch = 3000  train loss = 1.489563 train acc = 1.0000, test acc = 0.1830\n",
            "[2025-05-14 01:39:10] Evaluate_00: epoch = 3000  train loss = 1.490142 train acc = 1.0000, test acc = 0.1853\n",
            "[2025-05-14 01:39:56] Evaluate_00: epoch = 3000  train loss = 1.476525 train acc = 1.0000, test acc = 0.1825\n",
            "[2025-05-14 01:40:42] Evaluate_00: epoch = 3000  train loss = 1.473648 train acc = 1.0000, test acc = 0.1834\n",
            "[2025-05-14 01:41:27] Evaluate_00: epoch = 3000  train loss = 1.478704 train acc = 1.0000, test acc = 0.1843\n",
            "[2025-05-14 01:42:13] Evaluate_00: epoch = 3000  train loss = 1.484901 train acc = 1.0000, test acc = 0.1840\n",
            "[2025-05-14 01:42:59] Evaluate_00: epoch = 3000  train loss = 1.487840 train acc = 1.0000, test acc = 0.1840\n",
            "[2025-05-14 01:43:44] Evaluate_00: epoch = 3000  train loss = 1.480902 train acc = 1.0000, test acc = 0.1816\n",
            "[2025-05-14 01:44:30] Evaluate_00: epoch = 3000  train loss = 1.484877 train acc = 0.9990, test acc = 0.1846\n",
            "[2025-05-14 01:44:30] Evaluate_00: epoch = 3000 train time = 701 s train loss = 1.484877 train acc = 0.9990, test acc = 0.1846\n",
            "Evaluate 1 random ConvNet, mean = 0.1853 std = 0.0000\n",
            "-------------------------\n",
            "\n",
            "[✅] Output saved to output_ipc10_epoch57_Noisy0.8.txt\n",
            "\n",
            "▶️ Running: IPC=10, Expert Epoch=57, probability:0.9 \n",
            "\n",
            "CUDNN STATUS: True\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Hyper-parameters: \n",
            " {'dataset': 'CIFAR100', 'data_path': '/content/cifar100', 'expert_path': '../train_expert', 'ipc': 10, 'teacher_model': 'ConvNet', 'student_model': 'ConvNet', 'epoch_eval_train': 3000, 'optimizer': 'SGD', 'lr_net': 0.01, 'true_class_probability': 0.9, 'expt_type': 'add_noise', 'num_eval': 1, 'batch_train': 256, 'batch_real': 256, 'zca': False, 'load_checkpoint': None, 'selection_strategy': 'random', 'dsa': True, 'dsa_strategy': 'color_crop_cutout_flip_scale_rotate', 'teacher_label': True, 'max_expert_epoch': 57, 'min_expert_epoch': 1, 'ensemble': False, 'temp': 1.0, 'wandb': False, 'run_name': None, 'notes': 'No description', 'device': 'cuda', 'im_size': [32, 32], 'dc_aug_param': None, 'dsa_param': <utils.ParamDiffAug object at 0x000002180EB8DE50>, '_wandb': {}, 'zca_trans': None, 'distributed': False}\n",
            "Evaluation model:  ConvNet\n",
            "---------------Build label to index map--------------\n",
            "initialize synthetic data from random real images\n",
            "Expert Dir: ../train_expert\\CIFAR100_NO_ZCA\\ConvNet 2 buffers detected\n",
            "[2025-05-14 01:45:13] training begins\n",
            "-------------------------\n",
            "Experiment ID = 0\n",
            "loading file ../train_expert\\CIFAR100_NO_ZCA\\ConvNet\\replay_buffer_1.pt\n",
            "[INFO] Original labels saved to ../../labels_before_noise_IPC10_ExpEpoch57.pt\n",
            "[INFO] Noisy labels saved to ../../noisy_labels_top0.9_IPC10_ExpEpoch57.pt\n",
            "-------------------------\n",
            "Evaluation\n",
            "Experiment ID = 0, model_train = ConvNet, model_eval = ConvNet\n",
            "DSA augmentation strategy: \n",
            " color_crop_cutout_flip_scale_rotate\n",
            "DSA augmentation parameters: \n",
            " {'aug_mode': 'S', 'prob_flip': 0.5, 'ratio_scale': 1.2, 'ratio_rotate': 15.0, 'ratio_crop_pad': 0.125, 'ratio_cutout': 0.5, 'ratio_noise': 0.05, 'brightness': 1.0, 'saturation': 2.0, 'contrast': 0.5}\n",
            "[2025-05-14 01:46:15] Evaluate_00: epoch = 3000  train loss = 0.957280 train acc = 0.9970, test acc = 0.1695\n",
            "[2025-05-14 01:47:01] Evaluate_00: epoch = 3000  train loss = 0.906711 train acc = 0.9990, test acc = 0.1708\n",
            "[2025-05-14 01:47:47] Evaluate_00: epoch = 3000  train loss = 0.854547 train acc = 1.0000, test acc = 0.1736\n",
            "[2025-05-14 01:48:32] Evaluate_00: epoch = 3000  train loss = 0.852951 train acc = 1.0000, test acc = 0.1785\n",
            "[2025-05-14 01:49:18] Evaluate_00: epoch = 3000  train loss = 0.857069 train acc = 0.9990, test acc = 0.1801\n",
            "[2025-05-14 01:50:04] Evaluate_00: epoch = 3000  train loss = 0.834618 train acc = 1.0000, test acc = 0.1786\n",
            "[2025-05-14 01:50:51] Evaluate_00: epoch = 3000  train loss = 0.854308 train acc = 1.0000, test acc = 0.1831\n",
            "[2025-05-14 01:51:38] Evaluate_00: epoch = 3000  train loss = 0.859985 train acc = 1.0000, test acc = 0.1807\n",
            "[2025-05-14 01:52:25] Evaluate_00: epoch = 3000  train loss = 0.849706 train acc = 1.0000, test acc = 0.1825\n",
            "[2025-05-14 01:53:25] Evaluate_00: epoch = 3000  train loss = 0.837046 train acc = 1.0000, test acc = 0.1816\n",
            "[2025-05-14 01:54:11] Evaluate_00: epoch = 3000  train loss = 0.843995 train acc = 1.0000, test acc = 0.1793\n",
            "[2025-05-14 01:54:57] Evaluate_00: epoch = 3000  train loss = 0.847756 train acc = 1.0000, test acc = 0.1807\n",
            "[2025-05-14 01:55:43] Evaluate_00: epoch = 3000  train loss = 0.844621 train acc = 1.0000, test acc = 0.1822\n",
            "[2025-05-14 01:56:31] Evaluate_00: epoch = 3000  train loss = 0.850201 train acc = 1.0000, test acc = 0.1802\n",
            "[2025-05-14 01:57:17] Evaluate_00: epoch = 3000  train loss = 0.827994 train acc = 1.0000, test acc = 0.1813\n",
            "[2025-05-14 01:57:17] Evaluate_00: epoch = 3000 train time = 710 s train loss = 0.827994 train acc = 1.0000, test acc = 0.1813\n",
            "Evaluate 1 random ConvNet, mean = 0.1831 std = 0.0000\n",
            "-------------------------\n",
            "\n",
            "[✅] Output saved to output_ipc10_epoch57_Noisy0.9.txt\n",
            "\n",
            "▶️ Running: IPC=10, Expert Epoch=57, probability:0.95 \n",
            "\n",
            "CUDNN STATUS: True\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Hyper-parameters: \n",
            " {'dataset': 'CIFAR100', 'data_path': '/content/cifar100', 'expert_path': '../train_expert', 'ipc': 10, 'teacher_model': 'ConvNet', 'student_model': 'ConvNet', 'epoch_eval_train': 3000, 'optimizer': 'SGD', 'lr_net': 0.01, 'true_class_probability': 0.95, 'expt_type': 'add_noise', 'num_eval': 1, 'batch_train': 256, 'batch_real': 256, 'zca': False, 'load_checkpoint': None, 'selection_strategy': 'random', 'dsa': True, 'dsa_strategy': 'color_crop_cutout_flip_scale_rotate', 'teacher_label': True, 'max_expert_epoch': 57, 'min_expert_epoch': 1, 'ensemble': False, 'temp': 1.0, 'wandb': False, 'run_name': None, 'notes': 'No description', 'device': 'cuda', 'im_size': [32, 32], 'dc_aug_param': None, 'dsa_param': <utils.ParamDiffAug object at 0x000001B9C9B1D1D0>, '_wandb': {}, 'zca_trans': None, 'distributed': False}\n",
            "Evaluation model:  ConvNet\n",
            "---------------Build label to index map--------------\n",
            "initialize synthetic data from random real images\n",
            "Expert Dir: ../train_expert\\CIFAR100_NO_ZCA\\ConvNet 2 buffers detected\n",
            "[2025-05-14 01:58:00] training begins\n",
            "-------------------------\n",
            "Experiment ID = 0\n",
            "loading file ../train_expert\\CIFAR100_NO_ZCA\\ConvNet\\replay_buffer_1.pt\n",
            "[INFO] Original labels saved to ../../labels_before_noise_IPC10_ExpEpoch57.pt\n",
            "[INFO] Noisy labels saved to ../../noisy_labels_top0.95_IPC10_ExpEpoch57.pt\n",
            "-------------------------\n",
            "Evaluation\n",
            "Experiment ID = 0, model_train = ConvNet, model_eval = ConvNet\n",
            "DSA augmentation strategy: \n",
            " color_crop_cutout_flip_scale_rotate\n",
            "DSA augmentation parameters: \n",
            " {'aug_mode': 'S', 'prob_flip': 0.5, 'ratio_scale': 1.2, 'ratio_rotate': 15.0, 'ratio_crop_pad': 0.125, 'ratio_cutout': 0.5, 'ratio_noise': 0.05, 'brightness': 1.0, 'saturation': 2.0, 'contrast': 0.5}\n",
            "[2025-05-14 01:59:00] Evaluate_00: epoch = 3000  train loss = 0.639957 train acc = 0.9890, test acc = 0.1778\n",
            "[2025-05-14 01:59:46] Evaluate_00: epoch = 3000  train loss = 0.519680 train acc = 0.9990, test acc = 0.1787\n",
            "[2025-05-14 02:00:31] Evaluate_00: epoch = 3000  train loss = 0.484371 train acc = 1.0000, test acc = 0.1773\n",
            "[2025-05-14 02:01:17] Evaluate_00: epoch = 3000  train loss = 0.485010 train acc = 1.0000, test acc = 0.1845\n",
            "[2025-05-14 02:02:03] Evaluate_00: epoch = 3000  train loss = 0.489073 train acc = 1.0000, test acc = 0.1860\n",
            "[2025-05-14 02:02:49] Evaluate_00: epoch = 3000  train loss = 0.488723 train acc = 1.0000, test acc = 0.1870\n",
            "[2025-05-14 02:03:35] Evaluate_00: epoch = 3000  train loss = 0.502317 train acc = 1.0000, test acc = 0.1873\n",
            "[2025-05-14 02:04:20] Evaluate_00: epoch = 3000  train loss = 0.470537 train acc = 1.0000, test acc = 0.1856\n",
            "[2025-05-14 02:05:08] Evaluate_00: epoch = 3000  train loss = 0.495557 train acc = 1.0000, test acc = 0.1864\n",
            "[2025-05-14 02:05:54] Evaluate_00: epoch = 3000  train loss = 0.478984 train acc = 1.0000, test acc = 0.1874\n",
            "[2025-05-14 02:06:39] Evaluate_00: epoch = 3000  train loss = 0.492291 train acc = 1.0000, test acc = 0.1867\n",
            "[2025-05-14 02:07:24] Evaluate_00: epoch = 3000  train loss = 0.461336 train acc = 1.0000, test acc = 0.1858\n",
            "[2025-05-14 02:08:24] Evaluate_00: epoch = 3000  train loss = 0.473990 train acc = 1.0000, test acc = 0.1883\n",
            "[2025-05-14 02:09:12] Evaluate_00: epoch = 3000  train loss = 0.468948 train acc = 1.0000, test acc = 0.1875\n",
            "[2025-05-14 02:09:58] Evaluate_00: epoch = 3000  train loss = 0.471629 train acc = 1.0000, test acc = 0.1863\n",
            "[2025-05-14 02:09:58] Evaluate_00: epoch = 3000 train time = 704 s train loss = 0.471629 train acc = 1.0000, test acc = 0.1863\n",
            "Evaluate 1 random ConvNet, mean = 0.1883 std = 0.0000\n",
            "-------------------------\n",
            "\n",
            "[✅] Output saved to output_ipc10_epoch57_Noisy0.95.txt\n",
            "\n",
            "▶️ Running: IPC=10, Expert Epoch=57, probability:1 \n",
            "\n",
            "CUDNN STATUS: True\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Hyper-parameters: \n",
            " {'dataset': 'CIFAR100', 'data_path': '/content/cifar100', 'expert_path': '../train_expert', 'ipc': 10, 'teacher_model': 'ConvNet', 'student_model': 'ConvNet', 'epoch_eval_train': 3000, 'optimizer': 'SGD', 'lr_net': 0.01, 'true_class_probability': 1.0, 'expt_type': 'add_noise', 'num_eval': 1, 'batch_train': 256, 'batch_real': 256, 'zca': False, 'load_checkpoint': None, 'selection_strategy': 'random', 'dsa': True, 'dsa_strategy': 'color_crop_cutout_flip_scale_rotate', 'teacher_label': True, 'max_expert_epoch': 57, 'min_expert_epoch': 1, 'ensemble': False, 'temp': 1.0, 'wandb': False, 'run_name': None, 'notes': 'No description', 'device': 'cuda', 'im_size': [32, 32], 'dc_aug_param': None, 'dsa_param': <utils.ParamDiffAug object at 0x000001C1B0C59590>, '_wandb': {}, 'zca_trans': None, 'distributed': False}\n",
            "Evaluation model:  ConvNet\n",
            "---------------Build label to index map--------------\n",
            "initialize synthetic data from random real images\n",
            "Expert Dir: ../train_expert\\CIFAR100_NO_ZCA\\ConvNet 2 buffers detected\n",
            "[2025-05-14 02:10:42] training begins\n",
            "-------------------------\n",
            "Experiment ID = 0\n",
            "loading file ../train_expert\\CIFAR100_NO_ZCA\\ConvNet\\replay_buffer_1.pt\n",
            "[INFO] Original labels saved to ../../labels_before_noise_IPC10_ExpEpoch57.pt\n",
            "[INFO] Noisy labels saved to ../../noisy_labels_top1.0_IPC10_ExpEpoch57.pt\n",
            "-------------------------\n",
            "Evaluation\n",
            "Experiment ID = 0, model_train = ConvNet, model_eval = ConvNet\n",
            "DSA augmentation strategy: \n",
            " color_crop_cutout_flip_scale_rotate\n",
            "DSA augmentation parameters: \n",
            " {'aug_mode': 'S', 'prob_flip': 0.5, 'ratio_scale': 1.2, 'ratio_rotate': 15.0, 'ratio_crop_pad': 0.125, 'ratio_cutout': 0.5, 'ratio_noise': 0.05, 'brightness': 1.0, 'saturation': 2.0, 'contrast': 0.5}\n",
            "[2025-05-14 02:11:42] Evaluate_00: epoch = 3000  train loss = 0.110172 train acc = 0.9870, test acc = 0.1709\n",
            "[2025-05-14 02:12:28] Evaluate_00: epoch = 3000  train loss = 0.036800 train acc = 0.9990, test acc = 0.1796\n",
            "[2025-05-14 02:13:14] Evaluate_00: epoch = 3000  train loss = 0.029086 train acc = 0.9980, test acc = 0.1837\n",
            "[2025-05-14 02:13:59] Evaluate_00: epoch = 3000  train loss = 0.010937 train acc = 1.0000, test acc = 0.1908\n",
            "[2025-05-14 02:14:44] Evaluate_00: epoch = 3000  train loss = 0.012315 train acc = 1.0000, test acc = 0.1890\n",
            "[2025-05-14 02:15:31] Evaluate_00: epoch = 3000  train loss = 0.019898 train acc = 1.0000, test acc = 0.1898\n",
            "[2025-05-14 02:16:18] Evaluate_00: epoch = 3000  train loss = 0.015742 train acc = 1.0000, test acc = 0.1914\n",
            "[2025-05-14 02:17:04] Evaluate_00: epoch = 3000  train loss = 0.011492 train acc = 1.0000, test acc = 0.1909\n",
            "[2025-05-14 02:17:50] Evaluate_00: epoch = 3000  train loss = 0.014867 train acc = 1.0000, test acc = 0.1909\n",
            "[2025-05-14 02:18:36] Evaluate_00: epoch = 3000  train loss = 0.006230 train acc = 1.0000, test acc = 0.1901\n",
            "[2025-05-14 02:19:22] Evaluate_00: epoch = 3000  train loss = 0.016001 train acc = 1.0000, test acc = 0.1902\n",
            "[2025-05-14 02:20:09] Evaluate_00: epoch = 3000  train loss = 0.020234 train acc = 1.0000, test acc = 0.1909\n",
            "[2025-05-14 02:20:55] Evaluate_00: epoch = 3000  train loss = 0.005425 train acc = 1.0000, test acc = 0.1906\n",
            "[2025-05-14 02:21:42] Evaluate_00: epoch = 3000  train loss = 0.010313 train acc = 1.0000, test acc = 0.1916\n",
            "[2025-05-14 02:22:30] Evaluate_00: epoch = 3000  train loss = 0.011852 train acc = 1.0000, test acc = 0.1908\n",
            "[2025-05-14 02:22:30] Evaluate_00: epoch = 3000 train time = 693 s train loss = 0.011852 train acc = 1.0000, test acc = 0.1908\n",
            "Evaluate 1 random ConvNet, mean = 0.1916 std = 0.0000\n",
            "-------------------------\n",
            "\n",
            "[✅] Output saved to output_ipc10_epoch57_Noisy1.txt\n",
            "\n",
            "▶️ Running: IPC=50, Expert Epoch=125, probability:0.8 \n",
            "\n",
            "CUDNN STATUS: True\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Hyper-parameters: \n",
            " {'dataset': 'CIFAR100', 'data_path': '/content/cifar100', 'expert_path': '../train_expert', 'ipc': 50, 'teacher_model': 'ConvNet', 'student_model': 'ConvNet', 'epoch_eval_train': 3000, 'optimizer': 'SGD', 'lr_net': 0.01, 'true_class_probability': 0.8, 'expt_type': 'add_noise', 'num_eval': 1, 'batch_train': 256, 'batch_real': 256, 'zca': False, 'load_checkpoint': None, 'selection_strategy': 'random', 'dsa': True, 'dsa_strategy': 'color_crop_cutout_flip_scale_rotate', 'teacher_label': True, 'max_expert_epoch': 125, 'min_expert_epoch': 1, 'ensemble': False, 'temp': 1.0, 'wandb': False, 'run_name': None, 'notes': 'No description', 'device': 'cuda', 'im_size': [32, 32], 'dc_aug_param': None, 'dsa_param': <utils.ParamDiffAug object at 0x000001727FE51610>, '_wandb': {}, 'zca_trans': None, 'distributed': False}\n",
            "Evaluation model:  ConvNet\n",
            "---------------Build label to index map--------------\n",
            "initialize synthetic data from random real images\n",
            "Expert Dir: ../train_expert\\CIFAR100_NO_ZCA\\ConvNet 2 buffers detected\n",
            "[2025-05-14 02:23:30] training begins\n",
            "-------------------------\n",
            "Experiment ID = 0\n",
            "loading file ../train_expert\\CIFAR100_NO_ZCA\\ConvNet\\replay_buffer_1.pt\n",
            "[INFO] Original labels saved to ../../labels_before_noise_IPC50_ExpEpoch125.pt\n",
            "[INFO] Noisy labels saved to ../../noisy_labels_top0.8_IPC50_ExpEpoch125.pt\n",
            "-------------------------\n",
            "Evaluation\n",
            "Experiment ID = 0, model_train = ConvNet, model_eval = ConvNet\n",
            "DSA augmentation strategy: \n",
            " color_crop_cutout_flip_scale_rotate\n",
            "DSA augmentation parameters: \n",
            " {'aug_mode': 'S', 'prob_flip': 0.5, 'ratio_scale': 1.2, 'ratio_rotate': 15.0, 'ratio_crop_pad': 0.125, 'ratio_cutout': 0.5, 'ratio_noise': 0.05, 'brightness': 1.0, 'saturation': 2.0, 'contrast': 0.5}\n",
            "[2025-05-14 02:27:10] Evaluate_00: epoch = 3000  train loss = 1.748903 train acc = 0.9708, test acc = 0.3156\n",
            "[2025-05-14 02:29:48] Evaluate_00: epoch = 3000  train loss = 1.685479 train acc = 0.9852, test acc = 0.3182\n",
            "[2025-05-14 02:32:28] Evaluate_00: epoch = 3000  train loss = 1.626211 train acc = 0.9926, test acc = 0.3206\n",
            "[2025-05-14 02:35:08] Evaluate_00: epoch = 3000  train loss = 1.570037 train acc = 0.9984, test acc = 0.3283\n",
            "[2025-05-14 02:37:47] Evaluate_00: epoch = 3000  train loss = 1.570433 train acc = 0.9976, test acc = 0.3288\n",
            "[2025-05-14 02:40:37] Evaluate_00: epoch = 3000  train loss = 1.574173 train acc = 0.9984, test acc = 0.3292\n",
            "[2025-05-14 02:43:16] Evaluate_00: epoch = 3000  train loss = 1.562983 train acc = 0.9980, test acc = 0.3287\n",
            "[2025-05-14 02:45:54] Evaluate_00: epoch = 3000  train loss = 1.572712 train acc = 0.9974, test acc = 0.3326\n",
            "[2025-05-14 02:48:31] Evaluate_00: epoch = 3000  train loss = 1.548992 train acc = 0.9986, test acc = 0.3327\n",
            "[2025-05-14 02:51:10] Evaluate_00: epoch = 3000  train loss = 1.549252 train acc = 0.9986, test acc = 0.3300\n",
            "[2025-05-14 02:54:03] Evaluate_00: epoch = 3000  train loss = 1.558057 train acc = 0.9976, test acc = 0.3312\n",
            "[2025-05-14 02:56:41] Evaluate_00: epoch = 3000  train loss = 1.544794 train acc = 0.9994, test acc = 0.3306\n",
            "[2025-05-14 02:59:18] Evaluate_00: epoch = 3000  train loss = 1.560765 train acc = 0.9984, test acc = 0.3319\n",
            "[2025-05-14 03:01:56] Evaluate_00: epoch = 3000  train loss = 1.536327 train acc = 0.9992, test acc = 0.3331\n",
            "[2025-05-14 03:04:35] Evaluate_00: epoch = 3000  train loss = 1.542715 train acc = 0.9994, test acc = 0.3321\n",
            "[2025-05-14 03:04:35] Evaluate_00: epoch = 3000 train time = 2403 s train loss = 1.542715 train acc = 0.9994, test acc = 0.3321\n",
            "Evaluate 1 random ConvNet, mean = 0.3331 std = 0.0000\n",
            "-------------------------\n",
            "\n",
            "[✅] Output saved to output_ipc50_epoch125_Noisy0.8.txt\n",
            "\n",
            "▶️ Running: IPC=50, Expert Epoch=125, probability:0.9 \n",
            "\n",
            "CUDNN STATUS: True\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Hyper-parameters: \n",
            " {'dataset': 'CIFAR100', 'data_path': '/content/cifar100', 'expert_path': '../train_expert', 'ipc': 50, 'teacher_model': 'ConvNet', 'student_model': 'ConvNet', 'epoch_eval_train': 3000, 'optimizer': 'SGD', 'lr_net': 0.01, 'true_class_probability': 0.9, 'expt_type': 'add_noise', 'num_eval': 1, 'batch_train': 256, 'batch_real': 256, 'zca': False, 'load_checkpoint': None, 'selection_strategy': 'random', 'dsa': True, 'dsa_strategy': 'color_crop_cutout_flip_scale_rotate', 'teacher_label': True, 'max_expert_epoch': 125, 'min_expert_epoch': 1, 'ensemble': False, 'temp': 1.0, 'wandb': False, 'run_name': None, 'notes': 'No description', 'device': 'cuda', 'im_size': [32, 32], 'dc_aug_param': None, 'dsa_param': <utils.ParamDiffAug object at 0x00000216439C1A50>, '_wandb': {}, 'zca_trans': None, 'distributed': False}\n",
            "Evaluation model:  ConvNet\n",
            "---------------Build label to index map--------------\n",
            "initialize synthetic data from random real images\n",
            "Expert Dir: ../train_expert\\CIFAR100_NO_ZCA\\ConvNet 2 buffers detected\n",
            "[2025-05-14 03:05:21] training begins\n",
            "-------------------------\n",
            "Experiment ID = 0\n",
            "loading file ../train_expert\\CIFAR100_NO_ZCA\\ConvNet\\replay_buffer_1.pt\n",
            "[INFO] Original labels saved to ../../labels_before_noise_IPC50_ExpEpoch125.pt\n",
            "[INFO] Noisy labels saved to ../../noisy_labels_top0.9_IPC50_ExpEpoch125.pt\n",
            "-------------------------\n",
            "Evaluation\n",
            "Experiment ID = 0, model_train = ConvNet, model_eval = ConvNet\n",
            "DSA augmentation strategy: \n",
            " color_crop_cutout_flip_scale_rotate\n",
            "DSA augmentation parameters: \n",
            " {'aug_mode': 'S', 'prob_flip': 0.5, 'ratio_scale': 1.2, 'ratio_rotate': 15.0, 'ratio_crop_pad': 0.125, 'ratio_cutout': 0.5, 'ratio_noise': 0.05, 'brightness': 1.0, 'saturation': 2.0, 'contrast': 0.5}\n",
            "[2025-05-14 03:09:15] Evaluate_00: epoch = 3000  train loss = 1.082145 train acc = 0.9718, test acc = 0.3120\n",
            "[2025-05-14 03:11:53] Evaluate_00: epoch = 3000  train loss = 0.986963 train acc = 0.9928, test acc = 0.3232\n",
            "[2025-05-14 03:14:30] Evaluate_00: epoch = 3000  train loss = 0.967735 train acc = 0.9942, test acc = 0.3212\n",
            "[2025-05-14 03:17:09] Evaluate_00: epoch = 3000  train loss = 0.924879 train acc = 0.9988, test acc = 0.3320\n",
            "[2025-05-14 03:19:48] Evaluate_00: epoch = 3000  train loss = 0.922034 train acc = 0.9982, test acc = 0.3302\n",
            "[2025-05-14 03:22:27] Evaluate_00: epoch = 3000  train loss = 0.926651 train acc = 0.9980, test acc = 0.3353\n",
            "[2025-05-14 03:25:18] Evaluate_00: epoch = 3000  train loss = 0.904233 train acc = 0.9974, test acc = 0.3316\n",
            "[2025-05-14 03:27:58] Evaluate_00: epoch = 3000  train loss = 0.899350 train acc = 0.9996, test acc = 0.3349\n",
            "[2025-05-14 03:30:36] Evaluate_00: epoch = 3000  train loss = 0.883435 train acc = 0.9992, test acc = 0.3327\n",
            "[2025-05-14 03:33:14] Evaluate_00: epoch = 3000  train loss = 0.892249 train acc = 0.9992, test acc = 0.3354\n",
            "[2025-05-14 03:35:54] Evaluate_00: epoch = 3000  train loss = 0.885333 train acc = 0.9998, test acc = 0.3353\n",
            "[2025-05-14 03:38:47] Evaluate_00: epoch = 3000  train loss = 0.882082 train acc = 0.9994, test acc = 0.3366\n",
            "[2025-05-14 03:41:24] Evaluate_00: epoch = 3000  train loss = 0.903554 train acc = 0.9992, test acc = 0.3361\n",
            "[2025-05-14 03:44:01] Evaluate_00: epoch = 3000  train loss = 0.887175 train acc = 0.9994, test acc = 0.3363\n",
            "[2025-05-14 03:46:39] Evaluate_00: epoch = 3000  train loss = 0.892190 train acc = 0.9994, test acc = 0.3362\n",
            "[2025-05-14 03:46:39] Evaluate_00: epoch = 3000 train time = 2417 s train loss = 0.892190 train acc = 0.9994, test acc = 0.3362\n",
            "Evaluate 1 random ConvNet, mean = 0.3366 std = 0.0000\n",
            "-------------------------\n",
            "\n",
            "[✅] Output saved to output_ipc50_epoch125_Noisy0.9.txt\n",
            "\n",
            "▶️ Running: IPC=50, Expert Epoch=125, probability:0.95 \n",
            "\n",
            "CUDNN STATUS: True\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Hyper-parameters: \n",
            " {'dataset': 'CIFAR100', 'data_path': '/content/cifar100', 'expert_path': '../train_expert', 'ipc': 50, 'teacher_model': 'ConvNet', 'student_model': 'ConvNet', 'epoch_eval_train': 3000, 'optimizer': 'SGD', 'lr_net': 0.01, 'true_class_probability': 0.95, 'expt_type': 'add_noise', 'num_eval': 1, 'batch_train': 256, 'batch_real': 256, 'zca': False, 'load_checkpoint': None, 'selection_strategy': 'random', 'dsa': True, 'dsa_strategy': 'color_crop_cutout_flip_scale_rotate', 'teacher_label': True, 'max_expert_epoch': 125, 'min_expert_epoch': 1, 'ensemble': False, 'temp': 1.0, 'wandb': False, 'run_name': None, 'notes': 'No description', 'device': 'cuda', 'im_size': [32, 32], 'dc_aug_param': None, 'dsa_param': <utils.ParamDiffAug object at 0x000001FA7B03EC10>, '_wandb': {}, 'zca_trans': None, 'distributed': False}\n",
            "Evaluation model:  ConvNet\n",
            "---------------Build label to index map--------------\n",
            "initialize synthetic data from random real images\n",
            "Expert Dir: ../train_expert\\CIFAR100_NO_ZCA\\ConvNet 2 buffers detected\n",
            "[2025-05-14 03:47:24] training begins\n",
            "-------------------------\n",
            "Experiment ID = 0\n",
            "loading file ../train_expert\\CIFAR100_NO_ZCA\\ConvNet\\replay_buffer_1.pt\n",
            "[INFO] Original labels saved to ../../labels_before_noise_IPC50_ExpEpoch125.pt\n",
            "[INFO] Noisy labels saved to ../../noisy_labels_top0.95_IPC50_ExpEpoch125.pt\n",
            "-------------------------\n",
            "Evaluation\n",
            "Experiment ID = 0, model_train = ConvNet, model_eval = ConvNet\n",
            "DSA augmentation strategy: \n",
            " color_crop_cutout_flip_scale_rotate\n",
            "DSA augmentation parameters: \n",
            " {'aug_mode': 'S', 'prob_flip': 0.5, 'ratio_scale': 1.2, 'ratio_rotate': 15.0, 'ratio_crop_pad': 0.125, 'ratio_cutout': 0.5, 'ratio_noise': 0.05, 'brightness': 1.0, 'saturation': 2.0, 'contrast': 0.5}\n",
            "[2025-05-14 03:51:05] Evaluate_00: epoch = 3000  train loss = 0.714129 train acc = 0.9646, test acc = 0.3119\n",
            "[2025-05-14 03:53:58] Evaluate_00: epoch = 3000  train loss = 0.636222 train acc = 0.9860, test acc = 0.3235\n",
            "[2025-05-14 03:56:35] Evaluate_00: epoch = 3000  train loss = 0.563024 train acc = 0.9950, test acc = 0.3290\n",
            "[2025-05-14 03:59:12] Evaluate_00: epoch = 3000  train loss = 0.554624 train acc = 0.9976, test acc = 0.3361\n",
            "[2025-05-14 04:01:49] Evaluate_00: epoch = 3000  train loss = 0.524256 train acc = 0.9986, test acc = 0.3382\n",
            "[2025-05-14 04:04:29] Evaluate_00: epoch = 3000  train loss = 0.534279 train acc = 0.9982, test acc = 0.3414\n",
            "[2025-05-14 04:07:08] Evaluate_00: epoch = 3000  train loss = 0.524539 train acc = 0.9994, test acc = 0.3381\n",
            "[2025-05-14 04:10:02] Evaluate_00: epoch = 3000  train loss = 0.518082 train acc = 1.0000, test acc = 0.3447\n",
            "[2025-05-14 04:12:43] Evaluate_00: epoch = 3000  train loss = 0.525544 train acc = 0.9992, test acc = 0.3441\n",
            "[2025-05-14 04:15:22] Evaluate_00: epoch = 3000  train loss = 0.508098 train acc = 0.9996, test acc = 0.3443\n",
            "[2025-05-14 04:18:00] Evaluate_00: epoch = 3000  train loss = 0.513112 train acc = 0.9994, test acc = 0.3461\n",
            "[2025-05-14 04:20:39] Evaluate_00: epoch = 3000  train loss = 0.517208 train acc = 0.9998, test acc = 0.3465\n",
            "[2025-05-14 04:23:19] Evaluate_00: epoch = 3000  train loss = 0.514761 train acc = 0.9996, test acc = 0.3454\n",
            "[2025-05-14 04:26:08] Evaluate_00: epoch = 3000  train loss = 0.525302 train acc = 0.9998, test acc = 0.3466\n",
            "[2025-05-14 04:28:47] Evaluate_00: epoch = 3000  train loss = 0.534533 train acc = 0.9996, test acc = 0.3480\n",
            "[2025-05-14 04:28:47] Evaluate_00: epoch = 3000 train time = 2421 s train loss = 0.534533 train acc = 0.9996, test acc = 0.3480\n",
            "Evaluate 1 random ConvNet, mean = 0.3480 std = 0.0000\n",
            "-------------------------\n",
            "\n",
            "[✅] Output saved to output_ipc50_epoch125_Noisy0.95.txt\n",
            "\n",
            "▶️ Running: IPC=50, Expert Epoch=125, probability:1 \n",
            "\n",
            "CUDNN STATUS: True\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Hyper-parameters: \n",
            " {'dataset': 'CIFAR100', 'data_path': '/content/cifar100', 'expert_path': '../train_expert', 'ipc': 50, 'teacher_model': 'ConvNet', 'student_model': 'ConvNet', 'epoch_eval_train': 3000, 'optimizer': 'SGD', 'lr_net': 0.01, 'true_class_probability': 1.0, 'expt_type': 'add_noise', 'num_eval': 1, 'batch_train': 256, 'batch_real': 256, 'zca': False, 'load_checkpoint': None, 'selection_strategy': 'random', 'dsa': True, 'dsa_strategy': 'color_crop_cutout_flip_scale_rotate', 'teacher_label': True, 'max_expert_epoch': 125, 'min_expert_epoch': 1, 'ensemble': False, 'temp': 1.0, 'wandb': False, 'run_name': None, 'notes': 'No description', 'device': 'cuda', 'im_size': [32, 32], 'dc_aug_param': None, 'dsa_param': <utils.ParamDiffAug object at 0x000002165CDD5590>, '_wandb': {}, 'zca_trans': None, 'distributed': False}\n",
            "Evaluation model:  ConvNet\n",
            "---------------Build label to index map--------------\n",
            "initialize synthetic data from random real images\n",
            "Expert Dir: ../train_expert\\CIFAR100_NO_ZCA\\ConvNet 2 buffers detected\n",
            "[2025-05-14 04:29:31] training begins\n",
            "-------------------------\n",
            "Experiment ID = 0\n",
            "loading file ../train_expert\\CIFAR100_NO_ZCA\\ConvNet\\replay_buffer_1.pt\n",
            "[INFO] Original labels saved to ../../labels_before_noise_IPC50_ExpEpoch125.pt\n",
            "[INFO] Noisy labels saved to ../../noisy_labels_top1.0_IPC50_ExpEpoch125.pt\n",
            "-------------------------\n",
            "Evaluation\n",
            "Experiment ID = 0, model_train = ConvNet, model_eval = ConvNet\n",
            "DSA augmentation strategy: \n",
            " color_crop_cutout_flip_scale_rotate\n",
            "DSA augmentation parameters: \n",
            " {'aug_mode': 'S', 'prob_flip': 0.5, 'ratio_scale': 1.2, 'ratio_rotate': 15.0, 'ratio_crop_pad': 0.125, 'ratio_cutout': 0.5, 'ratio_noise': 0.05, 'brightness': 1.0, 'saturation': 2.0, 'contrast': 0.5}\n",
            "[2025-05-14 04:33:13] Evaluate_00: epoch = 3000  train loss = 0.176292 train acc = 0.9628, test acc = 0.3323\n",
            "[2025-05-14 04:35:50] Evaluate_00: epoch = 3000  train loss = 0.068625 train acc = 0.9900, test acc = 0.3398\n",
            "[2025-05-14 04:38:34] Evaluate_00: epoch = 3000  train loss = 0.045195 train acc = 0.9946, test acc = 0.3385\n",
            "[2025-05-14 04:41:24] Evaluate_00: epoch = 3000  train loss = 0.038466 train acc = 0.9976, test acc = 0.3505\n",
            "[2025-05-14 04:44:01] Evaluate_00: epoch = 3000  train loss = 0.033431 train acc = 0.9974, test acc = 0.3501\n",
            "[2025-05-14 04:46:38] Evaluate_00: epoch = 3000  train loss = 0.034795 train acc = 0.9984, test acc = 0.3483\n",
            "[2025-05-14 04:49:15] Evaluate_00: epoch = 3000  train loss = 0.023760 train acc = 0.9994, test acc = 0.3553\n",
            "[2025-05-14 04:52:18] Evaluate_00: epoch = 3000  train loss = 0.026999 train acc = 0.9992, test acc = 0.3585\n",
            "[2025-05-14 04:55:14] Evaluate_00: epoch = 3000  train loss = 0.028922 train acc = 0.9984, test acc = 0.3612\n",
            "[2025-05-14 04:57:52] Evaluate_00: epoch = 3000  train loss = 0.019827 train acc = 0.9998, test acc = 0.3584\n",
            "[2025-05-14 05:00:29] Evaluate_00: epoch = 3000  train loss = 0.021279 train acc = 1.0000, test acc = 0.3582\n",
            "[2025-05-14 05:03:08] Evaluate_00: epoch = 3000  train loss = 0.022927 train acc = 0.9996, test acc = 0.3618\n",
            "[2025-05-14 05:05:47] Evaluate_00: epoch = 3000  train loss = 0.022773 train acc = 0.9992, test acc = 0.3605\n",
            "[2025-05-14 05:08:24] Evaluate_00: epoch = 3000  train loss = 0.029916 train acc = 0.9994, test acc = 0.3616\n",
            "[2025-05-14 05:11:18] Evaluate_00: epoch = 3000  train loss = 0.022274 train acc = 0.9996, test acc = 0.3600\n",
            "[2025-05-14 05:11:18] Evaluate_00: epoch = 3000 train time = 2445 s train loss = 0.022274 train acc = 0.9996, test acc = 0.3600\n",
            "Evaluate 1 random ConvNet, mean = 0.3618 std = 0.0000\n",
            "-------------------------\n",
            "\n",
            "[✅] Output saved to output_ipc50_epoch125_Noisy1.txt\n",
            "\n",
            "▶️ Running: IPC=100, Expert Epoch=125, probability:0.8 \n",
            "\n",
            "CUDNN STATUS: True\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Hyper-parameters: \n",
            " {'dataset': 'CIFAR100', 'data_path': '/content/cifar100', 'expert_path': '../train_expert', 'ipc': 100, 'teacher_model': 'ConvNet', 'student_model': 'ConvNet', 'epoch_eval_train': 3000, 'optimizer': 'SGD', 'lr_net': 0.01, 'true_class_probability': 0.8, 'expt_type': 'add_noise', 'num_eval': 1, 'batch_train': 256, 'batch_real': 256, 'zca': False, 'load_checkpoint': None, 'selection_strategy': 'random', 'dsa': True, 'dsa_strategy': 'color_crop_cutout_flip_scale_rotate', 'teacher_label': True, 'max_expert_epoch': 125, 'min_expert_epoch': 1, 'ensemble': False, 'temp': 1.0, 'wandb': False, 'run_name': None, 'notes': 'No description', 'device': 'cuda', 'im_size': [32, 32], 'dc_aug_param': None, 'dsa_param': <utils.ParamDiffAug object at 0x000002B03678EBD0>, '_wandb': {}, 'zca_trans': None, 'distributed': False}\n",
            "Evaluation model:  ConvNet\n",
            "---------------Build label to index map--------------\n",
            "initialize synthetic data from random real images\n",
            "Expert Dir: ../train_expert\\CIFAR100_NO_ZCA\\ConvNet 2 buffers detected\n",
            "[2025-05-14 05:12:05] training begins\n",
            "-------------------------\n",
            "Experiment ID = 0\n",
            "loading file ../train_expert\\CIFAR100_NO_ZCA\\ConvNet\\replay_buffer_1.pt\n",
            "[INFO] Original labels saved to ../../labels_before_noise_IPC100_ExpEpoch125.pt\n",
            "[INFO] Noisy labels saved to ../../noisy_labels_top0.8_IPC100_ExpEpoch125.pt\n",
            "-------------------------\n",
            "Evaluation\n",
            "Experiment ID = 0, model_train = ConvNet, model_eval = ConvNet\n",
            "DSA augmentation strategy: \n",
            " color_crop_cutout_flip_scale_rotate\n",
            "DSA augmentation parameters: \n",
            " {'aug_mode': 'S', 'prob_flip': 0.5, 'ratio_scale': 1.2, 'ratio_rotate': 15.0, 'ratio_crop_pad': 0.125, 'ratio_cutout': 0.5, 'ratio_noise': 0.05, 'brightness': 1.0, 'saturation': 2.0, 'contrast': 0.5}\n",
            "[2025-05-14 05:19:04] Evaluate_00: epoch = 3000  train loss = 1.821500 train acc = 0.9448, test acc = 0.3858\n",
            "[2025-05-14 05:24:17] Evaluate_00: epoch = 3000  train loss = 1.732649 train acc = 0.9740, test acc = 0.3918\n",
            "[2025-05-14 05:29:15] Evaluate_00: epoch = 3000  train loss = 1.757126 train acc = 0.9693, test acc = 0.3980\n",
            "[2025-05-14 05:34:16] Evaluate_00: epoch = 3000  train loss = 1.656389 train acc = 0.9887, test acc = 0.4131\n",
            "[2025-05-14 05:39:29] Evaluate_00: epoch = 3000  train loss = 1.658592 train acc = 0.9918, test acc = 0.4130\n",
            "[2025-05-14 05:44:29] Evaluate_00: epoch = 3000  train loss = 1.628245 train acc = 0.9938, test acc = 0.4134\n",
            "[2025-05-14 05:49:29] Evaluate_00: epoch = 3000  train loss = 1.660475 train acc = 0.9897, test acc = 0.4121\n",
            "[2025-05-14 05:54:43] Evaluate_00: epoch = 3000  train loss = 1.640542 train acc = 0.9911, test acc = 0.4165\n",
            "[2025-05-14 05:59:42] Evaluate_00: epoch = 3000  train loss = 1.605990 train acc = 0.9952, test acc = 0.4141\n",
            "[2025-05-14 06:04:41] Evaluate_00: epoch = 3000  train loss = 1.667412 train acc = 0.9874, test acc = 0.4142\n",
            "[2025-05-14 06:09:57] Evaluate_00: epoch = 3000  train loss = 1.633173 train acc = 0.9935, test acc = 0.4146\n",
            "[2025-05-14 06:14:54] Evaluate_00: epoch = 3000  train loss = 1.615902 train acc = 0.9934, test acc = 0.4181\n",
            "[2025-05-14 06:19:53] Evaluate_00: epoch = 3000  train loss = 1.613151 train acc = 0.9952, test acc = 0.4175\n",
            "[2025-05-14 06:25:07] Evaluate_00: epoch = 3000  train loss = 1.593572 train acc = 0.9968, test acc = 0.4159\n",
            "[2025-05-14 06:30:03] Evaluate_00: epoch = 3000  train loss = 1.594062 train acc = 0.9977, test acc = 0.4169\n",
            "[2025-05-14 06:30:03] Evaluate_00: epoch = 3000 train time = 4558 s train loss = 1.594062 train acc = 0.9977, test acc = 0.4169\n",
            "Evaluate 1 random ConvNet, mean = 0.4181 std = 0.0000\n",
            "-------------------------\n",
            "\n",
            "[✅] Output saved to output_ipc100_epoch125_Noisy0.8.txt\n",
            "\n",
            "▶️ Running: IPC=100, Expert Epoch=125, probability:0.9 \n",
            "\n",
            "CUDNN STATUS: True\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Hyper-parameters: \n",
            " {'dataset': 'CIFAR100', 'data_path': '/content/cifar100', 'expert_path': '../train_expert', 'ipc': 100, 'teacher_model': 'ConvNet', 'student_model': 'ConvNet', 'epoch_eval_train': 3000, 'optimizer': 'SGD', 'lr_net': 0.01, 'true_class_probability': 0.9, 'expt_type': 'add_noise', 'num_eval': 1, 'batch_train': 256, 'batch_real': 256, 'zca': False, 'load_checkpoint': None, 'selection_strategy': 'random', 'dsa': True, 'dsa_strategy': 'color_crop_cutout_flip_scale_rotate', 'teacher_label': True, 'max_expert_epoch': 125, 'min_expert_epoch': 1, 'ensemble': False, 'temp': 1.0, 'wandb': False, 'run_name': None, 'notes': 'No description', 'device': 'cuda', 'im_size': [32, 32], 'dc_aug_param': None, 'dsa_param': <utils.ParamDiffAug object at 0x000001B5750FC650>, '_wandb': {}, 'zca_trans': None, 'distributed': False}\n",
            "Evaluation model:  ConvNet\n",
            "---------------Build label to index map--------------\n",
            "initialize synthetic data from random real images\n",
            "Expert Dir: ../train_expert\\CIFAR100_NO_ZCA\\ConvNet 2 buffers detected\n",
            "[2025-05-14 06:30:50] training begins\n",
            "-------------------------\n",
            "Experiment ID = 0\n",
            "loading file ../train_expert\\CIFAR100_NO_ZCA\\ConvNet\\replay_buffer_1.pt\n",
            "[INFO] Original labels saved to ../../labels_before_noise_IPC100_ExpEpoch125.pt\n",
            "[INFO] Noisy labels saved to ../../noisy_labels_top0.9_IPC100_ExpEpoch125.pt\n",
            "-------------------------\n",
            "Evaluation\n",
            "Experiment ID = 0, model_train = ConvNet, model_eval = ConvNet\n",
            "DSA augmentation strategy: \n",
            " color_crop_cutout_flip_scale_rotate\n",
            "DSA augmentation parameters: \n",
            " {'aug_mode': 'S', 'prob_flip': 0.5, 'ratio_scale': 1.2, 'ratio_rotate': 15.0, 'ratio_crop_pad': 0.125, 'ratio_cutout': 0.5, 'ratio_noise': 0.05, 'brightness': 1.0, 'saturation': 2.0, 'contrast': 0.5}\n",
            "[2025-05-14 06:37:50] Evaluate_00: epoch = 3000  train loss = 1.195546 train acc = 0.9409, test acc = 0.3899\n",
            "[2025-05-14 06:43:03] Evaluate_00: epoch = 3000  train loss = 1.088019 train acc = 0.9735, test acc = 0.3874\n",
            "[2025-05-14 06:48:02] Evaluate_00: epoch = 3000  train loss = 1.070593 train acc = 0.9742, test acc = 0.3882\n",
            "[2025-05-14 06:53:00] Evaluate_00: epoch = 3000  train loss = 0.972283 train acc = 0.9922, test acc = 0.4062\n",
            "[2025-05-14 06:58:19] Evaluate_00: epoch = 3000  train loss = 0.957997 train acc = 0.9950, test acc = 0.4058\n",
            "[2025-05-14 07:03:19] Evaluate_00: epoch = 3000  train loss = 1.000928 train acc = 0.9899, test acc = 0.4081\n",
            "[2025-05-14 07:08:17] Evaluate_00: epoch = 3000  train loss = 0.986874 train acc = 0.9920, test acc = 0.4093\n",
            "[2025-05-14 07:13:32] Evaluate_00: epoch = 3000  train loss = 0.965328 train acc = 0.9934, test acc = 0.4136\n",
            "[2025-05-14 07:18:30] Evaluate_00: epoch = 3000  train loss = 0.972255 train acc = 0.9939, test acc = 0.4146\n",
            "[2025-05-14 07:23:37] Evaluate_00: epoch = 3000  train loss = 0.935104 train acc = 0.9972, test acc = 0.4132\n",
            "[2025-05-14 07:28:52] Evaluate_00: epoch = 3000  train loss = 0.955097 train acc = 0.9955, test acc = 0.4143\n",
            "[2025-05-14 07:33:49] Evaluate_00: epoch = 3000  train loss = 0.962501 train acc = 0.9953, test acc = 0.4175\n",
            "[2025-05-14 07:38:48] Evaluate_00: epoch = 3000  train loss = 0.951064 train acc = 0.9969, test acc = 0.4168\n",
            "[2025-05-14 07:44:01] Evaluate_00: epoch = 3000  train loss = 0.934820 train acc = 0.9963, test acc = 0.4161\n",
            "[2025-05-14 07:48:58] Evaluate_00: epoch = 3000  train loss = 0.944903 train acc = 0.9967, test acc = 0.4160\n",
            "[2025-05-14 07:48:58] Evaluate_00: epoch = 3000 train time = 4568 s train loss = 0.944903 train acc = 0.9967, test acc = 0.4160\n",
            "Evaluate 1 random ConvNet, mean = 0.4175 std = 0.0000\n",
            "-------------------------\n",
            "\n",
            "[✅] Output saved to output_ipc100_epoch125_Noisy0.9.txt\n",
            "\n",
            "▶️ Running: IPC=100, Expert Epoch=125, probability:0.95 \n",
            "\n",
            "CUDNN STATUS: True\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Hyper-parameters: \n",
            " {'dataset': 'CIFAR100', 'data_path': '/content/cifar100', 'expert_path': '../train_expert', 'ipc': 100, 'teacher_model': 'ConvNet', 'student_model': 'ConvNet', 'epoch_eval_train': 3000, 'optimizer': 'SGD', 'lr_net': 0.01, 'true_class_probability': 0.95, 'expt_type': 'add_noise', 'num_eval': 1, 'batch_train': 256, 'batch_real': 256, 'zca': False, 'load_checkpoint': None, 'selection_strategy': 'random', 'dsa': True, 'dsa_strategy': 'color_crop_cutout_flip_scale_rotate', 'teacher_label': True, 'max_expert_epoch': 125, 'min_expert_epoch': 1, 'ensemble': False, 'temp': 1.0, 'wandb': False, 'run_name': None, 'notes': 'No description', 'device': 'cuda', 'im_size': [32, 32], 'dc_aug_param': None, 'dsa_param': <utils.ParamDiffAug object at 0x0000015F82247FD0>, '_wandb': {}, 'zca_trans': None, 'distributed': False}\n",
            "Evaluation model:  ConvNet\n",
            "---------------Build label to index map--------------\n",
            "initialize synthetic data from random real images\n",
            "Expert Dir: ../train_expert\\CIFAR100_NO_ZCA\\ConvNet 2 buffers detected\n",
            "[2025-05-14 07:49:47] training begins\n",
            "-------------------------\n",
            "Experiment ID = 0\n",
            "loading file ../train_expert\\CIFAR100_NO_ZCA\\ConvNet\\replay_buffer_1.pt\n",
            "[INFO] Original labels saved to ../../labels_before_noise_IPC100_ExpEpoch125.pt\n",
            "[INFO] Noisy labels saved to ../../noisy_labels_top0.95_IPC100_ExpEpoch125.pt\n",
            "-------------------------\n",
            "Evaluation\n",
            "Experiment ID = 0, model_train = ConvNet, model_eval = ConvNet\n",
            "DSA augmentation strategy: \n",
            " color_crop_cutout_flip_scale_rotate\n",
            "DSA augmentation parameters: \n",
            " {'aug_mode': 'S', 'prob_flip': 0.5, 'ratio_scale': 1.2, 'ratio_rotate': 15.0, 'ratio_crop_pad': 0.125, 'ratio_cutout': 0.5, 'ratio_noise': 0.05, 'brightness': 1.0, 'saturation': 2.0, 'contrast': 0.5}\n",
            "[2025-05-14 07:57:00] Evaluate_00: epoch = 3000  train loss = 0.799580 train acc = 0.9383, test acc = 0.3874\n",
            "[2025-05-14 08:01:57] Evaluate_00: epoch = 3000  train loss = 0.688296 train acc = 0.9737, test acc = 0.3935\n",
            "[2025-05-14 08:06:56] Evaluate_00: epoch = 3000  train loss = 0.661370 train acc = 0.9794, test acc = 0.4047\n",
            "[2025-05-14 08:12:13] Evaluate_00: epoch = 3000  train loss = 0.597799 train acc = 0.9904, test acc = 0.4142\n",
            "[2025-05-14 08:17:15] Evaluate_00: epoch = 3000  train loss = 0.588061 train acc = 0.9927, test acc = 0.4146\n",
            "[2025-05-14 08:22:15] Evaluate_00: epoch = 3000  train loss = 0.592222 train acc = 0.9929, test acc = 0.4176\n",
            "[2025-05-14 08:27:29] Evaluate_00: epoch = 3000  train loss = 0.600506 train acc = 0.9900, test acc = 0.4186\n",
            "[2025-05-14 08:32:29] Evaluate_00: epoch = 3000  train loss = 0.565746 train acc = 0.9945, test acc = 0.4203\n",
            "[2025-05-14 08:37:29] Evaluate_00: epoch = 3000  train loss = 0.555560 train acc = 0.9964, test acc = 0.4254\n",
            "[2025-05-14 08:42:44] Evaluate_00: epoch = 3000  train loss = 0.571368 train acc = 0.9958, test acc = 0.4254\n",
            "[2025-05-14 08:47:42] Evaluate_00: epoch = 3000  train loss = 0.574366 train acc = 0.9956, test acc = 0.4257\n",
            "[2025-05-14 08:52:58] Evaluate_00: epoch = 3000  train loss = 0.556637 train acc = 0.9970, test acc = 0.4263\n",
            "[2025-05-14 08:58:19] Evaluate_00: epoch = 3000  train loss = 0.552267 train acc = 0.9980, test acc = 0.4260\n",
            "[2025-05-14 09:03:19] Evaluate_00: epoch = 3000  train loss = 0.543886 train acc = 0.9971, test acc = 0.4273\n",
            "[2025-05-14 09:08:21] Evaluate_00: epoch = 3000  train loss = 0.548080 train acc = 0.9977, test acc = 0.4259\n",
            "[2025-05-14 09:08:21] Evaluate_00: epoch = 3000 train time = 4595 s train loss = 0.548080 train acc = 0.9977, test acc = 0.4259\n",
            "Evaluate 1 random ConvNet, mean = 0.4273 std = 0.0000\n",
            "-------------------------\n",
            "\n",
            "[✅] Output saved to output_ipc100_epoch125_Noisy0.95.txt\n",
            "\n",
            "▶️ Running: IPC=100, Expert Epoch=125, probability:1 \n",
            "\n",
            "CUDNN STATUS: True\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Hyper-parameters: \n",
            " {'dataset': 'CIFAR100', 'data_path': '/content/cifar100', 'expert_path': '../train_expert', 'ipc': 100, 'teacher_model': 'ConvNet', 'student_model': 'ConvNet', 'epoch_eval_train': 3000, 'optimizer': 'SGD', 'lr_net': 0.01, 'true_class_probability': 1.0, 'expt_type': 'add_noise', 'num_eval': 1, 'batch_train': 256, 'batch_real': 256, 'zca': False, 'load_checkpoint': None, 'selection_strategy': 'random', 'dsa': True, 'dsa_strategy': 'color_crop_cutout_flip_scale_rotate', 'teacher_label': True, 'max_expert_epoch': 125, 'min_expert_epoch': 1, 'ensemble': False, 'temp': 1.0, 'wandb': False, 'run_name': None, 'notes': 'No description', 'device': 'cuda', 'im_size': [32, 32], 'dc_aug_param': None, 'dsa_param': <utils.ParamDiffAug object at 0x0000019BB76C6910>, '_wandb': {}, 'zca_trans': None, 'distributed': False}\n",
            "Evaluation model:  ConvNet\n",
            "---------------Build label to index map--------------\n",
            "initialize synthetic data from random real images\n",
            "Expert Dir: ../train_expert\\CIFAR100_NO_ZCA\\ConvNet 2 buffers detected\n",
            "[2025-05-14 09:09:08] training begins\n",
            "-------------------------\n",
            "Experiment ID = 0\n",
            "loading file ../train_expert\\CIFAR100_NO_ZCA\\ConvNet\\replay_buffer_1.pt\n",
            "[INFO] Original labels saved to ../../labels_before_noise_IPC100_ExpEpoch125.pt\n",
            "[INFO] Noisy labels saved to ../../noisy_labels_top1.0_IPC100_ExpEpoch125.pt\n",
            "-------------------------\n",
            "Evaluation\n",
            "Experiment ID = 0, model_train = ConvNet, model_eval = ConvNet\n",
            "DSA augmentation strategy: \n",
            " color_crop_cutout_flip_scale_rotate\n",
            "DSA augmentation parameters: \n",
            " {'aug_mode': 'S', 'prob_flip': 0.5, 'ratio_scale': 1.2, 'ratio_rotate': 15.0, 'ratio_crop_pad': 0.125, 'ratio_cutout': 0.5, 'ratio_noise': 0.05, 'brightness': 1.0, 'saturation': 2.0, 'contrast': 0.5}\n",
            "[2025-05-14 09:16:25] Evaluate_00: epoch = 3000  train loss = 0.242471 train acc = 0.9431, test acc = 0.3915\n",
            "[2025-05-14 09:21:23] Evaluate_00: epoch = 3000  train loss = 0.198077 train acc = 0.9525, test acc = 0.4068\n",
            "[2025-05-14 09:26:38] Evaluate_00: epoch = 3000  train loss = 0.149940 train acc = 0.9701, test acc = 0.4093\n",
            "[2025-05-14 09:31:44] Evaluate_00: epoch = 3000  train loss = 0.082900 train acc = 0.9885, test acc = 0.4298\n",
            "[2025-05-14 09:36:48] Evaluate_00: epoch = 3000  train loss = 0.062964 train acc = 0.9919, test acc = 0.4351\n",
            "[2025-05-14 09:42:00] Evaluate_00: epoch = 3000  train loss = 0.053342 train acc = 0.9950, test acc = 0.4362\n",
            "[2025-05-14 09:46:59] Evaluate_00: epoch = 3000  train loss = 0.082357 train acc = 0.9889, test acc = 0.4341\n",
            "[2025-05-14 09:51:59] Evaluate_00: epoch = 3000  train loss = 0.051957 train acc = 0.9951, test acc = 0.4429\n",
            "[2025-05-14 09:57:14] Evaluate_00: epoch = 3000  train loss = 0.042412 train acc = 0.9969, test acc = 0.4426\n",
            "[2025-05-14 10:02:14] Evaluate_00: epoch = 3000  train loss = 0.064187 train acc = 0.9956, test acc = 0.4379\n",
            "[2025-05-14 10:07:14] Evaluate_00: epoch = 3000  train loss = 0.039763 train acc = 0.9976, test acc = 0.4429\n",
            "[2025-05-14 10:12:26] Evaluate_00: epoch = 3000  train loss = 0.047217 train acc = 0.9966, test acc = 0.4447\n",
            "[2025-05-14 10:17:27] Evaluate_00: epoch = 3000  train loss = 0.046668 train acc = 0.9971, test acc = 0.4490\n",
            "[2025-05-14 10:22:27] Evaluate_00: epoch = 3000  train loss = 0.046202 train acc = 0.9964, test acc = 0.4453\n",
            "[2025-05-14 10:27:37] Evaluate_00: epoch = 3000  train loss = 0.050804 train acc = 0.9968, test acc = 0.4483\n",
            "[2025-05-14 10:27:37] Evaluate_00: epoch = 3000 train time = 4590 s train loss = 0.050804 train acc = 0.9968, test acc = 0.4483\n",
            "Evaluate 1 random ConvNet, mean = 0.4490 std = 0.0000\n",
            "-------------------------\n",
            "\n",
            "[✅] Output saved to output_ipc100_epoch125_Noisy1.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "\n",
        "# Load soft labels from file\n",
        "labels = torch.load(\"../../labels_swapped_IPC10_ExpEpoch57_4.pt\")  # Adjust path if needed\n",
        "\n",
        "num_classes = labels.shape[1]\n",
        "num_examples_to_visualize = 5\n",
        "\n",
        "# Pick random samples\n",
        "#example_indices = random.sample(range(len(labels)), num_examples_to_visualize)\n",
        "\n",
        "\n",
        "# Pick non random sample\n",
        "example_indices = list(range(num_examples_to_visualize))\n",
        "\n",
        "\n",
        "plt.figure(figsize=(20, 4))\n",
        "\n",
        "for i, idx in enumerate(example_indices):\n",
        "    plt.subplot(1, num_examples_to_visualize, i + 1)\n",
        "    plt.bar(range(num_classes), labels[idx].cpu().numpy())\n",
        "    plt.title(f\"Example {i + 1}\")\n",
        "    plt.xlabel(\"Class\")\n",
        "    plt.ylabel(\"Probability\")\n",
        "    plt.xticks([])  # Hide class labels for cleaner look\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 462
        },
        "id": "8TQijCB_XGdx",
        "outputId": "a1ffff1c-1e85-407b-8568-fc169a6aa0b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "C:\\Users\\nv1n24\\AppData\\Local\\Temp\\ipykernel_6912\\2487782695.py:6: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  labels = torch.load(\"../../labels_swapped_IPC10_ExpEpoch57_4.pt\")  # Adjust path if needed\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 2000x400 with 5 Axes>"
            ],
            "image/png": "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"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "entropy_vals = []\n",
        "for i in range(300):\n",
        "  labels = torch.load(f\"soft_labels_epoch_{i}.pt\")\n",
        "  entropy_image = -torch.sum(labels * torch.log(labels + 1e-10), axis=1)\n",
        "  entropy_vals.append(torch.mean(entropy_image).cpu().numpy().item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hzZfGA5tzf5C",
        "outputId": "2d1f4d60-7898-4757-e31b-be55c0e85e8e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "C:\\Users\\nv1n24\\AppData\\Local\\Temp\\ipykernel_25780\\1868499417.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  labels = torch.load(f\"soft_labels_epoch_{i}.pt\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "entropy_vals"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "RJeSYOL8zg09",
        "outputId": "1820bf00-b65b-46b8-f4f7-1ae6968d20a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[4.492241382598877,\n",
              " 4.354091644287109,\n",
              " 4.21491003036499,\n",
              " 4.083327770233154,\n",
              " 3.9539976119995117,\n",
              " 3.867548704147339,\n",
              " 3.8047006130218506,\n",
              " 3.658531904220581,\n",
              " 3.5591037273406982,\n",
              " 3.434518814086914,\n",
              " 3.402005195617676,\n",
              " 3.363887071609497,\n",
              " 3.257476329803467,\n",
              " 3.260608196258545,\n",
              " 3.134767532348633,\n",
              " 3.098748207092285,\n",
              " 3.0096399784088135,\n",
              " 3.0463619232177734,\n",
              " 2.9384286403656006,\n",
              " 2.870436191558838,\n",
              " 2.8581669330596924,\n",
              " 2.893280029296875,\n",
              " 2.8230600357055664,\n",
              " 2.749382495880127,\n",
              " 2.7267887592315674,\n",
              " 2.717005968093872,\n",
              " 2.6611950397491455,\n",
              " 2.6343836784362793,\n",
              " 2.5985875129699707,\n",
              " 2.595534563064575,\n",
              " 2.5464959144592285,\n",
              " 2.5607855319976807,\n",
              " 2.5414011478424072,\n",
              " 2.484488010406494,\n",
              " 2.457017183303833,\n",
              " 2.434696912765503,\n",
              " 2.3879518508911133,\n",
              " 2.3519034385681152,\n",
              " 2.318274736404419,\n",
              " 2.357288360595703,\n",
              " 2.2881031036376953,\n",
              " 2.3413453102111816,\n",
              " 2.206984281539917,\n",
              " 2.2591166496276855,\n",
              " 2.215219259262085,\n",
              " 2.1494297981262207,\n",
              " 2.206542730331421,\n",
              " 2.1656885147094727,\n",
              " 2.1333487033843994,\n",
              " 2.1071300506591797,\n",
              " 2.073122978210449,\n",
              " 2.092026948928833,\n",
              " 2.048098087310791,\n",
              " 2.0322928428649902,\n",
              " 2.0153350830078125,\n",
              " 1.9772359132766724,\n",
              " 1.996653437614441,\n",
              " 1.9988863468170166,\n",
              " 2.017772674560547,\n",
              " 1.9193077087402344,\n",
              " 2.0082461833953857,\n",
              " 1.9135596752166748,\n",
              " 1.8855323791503906,\n",
              " 1.9145257472991943,\n",
              " 1.8589410781860352,\n",
              " 1.8686200380325317,\n",
              " 1.8705787658691406,\n",
              " 1.8747047185897827,\n",
              " 1.8298951387405396,\n",
              " 1.8397331237792969,\n",
              " 1.8101153373718262,\n",
              " 1.7690218687057495,\n",
              " 1.7677117586135864,\n",
              " 1.7396023273468018,\n",
              " 1.746773600578308,\n",
              " 1.7739092111587524,\n",
              " 1.7037537097930908,\n",
              " 1.7388393878936768,\n",
              " 1.7708930969238281,\n",
              " 1.7294915914535522,\n",
              " 1.6531487703323364,\n",
              " 1.6804856061935425,\n",
              " 1.6705987453460693,\n",
              " 1.639391541481018,\n",
              " 1.66078519821167,\n",
              " 1.6480700969696045,\n",
              " 1.619926929473877,\n",
              " 1.6436824798583984,\n",
              " 1.6251227855682373,\n",
              " 1.5522865056991577,\n",
              " 1.60463285446167,\n",
              " 1.5742732286453247,\n",
              " 1.6249456405639648,\n",
              " 1.5890356302261353,\n",
              " 1.4933675527572632,\n",
              " 1.5522712469100952,\n",
              " 1.5032137632369995,\n",
              " 1.5439828634262085,\n",
              " 1.4963862895965576,\n",
              " 1.4924525022506714,\n",
              " 1.4845722913742065,\n",
              " 1.4338477849960327,\n",
              " 1.4803547859191895,\n",
              " 1.435244083404541,\n",
              " 1.437626600265503,\n",
              " 1.4155465364456177,\n",
              " 1.3874281644821167,\n",
              " 1.3817821741104126,\n",
              " 1.406906247138977,\n",
              " 1.4017245769500732,\n",
              " 1.4027962684631348,\n",
              " 1.3533682823181152,\n",
              " 1.3416177034378052,\n",
              " 1.4645802974700928,\n",
              " 1.3723827600479126,\n",
              " 1.31424880027771,\n",
              " 1.3301571607589722,\n",
              " 1.39138925075531,\n",
              " 1.3619431257247925,\n",
              " 1.2812600135803223,\n",
              " 1.3670504093170166,\n",
              " 1.293097972869873,\n",
              " 1.3091557025909424,\n",
              " 1.283685326576233,\n",
              " 1.2460421323776245,\n",
              " 1.2651357650756836,\n",
              " 1.2935247421264648,\n",
              " 1.2481420040130615,\n",
              " 1.2485978603363037,\n",
              " 1.244464635848999,\n",
              " 1.2543772459030151,\n",
              " 1.2012150287628174,\n",
              " 1.2594358921051025,\n",
              " 1.2012982368469238,\n",
              " 1.208972692489624,\n",
              " 1.2503929138183594,\n",
              " 1.1880382299423218,\n",
              " 1.2023411989212036,\n",
              " 1.18468177318573,\n",
              " 1.2436856031417847,\n",
              " 1.165279507637024,\n",
              " 1.1371455192565918,\n",
              " 1.1519279479980469,\n",
              " 1.1511573791503906,\n",
              " 1.192846417427063,\n",
              " 1.1172547340393066,\n",
              " 1.11419677734375,\n",
              " 1.1438190937042236,\n",
              " 1.116746187210083,\n",
              " 1.126742959022522,\n",
              " 1.1021660566329956,\n",
              " 1.149621844291687,\n",
              " 1.1049270629882812,\n",
              " 1.0659598112106323,\n",
              " 1.081640362739563,\n",
              " 1.0847142934799194,\n",
              " 1.063454031944275,\n",
              " 1.0767948627471924,\n",
              " 1.0836353302001953,\n",
              " 1.0523555278778076,\n",
              " 1.0443116426467896,\n",
              " 1.0174821615219116,\n",
              " 1.0215075016021729,\n",
              " 1.0232515335083008,\n",
              " 1.0061464309692383,\n",
              " 1.0004521608352661,\n",
              " 0.9797319769859314,\n",
              " 1.0057810544967651,\n",
              " 0.9726552367210388,\n",
              " 1.0061877965927124,\n",
              " 1.0019809007644653,\n",
              " 0.9874539971351624,\n",
              " 0.9624512791633606,\n",
              " 0.9627000689506531,\n",
              " 0.9609967470169067,\n",
              " 0.930700957775116,\n",
              " 0.9464399218559265,\n",
              " 0.9832158088684082,\n",
              " 0.9633698463439941,\n",
              " 0.9154209494590759,\n",
              " 0.9390397071838379,\n",
              " 0.931723415851593,\n",
              " 0.9435179829597473,\n",
              " 0.9344111680984497,\n",
              " 0.9532779455184937,\n",
              " 0.9222531318664551,\n",
              " 0.8683640360832214,\n",
              " 0.893108606338501,\n",
              " 0.9014185667037964,\n",
              " 0.8859466910362244,\n",
              " 0.9047395586967468,\n",
              " 0.8545635938644409,\n",
              " 0.8536096811294556,\n",
              " 0.8832006454467773,\n",
              " 0.8934286832809448,\n",
              " 0.8628629446029663,\n",
              " 0.8408005237579346,\n",
              " 0.8907395601272583,\n",
              " 0.8505678176879883,\n",
              " 0.8382527232170105,\n",
              " 0.8439735770225525,\n",
              " 0.8321577906608582,\n",
              " 0.8168337345123291,\n",
              " 0.8429812788963318,\n",
              " 0.8056743741035461,\n",
              " 0.7746611833572388,\n",
              " 0.7995490431785583,\n",
              " 0.8064538836479187,\n",
              " 0.7818101644515991,\n",
              " 0.8159136176109314,\n",
              " 0.766707181930542,\n",
              " 0.7481744289398193,\n",
              " 0.7745676636695862,\n",
              " 0.7471776604652405,\n",
              " 0.7573693990707397,\n",
              " 0.7916726469993591,\n",
              " 0.7707212567329407,\n",
              " 0.7556964755058289,\n",
              " 0.75913405418396,\n",
              " 0.7354780435562134,\n",
              " 0.7615890502929688,\n",
              " 0.8289023637771606,\n",
              " 0.7053903341293335,\n",
              " 0.7642828226089478,\n",
              " 0.7174881100654602,\n",
              " 0.763370931148529,\n",
              " 0.728531539440155,\n",
              " 0.7791975736618042,\n",
              " 0.7519283890724182,\n",
              " 0.6896361112594604,\n",
              " 0.6704539060592651,\n",
              " 0.6631832718849182,\n",
              " 0.7015893459320068,\n",
              " 0.6723506450653076,\n",
              " 0.6874572038650513,\n",
              " 0.6865441203117371,\n",
              " 0.7302663326263428,\n",
              " 0.67845219373703,\n",
              " 0.6867071390151978,\n",
              " 0.6574501395225525,\n",
              " 0.6696352958679199,\n",
              " 0.6497836112976074,\n",
              " 0.6586173176765442,\n",
              " 0.6594440340995789,\n",
              " 0.682195782661438,\n",
              " 0.6760715842247009,\n",
              " 0.6882410645484924,\n",
              " 0.6298019886016846,\n",
              " 0.6610485911369324,\n",
              " 0.6596085429191589,\n",
              " 0.6666768193244934,\n",
              " 0.5967994928359985,\n",
              " 0.6742708086967468,\n",
              " 0.6080064177513123,\n",
              " 0.6188228726387024,\n",
              " 0.5994676351547241,\n",
              " 0.5905370712280273,\n",
              " 0.6816321611404419,\n",
              " 0.6445695161819458,\n",
              " 0.5853333473205566,\n",
              " 0.6008208394050598,\n",
              " 0.682338535785675,\n",
              " 0.5670385956764221,\n",
              " 0.5680420994758606,\n",
              " 0.6117088198661804,\n",
              " 0.5663049221038818,\n",
              " 0.604730486869812,\n",
              " 0.5921565890312195,\n",
              " 0.5808724164962769,\n",
              " 0.5609241127967834,\n",
              " 0.5608474016189575,\n",
              " 0.6036497950553894,\n",
              " 0.5599461197853088,\n",
              " 0.6191548705101013,\n",
              " 0.5545322895050049,\n",
              " 0.575366199016571,\n",
              " 0.6374840140342712,\n",
              " 0.5368940830230713,\n",
              " 0.5176280736923218,\n",
              " 0.5956672430038452,\n",
              " 0.5341545343399048,\n",
              " 0.5097196698188782,\n",
              " 0.5288265347480774,\n",
              " 0.5480047464370728,\n",
              " 0.5268304944038391,\n",
              " 0.5267177820205688,\n",
              " 0.5440435409545898,\n",
              " 0.5091617703437805,\n",
              " 0.5565779805183411,\n",
              " 0.5099740624427795,\n",
              " 0.4896654188632965,\n",
              " 0.5159018635749817,\n",
              " 0.5269283056259155,\n",
              " 0.5019847750663757,\n",
              " 0.4726344645023346,\n",
              " 0.4824307858943939,\n",
              " 0.4859790503978729,\n",
              " 0.5026256442070007,\n",
              " 0.5046775937080383,\n",
              " 0.5370319485664368]"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    }
  ]
}